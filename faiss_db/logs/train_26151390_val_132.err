INFO:root:Output: small_val_132
INFO:root:Steps per epochs:992
INFO:root:Total steps:198400
/scratch/zw2374/public/faiss_db/models.py:432: UserWarning: Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.
  warnings.warn("Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.")
Some weights of RetrievalGenerationModel were not initialized from the model checkpoint at sentence-transformers/multi-qa-MiniLM-L6-cos-v1 and are newly initialized: ['encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.0.crossattention.self.value.weight', 'cls.predictions.bias', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.1.crossattention.output.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.2.crossattention.self.key.bias', 'cls.predictions.transform.dense.bias', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.0.crossattention.output.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.5.crossattention.output.dense.bias', 'cls.predictions.transform.dense.weight', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.4.crossattention.output.dense.weight', 'cls.predictions.decoder.weight', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.1.crossattention.self.query.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/scratch/zw2374/public/faiss_db/models.py:446: UserWarning: Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.
  warnings.warn("Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.")
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:root:started training
  0%|          | 0/200 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 24455.36209753788
INFO:root:current train perplexity15503.408203125
INFO:root:current mean train loss 20539.371721890704
INFO:root:current train perplexity3277.403076171875
INFO:root:current mean train loss 17745.910649430392
INFO:root:current train perplexity1092.732421875
INFO:root:current mean train loss 15853.419246358082
INFO:root:current train perplexity513.8294677734375
INFO:root:current mean train loss 14480.013154238164
INFO:root:current train perplexity299.02099609375
INFO:root:current mean train loss 13437.431664264659
INFO:root:current train perplexity198.88514709472656
INFO:root:current mean train loss 12626.341149326046
INFO:root:current train perplexity144.44515991210938
INFO:root:current mean train loss 11976.06282266896
INFO:root:current train perplexity112.05024719238281
INFO:root:current mean train loss 11443.724073297588
INFO:root:current train perplexity90.88417053222656

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.02s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.02s/it]
INFO:root:final mean train loss: 11014.44331507529
INFO:root:final train perplexity: 77.13188171386719
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.58s/it]
INFO:root:eval mean loss: 6415.6092157025705
INFO:root:eval perplexity: 13.38700008392334
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/1
  0%|          | 1/200 [02:10<7:11:58, 130.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6822.613211495535
INFO:root:current train perplexity14.504887580871582
INFO:root:current mean train loss 6731.670282381718
INFO:root:current train perplexity14.401764869689941
INFO:root:current mean train loss 6703.088102921196
INFO:root:current train perplexity14.163623809814453
INFO:root:current mean train loss 6630.767197997252
INFO:root:current train perplexity13.740307807922363
INFO:root:current mean train loss 6577.056297508446
INFO:root:current train perplexity13.439475059509277
INFO:root:current mean train loss 6527.119100175666
INFO:root:current train perplexity13.169710159301758
INFO:root:current mean train loss 6484.465978782691
INFO:root:current train perplexity12.899394989013672
INFO:root:current mean train loss 6436.787353860944
INFO:root:current train perplexity12.657753944396973
INFO:root:current mean train loss 6392.64709185254
INFO:root:current train perplexity12.432087898254395
INFO:root:current mean train loss 6348.07394796117
INFO:root:current train perplexity12.227664947509766

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:03<00:00, 123.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:03<00:00, 123.59s/it]
INFO:root:final mean train loss: 6310.989468113069
INFO:root:final train perplexity: 12.059679985046387
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.04s/it]
INFO:root:eval mean loss: 5543.6573339151155
INFO:root:eval perplexity: 9.4092435836792
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/2
  1%|          | 2/200 [04:24<7:17:58, 132.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 5987.975260416667
INFO:root:current train perplexity10.641189575195312
INFO:root:current mean train loss 5882.679182235054
INFO:root:current train perplexity10.244375228881836
INFO:root:current mean train loss 5861.4403524709305
INFO:root:current train perplexity10.107771873474121
INFO:root:current mean train loss 5841.127273995536
INFO:root:current train perplexity9.994363784790039
INFO:root:current mean train loss 5816.988377729669
INFO:root:current train perplexity9.905390739440918
INFO:root:current mean train loss 5792.350873217536
INFO:root:current train perplexity9.82188606262207
INFO:root:current mean train loss 5767.8446519308945
INFO:root:current train perplexity9.7427396774292
INFO:root:current mean train loss 5749.016355031687
INFO:root:current train perplexity9.66472339630127
INFO:root:current mean train loss 5735.992398389571
INFO:root:current train perplexity9.594311714172363
INFO:root:current mean train loss 5717.375034153006
INFO:root:current train perplexity9.511385917663574

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.17s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.17s/it]
INFO:root:final mean train loss: 5693.804387738628
INFO:root:final train perplexity: 9.453363418579102
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.23s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.24s/it]
INFO:root:eval mean loss: 5180.328370872119
INFO:root:eval perplexity: 8.123597145080566
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/3
  2%|â–         | 3/200 [06:58<7:47:07, 142.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 5567.260912024457
INFO:root:current train perplexity8.829641342163086
INFO:root:current mean train loss 5487.636028010671
INFO:root:current train perplexity8.725125312805176
INFO:root:current mean train loss 5486.2518983849495
INFO:root:current train perplexity8.663527488708496
INFO:root:current mean train loss 5459.448828729683
INFO:root:current train perplexity8.583148002624512
INFO:root:current mean train loss 5446.899777676197
INFO:root:current train perplexity8.558257102966309
INFO:root:current mean train loss 5430.227527859106
INFO:root:current train perplexity8.503384590148926
INFO:root:current mean train loss 5420.610869626555
INFO:root:current train perplexity8.471421241760254
INFO:root:current mean train loss 5411.090960110866
INFO:root:current train perplexity8.437530517578125
INFO:root:current mean train loss 5400.1720179839385
INFO:root:current train perplexity8.400847434997559
INFO:root:current mean train loss 5389.40490471374
INFO:root:current train perplexity8.365598678588867

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:04<00:00, 124.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:04<00:00, 124.74s/it]
INFO:root:final mean train loss: 5378.118528919836
INFO:root:final train perplexity: 8.346339225769043
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.59s/it]
INFO:root:eval mean loss: 4959.387705701462
INFO:root:eval perplexity: 7.429294586181641
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/4
  2%|â–         | 4/200 [09:18<7:41:48, 141.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 5322.167543472782
INFO:root:current train perplexity7.974098205566406
INFO:root:current mean train loss 5242.570327409351
INFO:root:current train perplexity7.890812397003174
INFO:root:current mean train loss 5244.396518195346
INFO:root:current train perplexity7.911118984222412
INFO:root:current mean train loss 5233.484431056458
INFO:root:current train perplexity7.86412239074707
INFO:root:current mean train loss 5227.375683140589
INFO:root:current train perplexity7.833351135253906
INFO:root:current mean train loss 5217.647522547375
INFO:root:current train perplexity7.805764675140381
INFO:root:current mean train loss 5210.296097309578
INFO:root:current train perplexity7.782161235809326
INFO:root:current mean train loss 5202.22012867647
INFO:root:current train perplexity7.765684127807617
INFO:root:current mean train loss 5189.146085406325
INFO:root:current train perplexity7.741642951965332
INFO:root:current mean train loss 5180.629980363856
INFO:root:current train perplexity7.71063756942749

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.70s/it]
INFO:root:final mean train loss: 5171.057695942541
INFO:root:final train perplexity: 7.691622734069824
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.47s/it]
INFO:root:eval mean loss: 4819.917175725842
INFO:root:eval perplexity: 7.021894454956055
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/5
  2%|â–Ž         | 5/200 [11:59<8:02:42, 148.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 5080.854341947115
INFO:root:current train perplexity7.3404059410095215
INFO:root:current mean train loss 5071.594663331834
INFO:root:current train perplexity7.397947311401367
INFO:root:current mean train loss 5080.546223277327
INFO:root:current train perplexity7.412014961242676
INFO:root:current mean train loss 5063.511381706306
INFO:root:current train perplexity7.350009441375732
INFO:root:current mean train loss 5060.808738343537
INFO:root:current train perplexity7.338747978210449
INFO:root:current mean train loss 5049.000089684311
INFO:root:current train perplexity7.316334247589111
INFO:root:current mean train loss 5040.997371381064
INFO:root:current train perplexity7.295624256134033
INFO:root:current mean train loss 5041.171992610369
INFO:root:current train perplexity7.291385650634766
INFO:root:current mean train loss 5037.143148465435
INFO:root:current train perplexity7.274587154388428
INFO:root:current mean train loss 5029.134875865282
INFO:root:current train perplexity7.25908088684082

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.58s/it]
INFO:root:final mean train loss: 5020.482643496605
INFO:root:final train perplexity: 7.247999668121338
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.10s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.10s/it]
INFO:root:eval mean loss: 4704.241553773271
INFO:root:eval perplexity: 6.701003551483154
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/6
  3%|â–Ž         | 6/200 [14:13<7:43:47, 143.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4894.976853390957
INFO:root:current train perplexity6.941790580749512
INFO:root:current mean train loss 4944.26530944409
INFO:root:current train perplexity7.006056308746338
INFO:root:current mean train loss 4926.757705750253
INFO:root:current train perplexity6.990878105163574
INFO:root:current mean train loss 4927.013128714878
INFO:root:current train perplexity6.983225345611572
INFO:root:current mean train loss 4924.223355355145
INFO:root:current train perplexity6.972856521606445
INFO:root:current mean train loss 4915.778763961094
INFO:root:current train perplexity6.952308177947998
INFO:root:current mean train loss 4914.935793657023
INFO:root:current train perplexity6.947966575622559
INFO:root:current mean train loss 4911.696490388638
INFO:root:current train perplexity6.935462951660156
INFO:root:current mean train loss 4908.1928463049735
INFO:root:current train perplexity6.926995754241943
INFO:root:current mean train loss 4905.808250870347
INFO:root:current train perplexity6.915658473968506

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.30s/it]
INFO:root:final mean train loss: 4900.968754183861
INFO:root:final train perplexity: 6.914176940917969
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.06s/it]
INFO:root:eval mean loss: 4617.483777634641
INFO:root:eval perplexity: 6.469992637634277
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/7
  4%|â–Ž         | 7/200 [16:26<7:30:39, 140.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4793.931494140625
INFO:root:current train perplexity6.716492176055908
INFO:root:current mean train loss 4846.072768082157
INFO:root:current train perplexity6.778032302856445
INFO:root:current mean train loss 4837.411200788909
INFO:root:current train perplexity6.729733467102051
INFO:root:current mean train loss 4836.569882674956
INFO:root:current train perplexity6.722221851348877
INFO:root:current mean train loss 4830.403942200378
INFO:root:current train perplexity6.704303741455078
INFO:root:current mean train loss 4824.471333491695
INFO:root:current train perplexity6.686710357666016
INFO:root:current mean train loss 4822.885068657562
INFO:root:current train perplexity6.684207916259766
INFO:root:current mean train loss 4824.688566134623
INFO:root:current train perplexity6.679565906524658
INFO:root:current mean train loss 4815.905339969389
INFO:root:current train perplexity6.663558483123779
INFO:root:current mean train loss 4808.878434074362
INFO:root:current train perplexity6.656065464019775

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.88s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.88s/it]
INFO:root:final mean train loss: 4804.2710373786185
INFO:root:final train perplexity: 6.655369758605957
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.26s/it]
INFO:root:eval mean loss: 4544.696635361259
INFO:root:eval perplexity: 6.282336711883545
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/8
  4%|â–         | 8/200 [19:02<7:44:35, 145.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4751.216905381944
INFO:root:current train perplexity6.459979057312012
INFO:root:current mean train loss 4738.327786498275
INFO:root:current train perplexity6.477474689483643
INFO:root:current mean train loss 4731.168798642467
INFO:root:current train perplexity6.473210334777832
INFO:root:current mean train loss 4740.847375790935
INFO:root:current train perplexity6.470615386962891
INFO:root:current mean train loss 4744.8550843471585
INFO:root:current train perplexity6.479371547698975
INFO:root:current mean train loss 4736.751831271508
INFO:root:current train perplexity6.465996265411377
INFO:root:current mean train loss 4733.828274135676
INFO:root:current train perplexity6.462954044342041
INFO:root:current mean train loss 4731.760361737693
INFO:root:current train perplexity6.455273151397705
INFO:root:current mean train loss 4726.679656098367
INFO:root:current train perplexity6.4463300704956055
INFO:root:current mean train loss 4723.305193274192
INFO:root:current train perplexity6.43743896484375

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:05<00:00, 125.09s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:05<00:00, 125.09s/it]
INFO:root:final mean train loss: 4721.155564092821
INFO:root:final train perplexity: 6.440670967102051
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.45s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.45s/it]
INFO:root:eval mean loss: 4488.571264821587
INFO:root:eval perplexity: 6.141362190246582
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/9
  4%|â–         | 9/200 [21:24<7:39:19, 144.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4644.168608329665
INFO:root:current train perplexity6.219272613525391
INFO:root:current mean train loss 4655.839061357821
INFO:root:current train perplexity6.271139144897461
INFO:root:current mean train loss 4677.805307310886
INFO:root:current train perplexity6.306364059448242
INFO:root:current mean train loss 4671.75779933878
INFO:root:current train perplexity6.293766498565674
INFO:root:current mean train loss 4665.326133517449
INFO:root:current train perplexity6.2901434898376465
INFO:root:current mean train loss 4665.430189463386
INFO:root:current train perplexity6.284898281097412
INFO:root:current mean train loss 4667.086784897192
INFO:root:current train perplexity6.290458679199219
INFO:root:current mean train loss 4657.4569441700105
INFO:root:current train perplexity6.276861190795898
INFO:root:current mean train loss 4660.075598102487
INFO:root:current train perplexity6.274631977081299
INFO:root:current mean train loss 4656.1945619750095
INFO:root:current train perplexity6.268524646759033

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.64s/it]
INFO:root:final mean train loss: 4651.97000035932
INFO:root:final train perplexity: 6.267245292663574
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.87s/it]
INFO:root:eval mean loss: 4442.391759128435
INFO:root:eval perplexity: 6.027744293212891
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/10
  5%|â–Œ         | 10/200 [23:42<7:30:16, 142.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4640.9579861797865
INFO:root:current train perplexity6.151862144470215
INFO:root:current mean train loss 4601.823048511697
INFO:root:current train perplexity6.115272521972656
INFO:root:current mean train loss 4611.338408658155
INFO:root:current train perplexity6.127856254577637
INFO:root:current mean train loss 4604.7632054955475
INFO:root:current train perplexity6.128625392913818
INFO:root:current mean train loss 4609.851744968359
INFO:root:current train perplexity6.13059139251709
INFO:root:current mean train loss 4602.75494901298
INFO:root:current train perplexity6.128758430480957
INFO:root:current mean train loss 4601.890656641201
INFO:root:current train perplexity6.125192165374756
INFO:root:current mean train loss 4600.0127238321365
INFO:root:current train perplexity6.122071266174316
INFO:root:current mean train loss 4594.494275055105
INFO:root:current train perplexity6.117945671081543
INFO:root:current mean train loss 4593.828429988748
INFO:root:current train perplexity6.118168354034424

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.70s/it]
INFO:root:final mean train loss: 4591.039216010801
INFO:root:final train perplexity: 6.118383407592773
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.99s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.99s/it]
INFO:root:eval mean loss: 4395.984608751663
INFO:root:eval perplexity: 5.915683746337891
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/11
  6%|â–Œ         | 11/200 [26:10<7:33:25, 143.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4537.789618130388
INFO:root:current train perplexity5.973252296447754
INFO:root:current mean train loss 4559.53507791611
INFO:root:current train perplexity6.007546424865723
INFO:root:current mean train loss 4549.326535108613
INFO:root:current train perplexity5.9876909255981445
INFO:root:current mean train loss 4549.422680600977
INFO:root:current train perplexity6.000474452972412
INFO:root:current mean train loss 4546.064335817185
INFO:root:current train perplexity5.997631549835205
INFO:root:current mean train loss 4542.643218630217
INFO:root:current train perplexity5.991477966308594
INFO:root:current mean train loss 4541.720203827215
INFO:root:current train perplexity5.991933822631836
INFO:root:current mean train loss 4538.518670708585
INFO:root:current train perplexity5.984899520874023
INFO:root:current mean train loss 4539.671306072523
INFO:root:current train perplexity5.986844539642334
INFO:root:current mean train loss 4539.453735722597
INFO:root:current train perplexity5.987082004547119

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:09<00:00, 129.90s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:09<00:00, 129.90s/it]
INFO:root:final mean train loss: 4536.062657140917
INFO:root:final train perplexity: 5.987104415893555
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.63s/it]
INFO:root:eval mean loss: 4358.208296971964
INFO:root:eval perplexity: 5.8260040283203125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/12
  6%|â–Œ         | 12/200 [28:53<7:49:04, 149.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4503.7020867598685
INFO:root:current train perplexity5.896903038024902
INFO:root:current mean train loss 4487.551257011218
INFO:root:current train perplexity5.888888359069824
INFO:root:current mean train loss 4492.225046345339
INFO:root:current train perplexity5.898922920227051
INFO:root:current mean train loss 4485.8137924001185
INFO:root:current train perplexity5.884207725524902
INFO:root:current mean train loss 4491.7038801096905
INFO:root:current train perplexity5.885222911834717
INFO:root:current mean train loss 4486.051270762211
INFO:root:current train perplexity5.880965709686279
INFO:root:current mean train loss 4483.505234445256
INFO:root:current train perplexity5.876877307891846
INFO:root:current mean train loss 4487.481714327831
INFO:root:current train perplexity5.871279716491699
INFO:root:current mean train loss 4489.262861710021
INFO:root:current train perplexity5.873932838439941

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.47s/it]
INFO:root:final mean train loss: 4488.514332309846
INFO:root:final train perplexity: 5.8758392333984375
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.94s/it]
INFO:root:eval mean loss: 4325.904873462434
INFO:root:eval perplexity: 5.750397205352783
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/13
  6%|â–‹         | 13/200 [31:08<7:33:35, 145.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4713.893391927083
INFO:root:current train perplexity6.055875778198242
INFO:root:current mean train loss 4462.972454774727
INFO:root:current train perplexity5.772326469421387
INFO:root:current mean train loss 4460.5008767414565
INFO:root:current train perplexity5.77116060256958
INFO:root:current mean train loss 4459.0643209828795
INFO:root:current train perplexity5.777989387512207
INFO:root:current mean train loss 4460.62788728094
INFO:root:current train perplexity5.782164096832275
INFO:root:current mean train loss 4457.016120561785
INFO:root:current train perplexity5.784902095794678
INFO:root:current mean train loss 4453.087346875648
INFO:root:current train perplexity5.785704612731934
INFO:root:current mean train loss 4449.129584842861
INFO:root:current train perplexity5.777985572814941
INFO:root:current mean train loss 4447.648688633445
INFO:root:current train perplexity5.775335311889648
INFO:root:current mean train loss 4448.114809900159
INFO:root:current train perplexity5.7751851081848145

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.50s/it]
INFO:root:final mean train loss: 4444.35241809968
INFO:root:final train perplexity: 5.774350643157959
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.55s/it]
INFO:root:eval mean loss: 4301.367412594193
INFO:root:eval perplexity: 5.693622589111328
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/14
  7%|â–‹         | 14/200 [33:35<7:32:25, 145.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4372.189985795455
INFO:root:current train perplexity5.554366588592529
INFO:root:current mean train loss 4396.9115551097975
INFO:root:current train perplexity5.68780517578125
INFO:root:current mean train loss 4388.421890041839
INFO:root:current train perplexity5.684691429138184
INFO:root:current mean train loss 4392.758006399467
INFO:root:current train perplexity5.682265281677246
INFO:root:current mean train loss 4390.706036273
INFO:root:current train perplexity5.678115367889404
INFO:root:current mean train loss 4394.009496162549
INFO:root:current train perplexity5.682134628295898
INFO:root:current mean train loss 4401.416429585209
INFO:root:current train perplexity5.687131881713867
INFO:root:current mean train loss 4402.563672630428
INFO:root:current train perplexity5.683797836303711
INFO:root:current mean train loss 4404.427724139758
INFO:root:current train perplexity5.6868577003479
INFO:root:current mean train loss 4406.304354654055
INFO:root:current train perplexity5.6822943687438965

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.89s/it]
INFO:root:final mean train loss: 4402.7077378303775
INFO:root:final train perplexity: 5.6802520751953125
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.94s/it]
INFO:root:eval mean loss: 4273.315074731272
INFO:root:eval perplexity: 5.629400730133057
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/15
  8%|â–Š         | 15/200 [36:19<7:46:38, 151.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4423.9817922491775
INFO:root:current train perplexity5.669549465179443
INFO:root:current mean train loss 4366.348327123818
INFO:root:current train perplexity5.579617023468018
INFO:root:current mean train loss 4362.985237853168
INFO:root:current train perplexity5.591447830200195
INFO:root:current mean train loss 4360.33220957215
INFO:root:current train perplexity5.58964729309082
INFO:root:current mean train loss 4366.349716587112
INFO:root:current train perplexity5.597665786743164
INFO:root:current mean train loss 4364.237168269809
INFO:root:current train perplexity5.593801975250244
INFO:root:current mean train loss 4363.044249798061
INFO:root:current train perplexity5.593307018280029
INFO:root:current mean train loss 4366.173656649317
INFO:root:current train perplexity5.596815586090088
INFO:root:current mean train loss 4365.848688258357
INFO:root:current train perplexity5.597492694854736
INFO:root:current mean train loss 4366.044969693621
INFO:root:current train perplexity5.595256328582764

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.53s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.53s/it]
INFO:root:final mean train loss: 4366.519283540787
INFO:root:final train perplexity: 5.599730014801025
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.01s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.02s/it]
INFO:root:eval mean loss: 4249.471245082557
INFO:root:eval perplexity: 5.575385093688965
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/16
  8%|â–Š         | 16/200 [39:06<7:58:25, 156.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4362.456958912037
INFO:root:current train perplexity5.597385406494141
INFO:root:current mean train loss 4313.95378052719
INFO:root:current train perplexity5.532033920288086
INFO:root:current mean train loss 4326.94583304653
INFO:root:current train perplexity5.52913761138916
INFO:root:current mean train loss 4325.095809889861
INFO:root:current train perplexity5.521618366241455
INFO:root:current mean train loss 4331.000903949246
INFO:root:current train perplexity5.523467540740967
INFO:root:current mean train loss 4330.702974438894
INFO:root:current train perplexity5.517757892608643
INFO:root:current mean train loss 4331.409104537355
INFO:root:current train perplexity5.516617298126221
INFO:root:current mean train loss 4330.267480737405
INFO:root:current train perplexity5.521412372589111
INFO:root:current mean train loss 4330.210404641683
INFO:root:current train perplexity5.517709732055664
INFO:root:current mean train loss 4332.142523608161
INFO:root:current train perplexity5.522227764129639

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.67s/it]
INFO:root:final mean train loss: 4332.862497514294
INFO:root:final train perplexity: 5.525864124298096
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.45s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.45s/it]
INFO:root:eval mean loss: 4229.091822847407
INFO:root:eval perplexity: 5.529629230499268
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/17
  8%|â–Š         | 17/200 [41:46<7:59:43, 157.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4261.496065848214
INFO:root:current train perplexity5.420397758483887
INFO:root:current mean train loss 4298.236917679398
INFO:root:current train perplexity5.445066928863525
INFO:root:current mean train loss 4294.527752036237
INFO:root:current train perplexity5.451744079589844
INFO:root:current mean train loss 4296.920837220149
INFO:root:current train perplexity5.449408531188965
INFO:root:current mean train loss 4297.344754063398
INFO:root:current train perplexity5.454404354095459
INFO:root:current mean train loss 4308.103303884346
INFO:root:current train perplexity5.456583499908447
INFO:root:current mean train loss 4303.44390378937
INFO:root:current train perplexity5.44992733001709
INFO:root:current mean train loss 4302.066901174532
INFO:root:current train perplexity5.4464287757873535
INFO:root:current mean train loss 4303.07496403677
INFO:root:current train perplexity5.453981876373291
INFO:root:current mean train loss 4302.66295746992
INFO:root:current train perplexity5.457612991333008

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.88s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.88s/it]
INFO:root:final mean train loss: 4301.518136978149
INFO:root:final train perplexity: 5.457950592041016
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.33s/it]
INFO:root:eval mean loss: 4211.494620248781
INFO:root:eval perplexity: 5.490420341491699
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/18
  9%|â–‰         | 18/200 [44:36<8:08:37, 161.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4264.077233602834
INFO:root:current train perplexity5.395811557769775
INFO:root:current mean train loss 4284.223776223776
INFO:root:current train perplexity5.410559177398682
INFO:root:current mean train loss 4277.808036144869
INFO:root:current train perplexity5.401575088500977
INFO:root:current mean train loss 4284.188948472804
INFO:root:current train perplexity5.415416717529297
INFO:root:current mean train loss 4284.879424291055
INFO:root:current train perplexity5.411235809326172
INFO:root:current mean train loss 4285.238885531768
INFO:root:current train perplexity5.408998489379883
INFO:root:current mean train loss 4281.084673512223
INFO:root:current train perplexity5.402795791625977
INFO:root:current mean train loss 4277.565394857209
INFO:root:current train perplexity5.397912502288818
INFO:root:current mean train loss 4273.978056304678
INFO:root:current train perplexity5.395355701446533
INFO:root:current mean train loss 4272.969672711758
INFO:root:current train perplexity5.392422199249268

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:03<00:00, 123.18s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:03<00:00, 123.18s/it]
INFO:root:final mean train loss: 4272.072981742121
INFO:root:final train perplexity: 5.3949127197265625
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.78s/it]
INFO:root:eval mean loss: 4192.548784837655
INFO:root:eval perplexity: 5.448517799377441
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/19
 10%|â–‰         | 19/200 [46:51<7:42:07, 153.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4218.445575788909
INFO:root:current train perplexity5.262063980102539
INFO:root:current mean train loss 4214.584923750517
INFO:root:current train perplexity5.269303798675537
INFO:root:current mean train loss 4244.083377427789
INFO:root:current train perplexity5.29803991317749
INFO:root:current mean train loss 4236.0985924701745
INFO:root:current train perplexity5.309116840362549
INFO:root:current mean train loss 4241.832467563401
INFO:root:current train perplexity5.312972068786621
INFO:root:current mean train loss 4239.928504459222
INFO:root:current train perplexity5.31765079498291
INFO:root:current mean train loss 4247.402296496975
INFO:root:current train perplexity5.330323696136475
INFO:root:current mean train loss 4248.547904551743
INFO:root:current train perplexity5.331446647644043
INFO:root:current mean train loss 4248.3080925588465
INFO:root:current train perplexity5.334214687347412
INFO:root:current mean train loss 4250.576512542308
INFO:root:current train perplexity5.337281227111816

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.08s/it]
INFO:root:final mean train loss: 4244.150462181337
INFO:root:final train perplexity: 5.335806369781494
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.01s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.01s/it]
INFO:root:eval mean loss: 4176.7256707806955
INFO:root:eval perplexity: 5.413768291473389
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/20
 10%|â–ˆ         | 20/200 [49:03<7:20:22, 146.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4248.474079713983
INFO:root:current train perplexity5.282023906707764
INFO:root:current mean train loss 4239.8482566209705
INFO:root:current train perplexity5.277870178222656
INFO:root:current mean train loss 4232.372643430261
INFO:root:current train perplexity5.279345989227295
INFO:root:current mean train loss 4230.422283034471
INFO:root:current train perplexity5.288763523101807
INFO:root:current mean train loss 4226.085977392259
INFO:root:current train perplexity5.283273696899414
INFO:root:current mean train loss 4222.568995276163
INFO:root:current train perplexity5.279506206512451
INFO:root:current mean train loss 4224.932667201133
INFO:root:current train perplexity5.28139066696167
INFO:root:current mean train loss 4228.717622256876
INFO:root:current train perplexity5.290882110595703
INFO:root:current mean train loss 4226.718013599116
INFO:root:current train perplexity5.289740562438965
INFO:root:current mean train loss 4223.809261508977
INFO:root:current train perplexity5.285455226898193

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.19s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.19s/it]
INFO:root:final mean train loss: 4219.236140897197
INFO:root:final train perplexity: 5.283615589141846
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.94s/it]
INFO:root:eval mean loss: 4166.638090093085
INFO:root:eval perplexity: 5.391729354858398
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/21
 10%|â–ˆ         | 21/200 [51:15<7:04:39, 142.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4167.37743047458
INFO:root:current train perplexity5.2122039794921875
INFO:root:current mean train loss 4182.599983626497
INFO:root:current train perplexity5.235726356506348
INFO:root:current mean train loss 4188.738657061974
INFO:root:current train perplexity5.2271575927734375
INFO:root:current mean train loss 4186.3229987121085
INFO:root:current train perplexity5.218157768249512
INFO:root:current mean train loss 4191.609194116368
INFO:root:current train perplexity5.224910259246826
INFO:root:current mean train loss 4191.284487554426
INFO:root:current train perplexity5.220119953155518
INFO:root:current mean train loss 4193.226528825431
INFO:root:current train perplexity5.22152042388916
INFO:root:current mean train loss 4193.930619499674
INFO:root:current train perplexity5.221157550811768
INFO:root:current mean train loss 4195.459380857123
INFO:root:current train perplexity5.22218656539917
INFO:root:current mean train loss 4194.685783693931
INFO:root:current train perplexity5.22598123550415

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.70s/it]
INFO:root:final mean train loss: 4193.022917162987
INFO:root:final train perplexity: 5.229255199432373
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.07s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.07s/it]
INFO:root:eval mean loss: 4145.023937901707
INFO:root:eval perplexity: 5.3448100090026855
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/22
 11%|â–ˆ         | 22/200 [53:29<6:54:31, 139.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4182.3121484375
INFO:root:current train perplexity5.175649642944336
INFO:root:current mean train loss 4172.676085379464
INFO:root:current train perplexity5.182368278503418
INFO:root:current mean train loss 4164.684452237216
INFO:root:current train perplexity5.161734580993652
INFO:root:current mean train loss 4165.251482421875
INFO:root:current train perplexity5.169570446014404
INFO:root:current mean train loss 4167.127944078948
INFO:root:current train perplexity5.174609184265137
INFO:root:current mean train loss 4171.996541694973
INFO:root:current train perplexity5.17966890335083
INFO:root:current mean train loss 4170.025129846644
INFO:root:current train perplexity5.18198823928833
INFO:root:current mean train loss 4174.13865234375
INFO:root:current train perplexity5.184211730957031
INFO:root:current mean train loss 4174.364888950893
INFO:root:current train perplexity5.184054851531982
INFO:root:current mean train loss 4176.35219926883
INFO:root:current train perplexity5.185505390167236

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.42s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.42s/it]
INFO:root:final mean train loss: 4171.279148901663
INFO:root:final train perplexity: 5.184587001800537
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.85s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.85s/it]
INFO:root:eval mean loss: 4137.642775515293
INFO:root:eval perplexity: 5.328880786895752
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/23
 12%|â–ˆâ–        | 23/200 [56:05<7:06:41, 144.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4153.992234563253
INFO:root:current train perplexity5.098073482513428
INFO:root:current mean train loss 4150.553537504269
INFO:root:current train perplexity5.118495941162109
INFO:root:current mean train loss 4150.56181502595
INFO:root:current train perplexity5.1205363273620605
INFO:root:current mean train loss 4155.6923872746
INFO:root:current train perplexity5.128908157348633
INFO:root:current mean train loss 4158.319795407124
INFO:root:current train perplexity5.1360087394714355
INFO:root:current mean train loss 4151.4249801504875
INFO:root:current train perplexity5.1290059089660645
INFO:root:current mean train loss 4146.080881680088
INFO:root:current train perplexity5.122599124908447
INFO:root:current mean train loss 4151.611918365362
INFO:root:current train perplexity5.134128093719482
INFO:root:current mean train loss 4150.8255218471295
INFO:root:current train perplexity5.137011528015137
INFO:root:current mean train loss 4151.534930984947
INFO:root:current train perplexity5.138066291809082

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:03<00:00, 123.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:03<00:00, 123.69s/it]
INFO:root:final mean train loss: 4148.335625802317
INFO:root:final train perplexity: 5.137868881225586
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.34s/it]
INFO:root:eval mean loss: 4128.131408258533
INFO:root:eval perplexity: 5.3084259033203125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/24
 12%|â–ˆâ–        | 24/200 [59:06<7:36:45, 155.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4098.686233688187
INFO:root:current train perplexity5.058389663696289
INFO:root:current mean train loss 4113.106440199607
INFO:root:current train perplexity5.079925060272217
INFO:root:current mean train loss 4110.7062828876715
INFO:root:current train perplexity5.069431304931641
INFO:root:current mean train loss 4121.915689687899
INFO:root:current train perplexity5.089799404144287
INFO:root:current mean train loss 4122.486632430626
INFO:root:current train perplexity5.093495845794678
INFO:root:current mean train loss 4128.16181318409
INFO:root:current train perplexity5.096872806549072
INFO:root:current mean train loss 4130.395354473815
INFO:root:current train perplexity5.099554061889648
INFO:root:current mean train loss 4134.311119725821
INFO:root:current train perplexity5.100337505340576
INFO:root:current mean train loss 4132.802844799997
INFO:root:current train perplexity5.1023268699646
INFO:root:current mean train loss 4131.728471280588
INFO:root:current train perplexity5.097721576690674

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:03<00:00, 123.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:03<00:00, 123.34s/it]
INFO:root:final mean train loss: 4128.393979595554
INFO:root:final train perplexity: 5.0976057052612305
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.14s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.14s/it]
INFO:root:eval mean loss: 4115.965636774158
INFO:root:eval perplexity: 5.282374858856201
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/25
 12%|â–ˆâ–Ž        | 25/200 [1:01:51<7:42:21, 158.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4084.1961312342173
INFO:root:current train perplexity5.070169448852539
INFO:root:current mean train loss 4094.2216944095476
INFO:root:current train perplexity5.040805816650391
INFO:root:current mean train loss 4107.848509517402
INFO:root:current train perplexity5.0515899658203125
INFO:root:current mean train loss 4113.831298216243
INFO:root:current train perplexity5.0607500076293945
INFO:root:current mean train loss 4112.685139810872
INFO:root:current train perplexity5.05646276473999
INFO:root:current mean train loss 4110.668600907111
INFO:root:current train perplexity5.050352573394775
INFO:root:current mean train loss 4111.132163903232
INFO:root:current train perplexity5.052043437957764
INFO:root:current mean train loss 4114.102820786726
INFO:root:current train perplexity5.054693698883057
INFO:root:current mean train loss 4113.744520278608
INFO:root:current train perplexity5.058591842651367

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.13s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.13s/it]
INFO:root:final mean train loss: 4107.993347660188
INFO:root:final train perplexity: 5.056741237640381
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.50s/it]
INFO:root:eval mean loss: 4109.272658327793
INFO:root:eval perplexity: 5.268097400665283
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/26
 13%|â–ˆâ–Ž        | 26/200 [1:04:49<7:56:23, 164.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4156.533586774553
INFO:root:current train perplexity5.133966445922852
INFO:root:current mean train loss 4081.9818172276578
INFO:root:current train perplexity5.024381160736084
INFO:root:current mean train loss 4090.9994716183573
INFO:root:current train perplexity5.031797409057617
INFO:root:current mean train loss 4094.083702062551
INFO:root:current train perplexity5.030238628387451
INFO:root:current mean train loss 4101.026232220324
INFO:root:current train perplexity5.041232585906982
INFO:root:current mean train loss 4091.9119572084564
INFO:root:current train perplexity5.028777122497559
INFO:root:current mean train loss 4094.705855594239
INFO:root:current train perplexity5.022490978240967
INFO:root:current mean train loss 4095.4032578097376
INFO:root:current train perplexity5.025001525878906
INFO:root:current mean train loss 4093.155654018549
INFO:root:current train perplexity5.02170991897583
INFO:root:current mean train loss 4090.620281652684
INFO:root:current train perplexity5.017849922180176

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.04s/it]
INFO:root:final mean train loss: 4088.788524504631
INFO:root:final train perplexity: 5.018570899963379
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.32s/it]
INFO:root:eval mean loss: 4097.669766040559
INFO:root:eval perplexity: 5.243438243865967
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/27
 14%|â–ˆâ–Ž        | 27/200 [1:07:07<7:31:22, 156.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4101.456022135417
INFO:root:current train perplexity4.951620578765869
INFO:root:current mean train loss 4052.5315854279893
INFO:root:current train perplexity4.964480876922607
INFO:root:current mean train loss 4050.7411428052324
INFO:root:current train perplexity4.962126731872559
INFO:root:current mean train loss 4064.3500550285216
INFO:root:current train perplexity4.976630210876465
INFO:root:current mean train loss 4073.728006165286
INFO:root:current train perplexity4.988111972808838
INFO:root:current mean train loss 4068.640465716019
INFO:root:current train perplexity4.9782023429870605
INFO:root:current mean train loss 4071.3294123951982
INFO:root:current train perplexity4.979063987731934
INFO:root:current mean train loss 4072.377171315013
INFO:root:current train perplexity4.97818660736084
INFO:root:current mean train loss 4075.2608134825537
INFO:root:current train perplexity4.98085355758667
INFO:root:current mean train loss 4072.4623113580087
INFO:root:current train perplexity4.9800944328308105

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.50s/it]
INFO:root:final mean train loss: 4071.358767847861
INFO:root:final train perplexity: 4.984179973602295
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.21s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.21s/it]
INFO:root:eval mean loss: 4089.550249681405
INFO:root:eval perplexity: 5.226250171661377
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/28
 14%|â–ˆâ–        | 28/200 [1:10:06<7:47:26, 163.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4090.513767408288
INFO:root:current train perplexity4.970640659332275
INFO:root:current mean train loss 4056.1368040999746
INFO:root:current train perplexity4.933033466339111
INFO:root:current mean train loss 4063.849459387262
INFO:root:current train perplexity4.955777168273926
INFO:root:current mean train loss 4054.206204346459
INFO:root:current train perplexity4.946417331695557
INFO:root:current mean train loss 4066.1665546967347
INFO:root:current train perplexity4.956026554107666
INFO:root:current mean train loss 4066.8315569729925
INFO:root:current train perplexity4.95504093170166
INFO:root:current mean train loss 4059.7048923743478
INFO:root:current train perplexity4.950775623321533
INFO:root:current mean train loss 4057.0395230917184
INFO:root:current train perplexity4.9502153396606445
INFO:root:current mean train loss 4059.7156565632595
INFO:root:current train perplexity4.951235771179199
INFO:root:current mean train loss 4057.7329485098694
INFO:root:current train perplexity4.950575351715088

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.87s/it]
INFO:root:final mean train loss: 4053.4510635868196
INFO:root:final train perplexity: 4.949089527130127
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.25s/it]
INFO:root:eval mean loss: 4084.528166209552
INFO:root:eval perplexity: 5.215647220611572
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/29
 14%|â–ˆâ–        | 29/200 [1:12:58<7:52:48, 165.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4061.7904879662296
INFO:root:current train perplexity4.909974575042725
INFO:root:current mean train loss 4042.8358256798665
INFO:root:current train perplexity4.905316352844238
INFO:root:current mean train loss 4057.8779804180194
INFO:root:current train perplexity4.918928146362305
INFO:root:current mean train loss 4059.6543846476116
INFO:root:current train perplexity4.918290615081787
INFO:root:current mean train loss 4056.6271751740137
INFO:root:current train perplexity4.913795471191406
INFO:root:current mean train loss 4052.464264893038
INFO:root:current train perplexity4.915053844451904
INFO:root:current mean train loss 4046.8711452091175
INFO:root:current train perplexity4.911036014556885
INFO:root:current mean train loss 4046.850711180639
INFO:root:current train perplexity4.914429187774658
INFO:root:current mean train loss 4043.7117189850333
INFO:root:current train perplexity4.911149501800537
INFO:root:current mean train loss 4038.5610325339017
INFO:root:current train perplexity4.913117408752441

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.69s/it]
INFO:root:final mean train loss: 4035.513228016515
INFO:root:final train perplexity: 4.914188861846924
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.97s/it]
INFO:root:eval mean loss: 4075.1316143062945
INFO:root:eval perplexity: 5.195867538452148
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/30
 15%|â–ˆâ–Œ        | 30/200 [1:15:10<7:20:48, 155.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4021.106457832532
INFO:root:current train perplexity4.905718803405762
INFO:root:current mean train loss 4015.562819666142
INFO:root:current train perplexity4.889692306518555
INFO:root:current mean train loss 4019.022349593031
INFO:root:current train perplexity4.873134613037109
INFO:root:current mean train loss 4015.1418788313513
INFO:root:current train perplexity4.8699822425842285
INFO:root:current mean train loss 4021.833907073071
INFO:root:current train perplexity4.873605251312256
INFO:root:current mean train loss 4019.993781434804
INFO:root:current train perplexity4.875936985015869
INFO:root:current mean train loss 4019.2778117817147
INFO:root:current train perplexity4.877307891845703
INFO:root:current mean train loss 4024.0094203923163
INFO:root:current train perplexity4.881368160247803
INFO:root:current mean train loss 4023.96126253585
INFO:root:current train perplexity4.884831428527832
INFO:root:current mean train loss 4023.0813310682074
INFO:root:current train perplexity4.884538650512695

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.08s/it]
INFO:root:final mean train loss: 4019.523250272197
INFO:root:final train perplexity: 4.883285045623779
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.96s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.96s/it]
INFO:root:eval mean loss: 4067.4494490386746
INFO:root:eval perplexity: 5.179751873016357
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/31
 16%|â–ˆâ–Œ        | 31/200 [1:17:22<6:58:09, 148.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3993.85043529754
INFO:root:current train perplexity4.8252949714660645
INFO:root:current mean train loss 3992.731324072598
INFO:root:current train perplexity4.822100639343262
INFO:root:current mean train loss 3985.6318675670545
INFO:root:current train perplexity4.81026029586792
INFO:root:current mean train loss 3997.412587806196
INFO:root:current train perplexity4.825476169586182
INFO:root:current mean train loss 4001.874636246854
INFO:root:current train perplexity4.833906173706055
INFO:root:current mean train loss 4003.2220679916018
INFO:root:current train perplexity4.842909812927246
INFO:root:current mean train loss 4007.3460616004154
INFO:root:current train perplexity4.848858833312988
INFO:root:current mean train loss 4002.727046532484
INFO:root:current train perplexity4.848857879638672
INFO:root:current mean train loss 4005.1977605358065
INFO:root:current train perplexity4.852702617645264
INFO:root:current mean train loss 4005.1922732456937
INFO:root:current train perplexity4.851742267608643

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.13s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.13s/it]
INFO:root:final mean train loss: 4003.889027503229
INFO:root:final train perplexity: 4.853257179260254
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.33s/it]
INFO:root:eval mean loss: 4061.0687680075353
INFO:root:eval perplexity: 5.1664042472839355
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/32
 16%|â–ˆâ–Œ        | 32/200 [1:19:35<6:42:56, 143.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3980.3018022017045
INFO:root:current train perplexity4.7472758293151855
INFO:root:current mean train loss 4001.9681766633066
INFO:root:current train perplexity4.809282302856445
INFO:root:current mean train loss 3982.0927399280026
INFO:root:current train perplexity4.799716949462891
INFO:root:current mean train loss 3975.5717629016285
INFO:root:current train perplexity4.806110858917236
INFO:root:current mean train loss 3985.344937972184
INFO:root:current train perplexity4.817925453186035
INFO:root:current mean train loss 3986.231975383587
INFO:root:current train perplexity4.81803035736084
INFO:root:current mean train loss 3990.359147632395
INFO:root:current train perplexity4.819870948791504
INFO:root:current mean train loss 3992.1634303212954
INFO:root:current train perplexity4.820527076721191
INFO:root:current mean train loss 3992.680317411367
INFO:root:current train perplexity4.820824146270752
INFO:root:current mean train loss 3992.0307934186844
INFO:root:current train perplexity4.8244757652282715

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.46s/it]
INFO:root:final mean train loss: 3987.9268331220073
INFO:root:final train perplexity: 4.822789669036865
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.34s/it]
INFO:root:eval mean loss: 4053.7100267342644
INFO:root:eval perplexity: 5.151054859161377
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/33
 16%|â–ˆâ–‹        | 33/200 [1:21:47<6:30:22, 140.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3920.6708054315477
INFO:root:current train perplexity4.7393975257873535
INFO:root:current mean train loss 3951.612896316622
INFO:root:current train perplexity4.783961772918701
INFO:root:current mean train loss 3950.9638968928234
INFO:root:current train perplexity4.783755779266357
INFO:root:current mean train loss 3960.691873681775
INFO:root:current train perplexity4.784818172454834
INFO:root:current mean train loss 3970.191475326505
INFO:root:current train perplexity4.787414073944092
INFO:root:current mean train loss 3966.728814404557
INFO:root:current train perplexity4.781184196472168
INFO:root:current mean train loss 3969.9607644289686
INFO:root:current train perplexity4.788547992706299
INFO:root:current mean train loss 3968.035765161677
INFO:root:current train perplexity4.784548282623291
INFO:root:current mean train loss 3969.7460982763614
INFO:root:current train perplexity4.785581588745117
INFO:root:current mean train loss 3977.0541655004704
INFO:root:current train perplexity4.796242713928223

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.84s/it]
INFO:root:final mean train loss: 3973.847779058641
INFO:root:final train perplexity: 4.796075820922852
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.29s/it]
INFO:root:eval mean loss: 4048.537495498116
INFO:root:eval perplexity: 5.140291213989258
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/34
 17%|â–ˆâ–‹        | 34/200 [1:23:58<6:20:22, 137.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3990.4331983109596
INFO:root:current train perplexity4.770908832550049
INFO:root:current mean train loss 3956.4094509548613
INFO:root:current train perplexity4.751121520996094
INFO:root:current mean train loss 3952.006802604647
INFO:root:current train perplexity4.753298282623291
INFO:root:current mean train loss 3953.727660145721
INFO:root:current train perplexity4.747738838195801
INFO:root:current mean train loss 3953.5466997992967
INFO:root:current train perplexity4.748318195343018
INFO:root:current mean train loss 3957.950535228081
INFO:root:current train perplexity4.758653163909912
INFO:root:current mean train loss 3960.7481811341513
INFO:root:current train perplexity4.763347148895264
INFO:root:current mean train loss 3959.313895496413
INFO:root:current train perplexity4.7622480392456055
INFO:root:current mean train loss 3963.060219205206
INFO:root:current train perplexity4.766449928283691
INFO:root:current mean train loss 3961.8998281712634
INFO:root:current train perplexity4.768165111541748

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.94s/it]
INFO:root:final mean train loss: 3959.176769564229
INFO:root:final train perplexity: 4.76839542388916
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00, 10.00s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00, 10.00s/it]
INFO:root:eval mean loss: 4047.099666514295
INFO:root:eval perplexity: 5.137303829193115
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/35
 18%|â–ˆâ–Š        | 35/200 [1:26:09<6:13:22, 135.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3969.589581067049
INFO:root:current train perplexity4.759267807006836
INFO:root:current mean train loss 3949.4926539586245
INFO:root:current train perplexity4.72876501083374
INFO:root:current mean train loss 3953.4564519629257
INFO:root:current train perplexity4.741334438323975
INFO:root:current mean train loss 3951.108835185109
INFO:root:current train perplexity4.737835884094238
INFO:root:current mean train loss 3949.1069320646857
INFO:root:current train perplexity4.740106105804443
INFO:root:current mean train loss 3952.240283287457
INFO:root:current train perplexity4.739037036895752
INFO:root:current mean train loss 3952.4513642391153
INFO:root:current train perplexity4.742079734802246
INFO:root:current mean train loss 3950.0384247257102
INFO:root:current train perplexity4.7416510581970215
INFO:root:current mean train loss 3947.8958383328
INFO:root:current train perplexity4.73964262008667
INFO:root:current mean train loss 3947.69052245595
INFO:root:current train perplexity4.741596221923828

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:03<00:00, 123.86s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:03<00:00, 123.86s/it]
INFO:root:final mean train loss: 3944.674160803518
INFO:root:final train perplexity: 4.741189956665039
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.24s/it]
INFO:root:eval mean loss: 4043.1696898548316
INFO:root:eval perplexity: 5.129144668579102
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/36
 18%|â–ˆâ–Š        | 36/200 [1:28:24<6:10:24, 135.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3900.79144216954
INFO:root:current train perplexity4.69246768951416
INFO:root:current mean train loss 3909.5836958451705
INFO:root:current train perplexity4.687325477600098
INFO:root:current mean train loss 3920.2081039375544
INFO:root:current train perplexity4.694545745849609
INFO:root:current mean train loss 3925.1810709635415
INFO:root:current train perplexity4.703031063079834
INFO:root:current mean train loss 3921.327757535774
INFO:root:current train perplexity4.705887794494629
INFO:root:current mean train loss 3923.1248486078575
INFO:root:current train perplexity4.709649085998535
INFO:root:current mean train loss 3928.2945625938182
INFO:root:current train perplexity4.71237325668335
INFO:root:current mean train loss 3931.2991017362215
INFO:root:current train perplexity4.714832782745361
INFO:root:current mean train loss 3932.570537373608
INFO:root:current train perplexity4.71317195892334
INFO:root:current mean train loss 3934.876978602694
INFO:root:current train perplexity4.7178544998168945

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.39s/it]
INFO:root:final mean train loss: 3932.031881209343
INFO:root:final train perplexity: 4.717600345611572
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.32s/it]
INFO:root:eval mean loss: 4034.798525113586
INFO:root:eval perplexity: 5.111813068389893
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/37
 18%|â–ˆâ–Š        | 37/200 [1:30:38<6:06:30, 134.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3896.110521175987
INFO:root:current train perplexity4.65042781829834
INFO:root:current mean train loss 3896.927647986779
INFO:root:current train perplexity4.657935619354248
INFO:root:current mean train loss 3904.9569650423728
INFO:root:current train perplexity4.672021865844727
INFO:root:current mean train loss 3897.497211234177
INFO:root:current train perplexity4.671946048736572
INFO:root:current mean train loss 3903.555839646465
INFO:root:current train perplexity4.676270961761475
INFO:root:current mean train loss 3908.2871651785713
INFO:root:current train perplexity4.680466175079346
INFO:root:current mean train loss 3909.4137892030126
INFO:root:current train perplexity4.681280612945557
INFO:root:current mean train loss 3915.682528744104
INFO:root:current train perplexity4.687510967254639
INFO:root:current mean train loss 3921.238696425454
INFO:root:current train perplexity4.691625118255615

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:05<00:00, 125.88s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:05<00:00, 125.88s/it]
INFO:root:final mean train loss: 3918.0911902766074
INFO:root:final train perplexity: 4.691725254058838
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:11<00:00, 11.03s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:11<00:00, 11.03s/it]
INFO:root:eval mean loss: 4033.7096890929743
INFO:root:eval perplexity: 5.109562397003174
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/38
 19%|â–ˆâ–‰        | 38/200 [1:32:56<6:06:33, 135.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3837.0952962239585
INFO:root:current train perplexity4.501226425170898
INFO:root:current mean train loss 3908.8704585103155
INFO:root:current train perplexity4.654051303863525
INFO:root:current mean train loss 3901.481863839286
INFO:root:current train perplexity4.650881767272949
INFO:root:current mean train loss 3904.1242683838695
INFO:root:current train perplexity4.651206970214844
INFO:root:current mean train loss 3899.934616353908
INFO:root:current train perplexity4.648334503173828
INFO:root:current mean train loss 3901.8425244431846
INFO:root:current train perplexity4.647347927093506
INFO:root:current mean train loss 3906.484446258292
INFO:root:current train perplexity4.655158042907715
INFO:root:current mean train loss 3904.8878863186787
INFO:root:current train perplexity4.660464286804199
INFO:root:current mean train loss 3907.3921731495175
INFO:root:current train perplexity4.662489414215088
INFO:root:current mean train loss 3909.5567704007476
INFO:root:current train perplexity4.665846824645996

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.16s/it]
INFO:root:final mean train loss: 3906.488056798135
INFO:root:final train perplexity: 4.6702961921691895
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.66s/it]
INFO:root:eval mean loss: 4030.478070631095
INFO:root:eval perplexity: 5.102890491485596
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/39
 20%|â–ˆâ–‰        | 39/200 [1:35:08<6:01:49, 134.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3843.951970880682
INFO:root:current train perplexity4.573243141174316
INFO:root:current mean train loss 3872.476001636402
INFO:root:current train perplexity4.606324672698975
INFO:root:current mean train loss 3883.7728311981637
INFO:root:current train perplexity4.616320610046387
INFO:root:current mean train loss 3884.9894956729804
INFO:root:current train perplexity4.632570266723633
INFO:root:current mean train loss 3887.4746503621122
INFO:root:current train perplexity4.639005661010742
INFO:root:current mean train loss 3892.3217782992906
INFO:root:current train perplexity4.64169979095459
INFO:root:current mean train loss 3891.260061710439
INFO:root:current train perplexity4.63870096206665
INFO:root:current mean train loss 3894.096173894053
INFO:root:current train perplexity4.6447649002075195
INFO:root:current mean train loss 3894.6788912583806
INFO:root:current train perplexity4.64381217956543
INFO:root:current mean train loss 3896.3846410087644
INFO:root:current train perplexity4.646533489227295

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:04<00:00, 124.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:04<00:00, 124.34s/it]
INFO:root:final mean train loss: 3893.9874237429713
INFO:root:final train perplexity: 4.647319793701172
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.23s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.23s/it]
INFO:root:eval mean loss: 4026.048779643174
INFO:root:eval perplexity: 5.0937581062316895
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/40
 20%|â–ˆâ–ˆ        | 40/200 [1:37:24<6:00:02, 135.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3889.501824629934
INFO:root:current train perplexity4.633744239807129
INFO:root:current mean train loss 3879.7631815421482
INFO:root:current train perplexity4.624587535858154
INFO:root:current mean train loss 3875.84721033105
INFO:root:current train perplexity4.618241786956787
INFO:root:current mean train loss 3885.037182846787
INFO:root:current train perplexity4.620582103729248
INFO:root:current mean train loss 3884.6548038018345
INFO:root:current train perplexity4.621326446533203
INFO:root:current mean train loss 3888.1745313817137
INFO:root:current train perplexity4.624856472015381
INFO:root:current mean train loss 3886.7946296161904
INFO:root:current train perplexity4.623349189758301
INFO:root:current mean train loss 3891.4628067547155
INFO:root:current train perplexity4.6305646896362305
INFO:root:current mean train loss 3888.1520098824785
INFO:root:current train perplexity4.627039909362793
INFO:root:current mean train loss 3884.884463039394
INFO:root:current train perplexity4.62601900100708

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.62s/it]
INFO:root:final mean train loss: 3881.638213065363
INFO:root:final train perplexity: 4.624733924865723
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.39s/it]
INFO:root:eval mean loss: 4020.7952196919327
INFO:root:eval perplexity: 5.082947731018066
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/41
 20%|â–ˆâ–ˆ        | 41/200 [1:39:37<5:56:51, 134.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3828.7077907986113
INFO:root:current train perplexity4.582977294921875
INFO:root:current mean train loss 3852.1091673843503
INFO:root:current train perplexity4.574707508087158
INFO:root:current mean train loss 3857.161668416162
INFO:root:current train perplexity4.58614444732666
INFO:root:current mean train loss 3858.8464616781343
INFO:root:current train perplexity4.581302165985107
INFO:root:current mean train loss 3860.6889333970653
INFO:root:current train perplexity4.588742256164551
INFO:root:current mean train loss 3861.4869669673567
INFO:root:current train perplexity4.585146903991699
INFO:root:current mean train loss 3865.012394711922
INFO:root:current train perplexity4.586765766143799
INFO:root:current mean train loss 3870.035240204823
INFO:root:current train perplexity4.595072269439697
INFO:root:current mean train loss 3874.946826644215
INFO:root:current train perplexity4.60252571105957
INFO:root:current mean train loss 3874.5306732276495
INFO:root:current train perplexity4.603171348571777

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.51s/it]
INFO:root:final mean train loss: 3870.8162056707565
INFO:root:final train perplexity: 4.605029106140137
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.31s/it]
INFO:root:eval mean loss: 4017.9889738475176
INFO:root:eval perplexity: 5.077184677124023
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/42
 21%|â–ˆâ–ˆ        | 42/200 [1:41:50<5:53:00, 134.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3799.539076450893
INFO:root:current train perplexity4.546058177947998
INFO:root:current mean train loss 3822.538991970486
INFO:root:current train perplexity4.556359767913818
INFO:root:current mean train loss 3840.3802931765294
INFO:root:current train perplexity4.559868812561035
INFO:root:current mean train loss 3852.623288829291
INFO:root:current train perplexity4.573493480682373
INFO:root:current mean train loss 3855.8052189969467
INFO:root:current train perplexity4.573955535888672
INFO:root:current mean train loss 3858.5076929395445
INFO:root:current train perplexity4.576744079589844
INFO:root:current mean train loss 3858.2083253875494
INFO:root:current train perplexity4.579168319702148
INFO:root:current mean train loss 3862.3686633051657
INFO:root:current train perplexity4.58079719543457
INFO:root:current mean train loss 3859.621096966224
INFO:root:current train perplexity4.5785956382751465
INFO:root:current mean train loss 3859.6421436330215
INFO:root:current train perplexity4.579168319702148

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.36s/it]
INFO:root:final mean train loss: 3858.4783765731318
INFO:root:final train perplexity: 4.582667827606201
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.04s/it]
INFO:root:eval mean loss: 4013.0858128324467
INFO:root:eval perplexity: 5.067127227783203
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/43
 22%|â–ˆâ–ˆâ–       | 43/200 [1:44:01<5:48:32, 133.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3889.7505280250725
INFO:root:current train perplexity4.5704569816589355
INFO:root:current mean train loss 3865.073366818728
INFO:root:current train perplexity4.5703349113464355
INFO:root:current mean train loss 3854.839610661008
INFO:root:current train perplexity4.549989700317383
INFO:root:current mean train loss 3852.276966506469
INFO:root:current train perplexity4.551497936248779
INFO:root:current mean train loss 3852.353930608952
INFO:root:current train perplexity4.549787521362305
INFO:root:current mean train loss 3853.411436751842
INFO:root:current train perplexity4.555547714233398
INFO:root:current mean train loss 3852.1774628967
INFO:root:current train perplexity4.555774211883545
INFO:root:current mean train loss 3849.177829665419
INFO:root:current train perplexity4.556549549102783
INFO:root:current mean train loss 3850.2116389336816
INFO:root:current train perplexity4.560826301574707
INFO:root:current mean train loss 3850.04164863012
INFO:root:current train perplexity4.561956405639648

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:03<00:00, 123.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:03<00:00, 123.61s/it]
INFO:root:final mean train loss: 3847.0203610081826
INFO:root:final train perplexity: 4.5619988441467285
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.43s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.43s/it]
INFO:root:eval mean loss: 4016.257828083444
INFO:root:eval perplexity: 5.073631286621094
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/44
 22%|â–ˆâ–ˆâ–       | 44/200 [1:46:16<5:47:36, 133.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3790.4658346737133
INFO:root:current train perplexity4.498288154602051
INFO:root:current mean train loss 3825.6445474182533
INFO:root:current train perplexity4.513579368591309
INFO:root:current mean train loss 3828.865973605578
INFO:root:current train perplexity4.524722099304199
INFO:root:current mean train loss 3828.434218360488
INFO:root:current train perplexity4.524978160858154
INFO:root:current mean train loss 3834.8916048104907
INFO:root:current train perplexity4.525389671325684
INFO:root:current mean train loss 3838.2875706279774
INFO:root:current train perplexity4.5346174240112305
INFO:root:current mean train loss 3834.0307155907976
INFO:root:current train perplexity4.533124923706055
INFO:root:current mean train loss 3836.675695426931
INFO:root:current train perplexity4.536888599395752
INFO:root:current mean train loss 3835.3444462741445
INFO:root:current train perplexity4.538074016571045
INFO:root:current mean train loss 3837.750537314751
INFO:root:current train perplexity4.539434909820557

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.83s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.83s/it]
INFO:root:final mean train loss: 3835.6243944475727
INFO:root:final train perplexity: 4.541533946990967
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.97s/it]
INFO:root:eval mean loss: 4009.0812641982493
INFO:root:eval perplexity: 5.058929443359375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/45
 22%|â–ˆâ–ˆâ–Ž       | 45/200 [1:48:58<6:07:10, 142.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3821.1738488148835
INFO:root:current train perplexity4.4904985427856445
INFO:root:current mean train loss 3829.07645286704
INFO:root:current train perplexity4.520165920257568
INFO:root:current mean train loss 3825.381372164575
INFO:root:current train perplexity4.521265506744385
INFO:root:current mean train loss 3822.808022501741
INFO:root:current train perplexity4.51796817779541
INFO:root:current mean train loss 3823.0895948223038
INFO:root:current train perplexity4.519954681396484
INFO:root:current mean train loss 3824.014444914188
INFO:root:current train perplexity4.519679546356201
INFO:root:current mean train loss 3828.9571249792534
INFO:root:current train perplexity4.522195816040039
INFO:root:current mean train loss 3831.99605804564
INFO:root:current train perplexity4.520947456359863
INFO:root:current mean train loss 3829.5279329275504
INFO:root:current train perplexity4.522669792175293
INFO:root:current mean train loss 3828.673952359228
INFO:root:current train perplexity4.522696018218994

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:03<00:00, 123.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:03<00:00, 123.87s/it]
INFO:root:final mean train loss: 3824.904566487958
INFO:root:final train perplexity: 4.522366046905518
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.46s/it]
INFO:root:eval mean loss: 4006.436530363475
INFO:root:eval perplexity: 5.053521633148193
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/46
 23%|â–ˆâ–ˆâ–Ž       | 46/200 [1:51:20<6:04:20, 141.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3832.150929920709
INFO:root:current train perplexity4.504222393035889
INFO:root:current mean train loss 3821.2275960773763
INFO:root:current train perplexity4.483760356903076
INFO:root:current mean train loss 3814.7167282961727
INFO:root:current train perplexity4.488955497741699
INFO:root:current mean train loss 3820.0425396745145
INFO:root:current train perplexity4.4970703125
INFO:root:current mean train loss 3825.2212452531116
INFO:root:current train perplexity4.501388072967529
INFO:root:current mean train loss 3822.7775917658732
INFO:root:current train perplexity4.5008440017700195
INFO:root:current mean train loss 3823.9543764494706
INFO:root:current train perplexity4.507307529449463
INFO:root:current mean train loss 3819.967331947217
INFO:root:current train perplexity4.504088878631592
INFO:root:current mean train loss 3819.3605650095515
INFO:root:current train perplexity4.5043792724609375
INFO:root:current mean train loss 3818.0279438764383
INFO:root:current train perplexity4.503629207611084

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.64s/it]
INFO:root:final mean train loss: 3814.176294942056
INFO:root:final train perplexity: 4.503266334533691
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.07s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.07s/it]
INFO:root:eval mean loss: 4006.823275085882
INFO:root:eval perplexity: 5.054311275482178
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/47
 24%|â–ˆâ–ˆâ–Ž       | 47/200 [1:53:31<5:54:01, 138.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3802.6961067708335
INFO:root:current train perplexity4.429920673370361
INFO:root:current mean train loss 3795.7084933035712
INFO:root:current train perplexity4.4673614501953125
INFO:root:current mean train loss 3796.506602450284
INFO:root:current train perplexity4.467066287994385
INFO:root:current mean train loss 3804.26473828125
INFO:root:current train perplexity4.4782514572143555
INFO:root:current mean train loss 3804.3493482730264
INFO:root:current train perplexity4.48075532913208
INFO:root:current mean train loss 3806.5452632472825
INFO:root:current train perplexity4.482807636260986
INFO:root:current mean train loss 3805.7013158275463
INFO:root:current train perplexity4.484141826629639
INFO:root:current mean train loss 3805.6182566784273
INFO:root:current train perplexity4.487558841705322
INFO:root:current mean train loss 3806.748618303571
INFO:root:current train perplexity4.489017963409424
INFO:root:current mean train loss 3807.4862189503206
INFO:root:current train perplexity4.487229347229004

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.91s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.91s/it]
INFO:root:final mean train loss: 3804.89218102732
INFO:root:final train perplexity: 4.4868011474609375
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.87s/it]
INFO:root:eval mean loss: 4002.317973251884
INFO:root:eval perplexity: 5.045112133026123
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/48
 24%|â–ˆâ–ˆâ–       | 48/200 [1:55:44<5:46:57, 136.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3801.564997293863
INFO:root:current train perplexity4.449154376983643
INFO:root:current mean train loss 3803.153909985485
INFO:root:current train perplexity4.446854591369629
INFO:root:current mean train loss 3789.9434827393443
INFO:root:current train perplexity4.441324234008789
INFO:root:current mean train loss 3788.710410334734
INFO:root:current train perplexity4.453545570373535
INFO:root:current mean train loss 3788.9682061173653
INFO:root:current train perplexity4.4574055671691895
INFO:root:current mean train loss 3793.7458064764687
INFO:root:current train perplexity4.453429698944092
INFO:root:current mean train loss 3795.024137393622
INFO:root:current train perplexity4.457246780395508
INFO:root:current mean train loss 3794.005965387532
INFO:root:current train perplexity4.460577011108398
INFO:root:current mean train loss 3794.4833033249574
INFO:root:current train perplexity4.462174892425537
INFO:root:current mean train loss 3797.8019930120645
INFO:root:current train perplexity4.470134735107422

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.33s/it]
INFO:root:final mean train loss: 3795.2586405969437
INFO:root:final train perplexity: 4.469780445098877
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.29s/it]
INFO:root:eval mean loss: 4002.2939678219195
INFO:root:eval perplexity: 5.0450639724731445
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/49
 24%|â–ˆâ–ˆâ–       | 49/200 [1:57:55<5:40:30, 135.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3787.689702631353
INFO:root:current train perplexity4.403323650360107
INFO:root:current mean train loss 3782.186945251145
INFO:root:current train perplexity4.41900110244751
INFO:root:current mean train loss 3772.825237261061
INFO:root:current train perplexity4.425587177276611
INFO:root:current mean train loss 3777.0457111173273
INFO:root:current train perplexity4.431817531585693
INFO:root:current mean train loss 3778.01388270112
INFO:root:current train perplexity4.4353156089782715
INFO:root:current mean train loss 3781.030475029082
INFO:root:current train perplexity4.439081192016602
INFO:root:current mean train loss 3778.970898861478
INFO:root:current train perplexity4.437074184417725
INFO:root:current mean train loss 3781.9282235821943
INFO:root:current train perplexity4.44044303894043
INFO:root:current mean train loss 3781.18287256243
INFO:root:current train perplexity4.4415998458862305
INFO:root:current mean train loss 3786.139016529626
INFO:root:current train perplexity4.448570251464844

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.87s/it]
INFO:root:final mean train loss: 3783.262984183527
INFO:root:final train perplexity: 4.448676586151123
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.06s/it]
INFO:root:eval mean loss: 4001.2161112034573
INFO:root:eval perplexity: 5.04286527633667
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/50
 25%|â–ˆâ–ˆâ–Œ       | 50/200 [2:00:09<5:37:05, 134.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3761.236234414457
INFO:root:current train perplexity4.383851051330566
INFO:root:current mean train loss 3768.0697395669754
INFO:root:current train perplexity4.402951240539551
INFO:root:current mean train loss 3771.239952674279
INFO:root:current train perplexity4.412197113037109
INFO:root:current mean train loss 3773.1498509457238
INFO:root:current train perplexity4.414602279663086
INFO:root:current mean train loss 3777.4043702639656
INFO:root:current train perplexity4.426595211029053
INFO:root:current mean train loss 3776.1175420948975
INFO:root:current train perplexity4.431151390075684
INFO:root:current mean train loss 3771.372046911324
INFO:root:current train perplexity4.430367946624756
INFO:root:current mean train loss 3771.3746403585537
INFO:root:current train perplexity4.430372714996338
INFO:root:current mean train loss 3773.0691398646063
INFO:root:current train perplexity4.43167781829834

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:03<00:00, 123.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:03<00:00, 123.56s/it]
INFO:root:final mean train loss: 3775.245878650296
INFO:root:final train perplexity: 4.434627056121826
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.09s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.09s/it]
INFO:root:eval mean loss: 4000.9703135388963
INFO:root:eval perplexity: 5.04236364364624
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/51
 26%|â–ˆâ–ˆâ–Œ       | 51/200 [2:02:23<5:34:34, 134.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3722.557547433036
INFO:root:current train perplexity4.40023136138916
INFO:root:current mean train loss 3764.9090313777747
INFO:root:current train perplexity4.398489952087402
INFO:root:current mean train loss 3763.0844101468147
INFO:root:current train perplexity4.405354976654053
INFO:root:current mean train loss 3772.1296824104234
INFO:root:current train perplexity4.405476093292236
INFO:root:current mean train loss 3773.175287570063
INFO:root:current train perplexity4.411688327789307
INFO:root:current mean train loss 3769.824437850561
INFO:root:current train perplexity4.409839630126953
INFO:root:current mean train loss 3766.200757197925
INFO:root:current train perplexity4.409689426422119
INFO:root:current mean train loss 3762.9396002309495
INFO:root:current train perplexity4.411214351654053
INFO:root:current mean train loss 3766.2641174997098
INFO:root:current train perplexity4.415097713470459
INFO:root:current mean train loss 3766.6065990107327
INFO:root:current train perplexity4.414004325866699

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.72s/it]
INFO:root:final mean train loss: 3764.624216264294
INFO:root:final train perplexity: 4.416083335876465
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.89s/it]
INFO:root:eval mean loss: 3997.1841114666445
INFO:root:eval perplexity: 5.034649848937988
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/52
 26%|â–ˆâ–ˆâ–Œ       | 52/200 [2:05:03<5:51:02, 142.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3773.6473470052083
INFO:root:current train perplexity4.473240852355957
INFO:root:current mean train loss 3736.4428923233695
INFO:root:current train perplexity4.3729376792907715
INFO:root:current mean train loss 3739.414903933503
INFO:root:current train perplexity4.376950263977051
INFO:root:current mean train loss 3740.2119659908235
INFO:root:current train perplexity4.3835248947143555
INFO:root:current mean train loss 3746.275901849586
INFO:root:current train perplexity4.383759021759033
INFO:root:current mean train loss 3749.325893128034
INFO:root:current train perplexity4.387577056884766
INFO:root:current mean train loss 3747.495308133257
INFO:root:current train perplexity4.392805099487305
INFO:root:current mean train loss 3751.4993061625873
INFO:root:current train perplexity4.396366596221924
INFO:root:current mean train loss 3757.021331000767
INFO:root:current train perplexity4.398192405700684
INFO:root:current mean train loss 3758.2127305327867
INFO:root:current train perplexity4.399585247039795

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:03<00:00, 123.02s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:03<00:00, 123.02s/it]
INFO:root:final mean train loss: 3756.17033490827
INFO:root:final train perplexity: 4.401378154754639
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.24s/it]
INFO:root:eval mean loss: 3993.4641303745566
INFO:root:eval perplexity: 5.0270819664001465
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/53
 26%|â–ˆâ–ˆâ–‹       | 53/200 [2:07:17<5:42:36, 139.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3745.3243992017665
INFO:root:current train perplexity4.431926250457764
INFO:root:current mean train loss 3748.4391375285823
INFO:root:current train perplexity4.3906450271606445
INFO:root:current mean train loss 3738.447602823711
INFO:root:current train perplexity4.378586292266846
INFO:root:current mean train loss 3748.712038022446
INFO:root:current train perplexity4.3850555419921875
INFO:root:current mean train loss 3743.811196185173
INFO:root:current train perplexity4.383326053619385
INFO:root:current mean train loss 3750.061313373865
INFO:root:current train perplexity4.389811038970947
INFO:root:current mean train loss 3752.408148261938
INFO:root:current train perplexity4.389467239379883
INFO:root:current mean train loss 3748.174904639955
INFO:root:current train perplexity4.383860111236572
INFO:root:current mean train loss 3750.9117151309047
INFO:root:current train perplexity4.387505531311035
INFO:root:current mean train loss 3752.799980056118
INFO:root:current train perplexity4.388535976409912

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.92s/it]
INFO:root:final mean train loss: 3747.5430980805427
INFO:root:final train perplexity: 4.386423587799072
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.15s/it]
INFO:root:eval mean loss: 3994.64125006926
INFO:root:eval perplexity: 5.02947473526001
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/54
 27%|â–ˆâ–ˆâ–‹       | 54/200 [2:10:02<5:58:15, 147.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3718.3561224168348
INFO:root:current train perplexity4.372727394104004
INFO:root:current mean train loss 3725.9417342557254
INFO:root:current train perplexity4.348959922790527
INFO:root:current mean train loss 3736.638652851055
INFO:root:current train perplexity4.362195014953613
INFO:root:current mean train loss 3727.7287494394354
INFO:root:current train perplexity4.353505611419678
INFO:root:current mean train loss 3731.690723675863
INFO:root:current train perplexity4.361256122589111
INFO:root:current mean train loss 3734.93189763933
INFO:root:current train perplexity4.368067264556885
INFO:root:current mean train loss 3733.4515510474444
INFO:root:current train perplexity4.369784355163574
INFO:root:current mean train loss 3734.517547732665
INFO:root:current train perplexity4.3688273429870605
INFO:root:current mean train loss 3736.084213826057
INFO:root:current train perplexity4.369100570678711
INFO:root:current mean train loss 3737.7981224510777
INFO:root:current train perplexity4.367613315582275

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.95s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.95s/it]
INFO:root:final mean train loss: 3736.5276635077694
INFO:root:final train perplexity: 4.367402076721191
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.17s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.17s/it]
INFO:root:eval mean loss: 3995.591971755873
INFO:root:eval perplexity: 5.031409740447998
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/55
 28%|â–ˆâ–ˆâ–Š       | 55/200 [2:12:35<6:00:12, 149.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3689.043056390224
INFO:root:current train perplexity4.306593894958496
INFO:root:current mean train loss 3726.321125716614
INFO:root:current train perplexity4.332901477813721
INFO:root:current mean train loss 3727.7987290386377
INFO:root:current train perplexity4.336246013641357
INFO:root:current mean train loss 3723.3591812718932
INFO:root:current train perplexity4.330789566040039
INFO:root:current mean train loss 3720.233572505873
INFO:root:current train perplexity4.33159875869751
INFO:root:current mean train loss 3718.3989429933904
INFO:root:current train perplexity4.3378190994262695
INFO:root:current mean train loss 3720.517833727626
INFO:root:current train perplexity4.3416032791137695
INFO:root:current mean train loss 3724.293982313515
INFO:root:current train perplexity4.348618030548096
INFO:root:current mean train loss 3726.8038389731078
INFO:root:current train perplexity4.350401401519775
INFO:root:current mean train loss 3730.5004838612385
INFO:root:current train perplexity4.352880001068115

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.44s/it]
INFO:root:final mean train loss: 3728.297408257761
INFO:root:final train perplexity: 4.353243350982666
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.89s/it]
INFO:root:eval mean loss: 3991.0031530501997
INFO:root:eval perplexity: 5.0220818519592285
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/56
 28%|â–ˆâ–ˆâ–Š       | 56/200 [2:15:21<6:09:32, 153.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3723.3402177526596
INFO:root:current train perplexity4.3366875648498535
INFO:root:current mean train loss 3707.1110557504253
INFO:root:current train perplexity4.316284656524658
INFO:root:current mean train loss 3713.1990378684845
INFO:root:current train perplexity4.319086074829102
INFO:root:current mean train loss 3714.720632063896
INFO:root:current train perplexity4.320495128631592
INFO:root:current mean train loss 3713.703248981928
INFO:root:current train perplexity4.326994895935059
INFO:root:current mean train loss 3716.1716942377457
INFO:root:current train perplexity4.328415870666504
INFO:root:current mean train loss 3721.7203918174023
INFO:root:current train perplexity4.3334269523620605
INFO:root:current mean train loss 3722.185477587433
INFO:root:current train perplexity4.335710525512695
INFO:root:current mean train loss 3723.7196968736166
INFO:root:current train perplexity4.337676525115967
INFO:root:current mean train loss 3723.7381964324018
INFO:root:current train perplexity4.338038921356201

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.43s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.43s/it]
INFO:root:final mean train loss: 3719.2758950264224
INFO:root:final train perplexity: 4.3377766609191895
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.08s/it]
INFO:root:eval mean loss: 3992.3563137189717
INFO:root:eval perplexity: 5.024830341339111
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/57
 28%|â–ˆâ–ˆâ–Š       | 57/200 [2:17:43<5:58:55, 150.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3672.5572088068184
INFO:root:current train perplexity4.2531538009643555
INFO:root:current mean train loss 3687.4958937121974
INFO:root:current train perplexity4.28834342956543
INFO:root:current mean train loss 3696.832423789828
INFO:root:current train perplexity4.292907238006592
INFO:root:current mean train loss 3695.180458434199
INFO:root:current train perplexity4.29336404800415
INFO:root:current mean train loss 3705.990290715144
INFO:root:current train perplexity4.305431365966797
INFO:root:current mean train loss 3707.180717729448
INFO:root:current train perplexity4.311474800109863
INFO:root:current mean train loss 3708.2508039867603
INFO:root:current train perplexity4.316427707672119
INFO:root:current mean train loss 3709.4111839041807
INFO:root:current train perplexity4.322158336639404
INFO:root:current mean train loss 3710.8636170504387
INFO:root:current train perplexity4.321528434753418
INFO:root:current mean train loss 3713.5233390768162
INFO:root:current train perplexity4.323472499847412

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.84s/it]
INFO:root:final mean train loss: 3711.304002638786
INFO:root:final train perplexity: 4.324155330657959
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.07s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.07s/it]
INFO:root:eval mean loss: 3991.2403746259974
INFO:root:eval perplexity: 5.0225629806518555
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/58
 29%|â–ˆâ–ˆâ–‰       | 58/200 [2:19:55<5:42:59, 144.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3699.8948761470733
INFO:root:current train perplexity4.30927038192749
INFO:root:current mean train loss 3681.038695540165
INFO:root:current train perplexity4.268028736114502
INFO:root:current mean train loss 3690.298093846542
INFO:root:current train perplexity4.276655197143555
INFO:root:current mean train loss 3682.826133538869
INFO:root:current train perplexity4.278131484985352
INFO:root:current mean train loss 3690.1361144624057
INFO:root:current train perplexity4.288355350494385
INFO:root:current mean train loss 3693.42435196492
INFO:root:current train perplexity4.29150915145874
INFO:root:current mean train loss 3695.7373864359447
INFO:root:current train perplexity4.294785499572754
INFO:root:current mean train loss 3697.413880754423
INFO:root:current train perplexity4.300828456878662
INFO:root:current mean train loss 3699.8802944285017
INFO:root:current train perplexity4.305847644805908
INFO:root:current mean train loss 3704.7724606839793
INFO:root:current train perplexity4.30994987487793

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.60s/it]
INFO:root:final mean train loss: 3703.7215518951416
INFO:root:final train perplexity: 4.3112382888793945
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.44s/it]
INFO:root:eval mean loss: 3988.9448623116136
INFO:root:eval perplexity: 5.017903804779053
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/59
 30%|â–ˆâ–ˆâ–‰       | 59/200 [2:22:33<5:49:39, 148.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3690.891962615537
INFO:root:current train perplexity4.2745208740234375
INFO:root:current mean train loss 3676.916077017087
INFO:root:current train perplexity4.257144451141357
INFO:root:current mean train loss 3690.763760162016
INFO:root:current train perplexity4.272314548492432
INFO:root:current mean train loss 3694.457976883634
INFO:root:current train perplexity4.2830305099487305
INFO:root:current mean train loss 3691.894933485934
INFO:root:current train perplexity4.288625240325928
INFO:root:current mean train loss 3693.2126392157397
INFO:root:current train perplexity4.291938781738281
INFO:root:current mean train loss 3693.648287959319
INFO:root:current train perplexity4.292904853820801
INFO:root:current mean train loss 3696.2129533225925
INFO:root:current train perplexity4.293065071105957
INFO:root:current mean train loss 3696.2380124430433
INFO:root:current train perplexity4.293824672698975
INFO:root:current mean train loss 3696.0160821036625
INFO:root:current train perplexity4.294407844543457

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.27s/it]
INFO:root:final mean train loss: 3694.3382995974634
INFO:root:final train perplexity: 4.295308589935303
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.37s/it]
INFO:root:eval mean loss: 3989.345554216534
INFO:root:eval perplexity: 5.018716812133789
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/60
 30%|â–ˆâ–ˆâ–ˆ       | 60/200 [2:25:19<5:59:23, 154.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3679.6709200702135
INFO:root:current train perplexity4.27569580078125
INFO:root:current mean train loss 3691.4130041026538
INFO:root:current train perplexity4.282035827636719
INFO:root:current mean train loss 3687.7315293178763
INFO:root:current train perplexity4.283681392669678
INFO:root:current mean train loss 3678.1845548524075
INFO:root:current train perplexity4.27713680267334
INFO:root:current mean train loss 3679.1116064147313
INFO:root:current train perplexity4.273901462554932
INFO:root:current mean train loss 3682.9050655595584
INFO:root:current train perplexity4.279121398925781
INFO:root:current mean train loss 3680.983215062362
INFO:root:current train perplexity4.275051116943359
INFO:root:current mean train loss 3684.0586699068317
INFO:root:current train perplexity4.278351306915283
INFO:root:current mean train loss 3685.9020637798635
INFO:root:current train perplexity4.278465747833252
INFO:root:current mean train loss 3690.250026683398
INFO:root:current train perplexity4.282703876495361

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.22s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.22s/it]
INFO:root:final mean train loss: 3687.015744978382
INFO:root:final train perplexity: 4.282917022705078
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.03s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.03s/it]
INFO:root:eval mean loss: 3987.692955936946
INFO:root:eval perplexity: 5.015364646911621
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/61
 30%|â–ˆâ–ˆâ–ˆ       | 61/200 [2:28:08<6:07:05, 158.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3665.743155643858
INFO:root:current train perplexity4.226467609405518
INFO:root:current mean train loss 3673.59147309492
INFO:root:current train perplexity4.253143310546875
INFO:root:current mean train loss 3677.566921752504
INFO:root:current train perplexity4.257981300354004
INFO:root:current mean train loss 3680.1462755622174
INFO:root:current train perplexity4.261631488800049
INFO:root:current mean train loss 3681.487521255775
INFO:root:current train perplexity4.265480995178223
INFO:root:current mean train loss 3682.2972792669293
INFO:root:current train perplexity4.264979839324951
INFO:root:current mean train loss 3681.612159340316
INFO:root:current train perplexity4.265400409698486
INFO:root:current mean train loss 3680.834665921319
INFO:root:current train perplexity4.268492698669434
INFO:root:current mean train loss 3679.8223399406356
INFO:root:current train perplexity4.267367839813232
INFO:root:current mean train loss 3681.395577566964
INFO:root:current train perplexity4.268579006195068

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.31s/it]
INFO:root:final mean train loss: 3678.542455673218
INFO:root:final train perplexity: 4.2686238288879395
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.33s/it]
INFO:root:eval mean loss: 3989.7883577820257
INFO:root:eval perplexity: 5.019615650177002
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/62
 31%|â–ˆâ–ˆâ–ˆ       | 62/200 [2:30:23<5:48:10, 151.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3646.6190044202303
INFO:root:current train perplexity4.221442699432373
INFO:root:current mean train loss 3660.620336288061
INFO:root:current train perplexity4.240149974822998
INFO:root:current mean train loss 3664.7766775357522
INFO:root:current train perplexity4.242249011993408
INFO:root:current mean train loss 3666.644020717959
INFO:root:current train perplexity4.243844985961914
INFO:root:current mean train loss 3669.6553331163195
INFO:root:current train perplexity4.242428302764893
INFO:root:current mean train loss 3671.7971716616335
INFO:root:current train perplexity4.249992847442627
INFO:root:current mean train loss 3671.5736141945818
INFO:root:current train perplexity4.249621868133545
INFO:root:current mean train loss 3673.723820447622
INFO:root:current train perplexity4.25622034072876
INFO:root:current mean train loss 3673.6781640079435
INFO:root:current train perplexity4.255677700042725

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.53s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.53s/it]
INFO:root:final mean train loss: 3670.4299696030157
INFO:root:final train perplexity: 4.254982948303223
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.89s/it]
INFO:root:eval mean loss: 3990.243221201795
INFO:root:eval perplexity: 5.020539283752441
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/63
 32%|â–ˆâ–ˆâ–ˆâ–      | 63/200 [2:32:38<5:34:15, 146.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3691.723876953125
INFO:root:current train perplexity4.314877510070801
INFO:root:current mean train loss 3670.101899082221
INFO:root:current train perplexity4.238034248352051
INFO:root:current mean train loss 3656.3931650246304
INFO:root:current train perplexity4.226706504821777
INFO:root:current mean train loss 3655.661721006085
INFO:root:current train perplexity4.234276294708252
INFO:root:current mean train loss 3663.073957040943
INFO:root:current train perplexity4.238320350646973
INFO:root:current mean train loss 3667.485369521154
INFO:root:current train perplexity4.2361297607421875
INFO:root:current mean train loss 3668.059262201363
INFO:root:current train perplexity4.23633337020874
INFO:root:current mean train loss 3665.1767689255867
INFO:root:current train perplexity4.23473596572876
INFO:root:current mean train loss 3665.575880304814
INFO:root:current train perplexity4.23626708984375
INFO:root:current mean train loss 3665.3676146785024
INFO:root:current train perplexity4.236934661865234

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.04s/it]
INFO:root:final mean train loss: 3661.783475814327
INFO:root:final train perplexity: 4.240492820739746
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.55s/it]
INFO:root:eval mean loss: 3987.9367658466313
INFO:root:eval perplexity: 5.015858173370361
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/64
 32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [2:34:49<5:21:36, 141.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3637.041814630682
INFO:root:current train perplexity4.207610130310059
INFO:root:current mean train loss 3664.6716176625846
INFO:root:current train perplexity4.217580795288086
INFO:root:current mean train loss 3644.144465297319
INFO:root:current train perplexity4.212569713592529
INFO:root:current mean train loss 3633.228147451517
INFO:root:current train perplexity4.2061967849731445
INFO:root:current mean train loss 3639.5640301855233
INFO:root:current train perplexity4.206879138946533
INFO:root:current mean train loss 3645.94209949471
INFO:root:current train perplexity4.212396144866943
INFO:root:current mean train loss 3650.416193036518
INFO:root:current train perplexity4.21839714050293
INFO:root:current mean train loss 3653.1101530909373
INFO:root:current train perplexity4.222992420196533
INFO:root:current mean train loss 3657.1216636121494
INFO:root:current train perplexity4.226313591003418
INFO:root:current mean train loss 3655.507376745163
INFO:root:current train perplexity4.225611686706543

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.61s/it]
INFO:root:final mean train loss: 3653.650310147193
INFO:root:final train perplexity: 4.226907730102539
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.83s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.83s/it]
INFO:root:eval mean loss: 3984.93818913453
INFO:root:eval perplexity: 5.0097808837890625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/65
 32%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/200 [2:37:06<5:16:08, 140.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3585.452700966283
INFO:root:current train perplexity4.184085845947266
INFO:root:current mean train loss 3643.09111369157
INFO:root:current train perplexity4.200782775878906
INFO:root:current mean train loss 3645.091654180936
INFO:root:current train perplexity4.2027268409729
INFO:root:current mean train loss 3646.6312496938676
INFO:root:current train perplexity4.202694892883301
INFO:root:current mean train loss 3648.944658156511
INFO:root:current train perplexity4.208713531494141
INFO:root:current mean train loss 3645.228938049434
INFO:root:current train perplexity4.203094482421875
INFO:root:current mean train loss 3650.569304384592
INFO:root:current train perplexity4.210629940032959
INFO:root:current mean train loss 3648.7751128683503
INFO:root:current train perplexity4.212949275970459
INFO:root:current mean train loss 3647.129810971364
INFO:root:current train perplexity4.20927095413208
INFO:root:current mean train loss 3648.265922272426
INFO:root:current train perplexity4.2150044441223145

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:06<00:00, 126.10s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:06<00:00, 126.10s/it]
INFO:root:final mean train loss: 3648.0905847857075
INFO:root:final train perplexity: 4.21764612197876
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.90s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.90s/it]
INFO:root:eval mean loss: 3985.310217891179
INFO:root:eval perplexity: 5.010533809661865
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/66
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/200 [2:39:23<5:11:20, 139.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3611.4890769675926
INFO:root:current train perplexity4.1193461418151855
INFO:root:current mean train loss 3626.397508996678
INFO:root:current train perplexity4.173523902893066
INFO:root:current mean train loss 3639.790319770443
INFO:root:current train perplexity4.200133323669434
INFO:root:current mean train loss 3640.734687828507
INFO:root:current train perplexity4.19658088684082
INFO:root:current mean train loss 3642.7176202063815
INFO:root:current train perplexity4.194318771362305
INFO:root:current mean train loss 3637.014558564101
INFO:root:current train perplexity4.190020561218262
INFO:root:current mean train loss 3637.0852097506727
INFO:root:current train perplexity4.1935858726501465
INFO:root:current mean train loss 3637.580979799798
INFO:root:current train perplexity4.198141574859619
INFO:root:current mean train loss 3638.169911247355
INFO:root:current train perplexity4.199875831604004
INFO:root:current mean train loss 3641.351994684213
INFO:root:current train perplexity4.202437877655029

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.47s/it]
INFO:root:final mean train loss: 3639.7750524090184
INFO:root:final train perplexity: 4.203832149505615
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.08s/it]
INFO:root:eval mean loss: 3988.8449152953235
INFO:root:eval perplexity: 5.017700672149658
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/67
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/200 [2:42:20<5:33:40, 150.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3611.613685825893
INFO:root:current train perplexity4.157944202423096
INFO:root:current mean train loss 3606.4661205150464
INFO:root:current train perplexity4.160308361053467
INFO:root:current mean train loss 3615.0680518617023
INFO:root:current train perplexity4.167916297912598
INFO:root:current mean train loss 3624.7304417852147
INFO:root:current train perplexity4.1749114990234375
INFO:root:current mean train loss 3624.2148577810704
INFO:root:current train perplexity4.174795150756836
INFO:root:current mean train loss 3631.96776385441
INFO:root:current train perplexity4.181283473968506
INFO:root:current mean train loss 3632.1241902989664
INFO:root:current train perplexity4.178622245788574
INFO:root:current mean train loss 3633.9920446694305
INFO:root:current train perplexity4.184850215911865
INFO:root:current mean train loss 3635.8558818885667
INFO:root:current train perplexity4.189229488372803
INFO:root:current mean train loss 3637.576401915525
INFO:root:current train perplexity4.191399574279785

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.80s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.80s/it]
INFO:root:final mean train loss: 3633.1673065923874
INFO:root:final train perplexity: 4.192887783050537
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.86s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.86s/it]
INFO:root:eval mean loss: 3988.65367180574
INFO:root:eval perplexity: 5.017312049865723
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/68
 34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [2:44:36<5:22:11, 146.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3618.4043650072676
INFO:root:current train perplexity4.158668518066406
INFO:root:current mean train loss 3631.973841100306
INFO:root:current train perplexity4.177586555480957
INFO:root:current mean train loss 3630.81287073206
INFO:root:current train perplexity4.177597999572754
INFO:root:current mean train loss 3628.8400209832816
INFO:root:current train perplexity4.172750949859619
INFO:root:current mean train loss 3623.1091104683974
INFO:root:current train perplexity4.168622016906738
INFO:root:current mean train loss 3625.671126392006
INFO:root:current train perplexity4.17177677154541
INFO:root:current mean train loss 3629.293787361489
INFO:root:current train perplexity4.174867153167725
INFO:root:current mean train loss 3630.1838917789996
INFO:root:current train perplexity4.178034782409668
INFO:root:current mean train loss 3632.080509063612
INFO:root:current train perplexity4.1816816329956055
INFO:root:current mean train loss 3629.080803038839
INFO:root:current train perplexity4.179437637329102

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:03<00:00, 123.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:03<00:00, 123.52s/it]
INFO:root:final mean train loss: 3625.6671197337487
INFO:root:final train perplexity: 4.1804986000061035
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.48s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.48s/it]
INFO:root:eval mean loss: 3987.754181557513
INFO:root:eval perplexity: 5.015488147735596
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/69
 34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [2:47:25<5:34:05, 153.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3603.1432004442404
INFO:root:current train perplexity4.1462082862854
INFO:root:current mean train loss 3600.19873046875
INFO:root:current train perplexity4.1523027420043945
INFO:root:current mean train loss 3604.8130943024776
INFO:root:current train perplexity4.149666786193848
INFO:root:current mean train loss 3614.330793853499
INFO:root:current train perplexity4.161666393280029
INFO:root:current mean train loss 3606.888833191865
INFO:root:current train perplexity4.15859842300415
INFO:root:current mean train loss 3608.679189913935
INFO:root:current train perplexity4.162922382354736
INFO:root:current mean train loss 3612.996829922115
INFO:root:current train perplexity4.164835453033447
INFO:root:current mean train loss 3614.9413802430095
INFO:root:current train perplexity4.166912078857422
INFO:root:current mean train loss 3618.458128878709
INFO:root:current train perplexity4.167421817779541
INFO:root:current mean train loss 3621.813525852721
INFO:root:current train perplexity4.169834136962891

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.69s/it]
INFO:root:final mean train loss: 3619.826369931621
INFO:root:final train perplexity: 4.1708760261535645
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.15s/it]
INFO:root:eval mean loss: 3988.01468479887
INFO:root:eval perplexity: 5.016016483306885
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/70
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [2:49:47<5:24:12, 149.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3571.425971596928
INFO:root:current train perplexity4.137842178344727
INFO:root:current mean train loss 3582.584382063188
INFO:root:current train perplexity4.135152339935303
INFO:root:current mean train loss 3598.3700210394545
INFO:root:current train perplexity4.143616676330566
INFO:root:current mean train loss 3598.888524302533
INFO:root:current train perplexity4.146422863006592
INFO:root:current mean train loss 3600.874307470384
INFO:root:current train perplexity4.142299652099609
INFO:root:current mean train loss 3601.5309717932414
INFO:root:current train perplexity4.143454551696777
INFO:root:current mean train loss 3603.6054609701014
INFO:root:current train perplexity4.145471572875977
INFO:root:current mean train loss 3604.8641860821185
INFO:root:current train perplexity4.149288654327393
INFO:root:current mean train loss 3607.635713197577
INFO:root:current train perplexity4.151736259460449
INFO:root:current mean train loss 3613.2189136938705
INFO:root:current train perplexity4.154824256896973

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.64s/it]
INFO:root:final mean train loss: 3611.498519712879
INFO:root:final train perplexity: 4.157195568084717
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.97s/it]
INFO:root:eval mean loss: 3987.484156831782
INFO:root:eval perplexity: 5.01494026184082
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/71
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [2:52:36<5:34:30, 155.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3576.32616823111
INFO:root:current train perplexity4.085078239440918
INFO:root:current mean train loss 3581.673224352077
INFO:root:current train perplexity4.102138996124268
INFO:root:current mean train loss 3583.0124237403443
INFO:root:current train perplexity4.114298343658447
INFO:root:current mean train loss 3597.402468148629
INFO:root:current train perplexity4.129027366638184
INFO:root:current mean train loss 3597.371412648889
INFO:root:current train perplexity4.128404140472412
INFO:root:current mean train loss 3593.4692133074295
INFO:root:current train perplexity4.130598545074463
INFO:root:current mean train loss 3599.67518901682
INFO:root:current train perplexity4.134466171264648
INFO:root:current mean train loss 3601.7622525489937
INFO:root:current train perplexity4.1384124755859375
INFO:root:current mean train loss 3601.3844111001476
INFO:root:current train perplexity4.139610767364502
INFO:root:current mean train loss 3607.7776406573166
INFO:root:current train perplexity4.1470842361450195

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.75s/it]
INFO:root:final mean train loss: 3605.1916444224694
INFO:root:final train perplexity: 4.14686393737793
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.95s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.95s/it]
INFO:root:eval mean loss: 3989.90377223238
INFO:root:eval perplexity: 5.01984977722168
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/72
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/200 [2:55:22<5:38:38, 158.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3613.2826302083336
INFO:root:current train perplexity4.117482662200928
INFO:root:current mean train loss 3595.2699539620535
INFO:root:current train perplexity4.114675998687744
INFO:root:current mean train loss 3597.0418013139206
INFO:root:current train perplexity4.117442607879639
INFO:root:current mean train loss 3594.476205078125
INFO:root:current train perplexity4.125097751617432
INFO:root:current mean train loss 3598.0962021998357
INFO:root:current train perplexity4.131251811981201
INFO:root:current mean train loss 3598.5075666610055
INFO:root:current train perplexity4.1297993659973145
INFO:root:current mean train loss 3598.1324421296295
INFO:root:current train perplexity4.129990577697754
INFO:root:current mean train loss 3599.6899076990926
INFO:root:current train perplexity4.133922576904297
INFO:root:current mean train loss 3601.212060546875
INFO:root:current train perplexity4.13452672958374
INFO:root:current mean train loss 3601.5739190204326
INFO:root:current train perplexity4.136115074157715

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.52s/it]
INFO:root:final mean train loss: 3598.2434820359754
INFO:root:final train perplexity: 4.135511875152588
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.01s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.01s/it]
INFO:root:eval mean loss: 3988.0411021996897
INFO:root:eval perplexity: 5.016068935394287
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/73
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/200 [2:58:13<5:43:27, 162.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3558.7891595679594
INFO:root:current train perplexity4.0886383056640625
INFO:root:current mean train loss 3577.1757732453893
INFO:root:current train perplexity4.105720520019531
INFO:root:current mean train loss 3588.479235969247
INFO:root:current train perplexity4.118897914886475
INFO:root:current mean train loss 3586.9023450248856
INFO:root:current train perplexity4.113493919372559
INFO:root:current mean train loss 3588.2671752676956
INFO:root:current train perplexity4.118923187255859
INFO:root:current mean train loss 3588.638142135908
INFO:root:current train perplexity4.122096061706543
INFO:root:current mean train loss 3590.781346154946
INFO:root:current train perplexity4.12282657623291
INFO:root:current mean train loss 3590.7692867975734
INFO:root:current train perplexity4.1214599609375
INFO:root:current mean train loss 3592.0619536558606
INFO:root:current train perplexity4.121285438537598
INFO:root:current mean train loss 3593.183579344958
INFO:root:current train perplexity4.12359619140625

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.41s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.41s/it]
INFO:root:final mean train loss: 3591.164673466836
INFO:root:final train perplexity: 4.123978614807129
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.50s/it]
INFO:root:eval mean loss: 3991.892933081228
INFO:root:eval perplexity: 5.023889541625977
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/74
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/200 [3:01:01<5:44:52, 164.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3591.2064034598216
INFO:root:current train perplexity4.107353687286377
INFO:root:current mean train loss 3573.038328799902
INFO:root:current train perplexity4.086772918701172
INFO:root:current mean train loss 3580.742903142451
INFO:root:current train perplexity4.096216201782227
INFO:root:current mean train loss 3577.5559631503756
INFO:root:current train perplexity4.097655773162842
INFO:root:current mean train loss 3577.3739483555564
INFO:root:current train perplexity4.097583293914795
INFO:root:current mean train loss 3578.8042839037385
INFO:root:current train perplexity4.095486164093018
INFO:root:current mean train loss 3580.3559517315257
INFO:root:current train perplexity4.101779937744141
INFO:root:current mean train loss 3581.9135239091142
INFO:root:current train perplexity4.106152534484863
INFO:root:current mean train loss 3583.416770789492
INFO:root:current train perplexity4.108741283416748
INFO:root:current mean train loss 3587.24185811956
INFO:root:current train perplexity4.113078594207764

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:03<00:00, 123.12s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:03<00:00, 123.12s/it]
INFO:root:final mean train loss: 3584.463607972668
INFO:root:final train perplexity: 4.1130900382995605
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.16s/it]
INFO:root:eval mean loss: 3987.4008581283247
INFO:root:eval perplexity: 5.014770984649658
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/75
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/200 [3:03:45<5:41:53, 164.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3566.1940227470013
INFO:root:current train perplexity4.085684776306152
INFO:root:current mean train loss 3577.705570086762
INFO:root:current train perplexity4.088315963745117
INFO:root:current mean train loss 3576.6751231317935
INFO:root:current train perplexity4.094257831573486
INFO:root:current mean train loss 3574.9350653244437
INFO:root:current train perplexity4.09428596496582
INFO:root:current mean train loss 3579.2564484437626
INFO:root:current train perplexity4.097233772277832
INFO:root:current mean train loss 3579.1501195840724
INFO:root:current train perplexity4.094274520874023
INFO:root:current mean train loss 3581.9450830287687
INFO:root:current train perplexity4.101102352142334
INFO:root:current mean train loss 3579.4603826071652
INFO:root:current train perplexity4.1001973152160645
INFO:root:current mean train loss 3580.123513702291
INFO:root:current train perplexity4.102684020996094

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.77s/it]
INFO:root:final mean train loss: 3578.314188803396
INFO:root:final train perplexity: 4.103123188018799
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.94s/it]
INFO:root:eval mean loss: 3990.0954243544993
INFO:root:eval perplexity: 5.020239353179932
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/76
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [3:06:37<5:43:48, 166.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3569.1100725446427
INFO:root:current train perplexity4.0907440185546875
INFO:root:current mean train loss 3570.1850083966124
INFO:root:current train perplexity4.092079162597656
INFO:root:current mean train loss 3564.460448039327
INFO:root:current train perplexity4.081789970397949
INFO:root:current mean train loss 3564.3605829791836
INFO:root:current train perplexity4.076745986938477
INFO:root:current mean train loss 3568.534954099163
INFO:root:current train perplexity4.081777095794678
INFO:root:current mean train loss 3567.3200360962155
INFO:root:current train perplexity4.080404758453369
INFO:root:current mean train loss 3572.1841198388593
INFO:root:current train perplexity4.085825443267822
INFO:root:current mean train loss 3572.930098084446
INFO:root:current train perplexity4.088348865509033
INFO:root:current mean train loss 3572.9168336624844
INFO:root:current train perplexity4.088368892669678
INFO:root:current mean train loss 3572.071992413606
INFO:root:current train perplexity4.088295936584473

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.45s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.45s/it]
INFO:root:final mean train loss: 3570.721481569352
INFO:root:final train perplexity: 4.090850830078125
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.95s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.95s/it]
INFO:root:eval mean loss: 3990.7765126329787
INFO:root:eval perplexity: 5.021620750427246
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/77
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/200 [3:09:31<5:45:54, 168.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3525.5238118489583
INFO:root:current train perplexity4.01429557800293
INFO:root:current mean train loss 3555.698583984375
INFO:root:current train perplexity4.050565719604492
INFO:root:current mean train loss 3561.43504837391
INFO:root:current train perplexity4.058375835418701
INFO:root:current mean train loss 3557.538678075397
INFO:root:current train perplexity4.059256076812744
INFO:root:current mean train loss 3562.41349421122
INFO:root:current train perplexity4.069356441497803
INFO:root:current mean train loss 3563.974154751972
INFO:root:current train perplexity4.073068141937256
INFO:root:current mean train loss 3559.8899449790397
INFO:root:current train perplexity4.068979263305664
INFO:root:current mean train loss 3563.5601989319275
INFO:root:current train perplexity4.0744547843933105
INFO:root:current mean train loss 3564.6051176667943
INFO:root:current train perplexity4.0769853591918945
INFO:root:current mean train loss 3565.651725527237
INFO:root:current train perplexity4.077928066253662

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.28s/it]
INFO:root:final mean train loss: 3564.637123846239
INFO:root:final train perplexity: 4.081042766571045
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.88s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.88s/it]
INFO:root:eval mean loss: 3990.276164602726
INFO:root:eval perplexity: 5.020605564117432
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/78
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [3:12:13<5:39:01, 166.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3555.94288170856
INFO:root:current train perplexity4.067300319671631
INFO:root:current mean train loss 3553.1963605182927
INFO:root:current train perplexity4.069807052612305
INFO:root:current mean train loss 3549.6136874211743
INFO:root:current train perplexity4.068282127380371
INFO:root:current mean train loss 3552.018807142512
INFO:root:current train perplexity4.062507152557373
INFO:root:current mean train loss 3551.4196869690086
INFO:root:current train perplexity4.061673164367676
INFO:root:current mean train loss 3550.4426526275693
INFO:root:current train perplexity4.063283920288086
INFO:root:current mean train loss 3555.6934036573284
INFO:root:current train perplexity4.066177845001221
INFO:root:current mean train loss 3555.467797412582
INFO:root:current train perplexity4.064265727996826
INFO:root:current mean train loss 3557.6045319382215
INFO:root:current train perplexity4.064908981323242
INFO:root:current mean train loss 3558.7195207754944
INFO:root:current train perplexity4.069464683532715

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.22s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.22s/it]
INFO:root:final mean train loss: 3558.5248786557104
INFO:root:final train perplexity: 4.0712127685546875
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.67s/it]
INFO:root:eval mean loss: 3993.2347905585107
INFO:root:eval perplexity: 5.026615619659424
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/79
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/200 [3:15:01<5:37:09, 167.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3541.2338000882055
INFO:root:current train perplexity4.028555870056152
INFO:root:current mean train loss 3549.915335385854
INFO:root:current train perplexity4.049921035766602
INFO:root:current mean train loss 3555.256121482684
INFO:root:current train perplexity4.0517897605896
INFO:root:current mean train loss 3558.2622232581193
INFO:root:current train perplexity4.052751541137695
INFO:root:current mean train loss 3556.24125795298
INFO:root:current train perplexity4.057765007019043
INFO:root:current mean train loss 3555.591642850312
INFO:root:current train perplexity4.06063985824585
INFO:root:current mean train loss 3555.566824500421
INFO:root:current train perplexity4.0605316162109375
INFO:root:current mean train loss 3554.2604703263937
INFO:root:current train perplexity4.059027194976807
INFO:root:current mean train loss 3557.3787643487703
INFO:root:current train perplexity4.063256740570068
INFO:root:current mean train loss 3554.306031977964
INFO:root:current train perplexity4.062017440795898

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.89s/it]
INFO:root:final mean train loss: 3551.838121783349
INFO:root:final train perplexity: 4.060486793518066
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.08s/it]
INFO:root:eval mean loss: 3992.403692583666
INFO:root:eval perplexity: 5.024926662445068
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/80
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/200 [3:17:52<5:36:42, 168.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3503.1883075420674
INFO:root:current train perplexity4.04353666305542
INFO:root:current mean train loss 3527.595580176484
INFO:root:current train perplexity4.026782989501953
INFO:root:current mean train loss 3530.317531952798
INFO:root:current train perplexity4.027061939239502
INFO:root:current mean train loss 3535.0018278138828
INFO:root:current train perplexity4.026175022125244
INFO:root:current mean train loss 3536.3916682979784
INFO:root:current train perplexity4.036001682281494
INFO:root:current mean train loss 3537.376778285888
INFO:root:current train perplexity4.039198875427246
INFO:root:current mean train loss 3544.388665379866
INFO:root:current train perplexity4.045833110809326
INFO:root:current mean train loss 3546.5156547329584
INFO:root:current train perplexity4.050368785858154
INFO:root:current mean train loss 3547.978627074177
INFO:root:current train perplexity4.051815032958984
INFO:root:current mean train loss 3548.081679989101
INFO:root:current train perplexity4.051606178283691

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.75s/it]
INFO:root:final mean train loss: 3547.3665228812924
INFO:root:final train perplexity: 4.053329944610596
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.15s/it]
INFO:root:eval mean loss: 3992.5211934840427
INFO:root:eval perplexity: 5.025165557861328
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/81
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/200 [3:20:45<5:36:13, 169.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3548.358195852726
INFO:root:current train perplexity4.078787803649902
INFO:root:current mean train loss 3555.0322165975767
INFO:root:current train perplexity4.052212238311768
INFO:root:current mean train loss 3539.437023579833
INFO:root:current train perplexity4.036844253540039
INFO:root:current mean train loss 3536.835769345506
INFO:root:current train perplexity4.032679557800293
INFO:root:current mean train loss 3542.868027518526
INFO:root:current train perplexity4.033515453338623
INFO:root:current mean train loss 3537.4024780942927
INFO:root:current train perplexity4.033804416656494
INFO:root:current mean train loss 3540.2925370701314
INFO:root:current train perplexity4.037263870239258
INFO:root:current mean train loss 3543.6422545651353
INFO:root:current train perplexity4.0378737449646
INFO:root:current mean train loss 3546.1407437555345
INFO:root:current train perplexity4.041102409362793
INFO:root:current mean train loss 3543.6004315127543
INFO:root:current train perplexity4.04341983795166

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:03<00:00, 123.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:03<00:00, 123.37s/it]
INFO:root:final mean train loss: 3540.6030085779007
INFO:root:final train perplexity: 4.04252815246582
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.17s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.17s/it]
INFO:root:eval mean loss: 3995.6212997977614
INFO:root:eval perplexity: 5.031469345092773
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/82
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/200 [3:23:04<5:15:44, 160.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3538.9484375
INFO:root:current train perplexity4.028589725494385
INFO:root:current mean train loss 3532.3044606854837
INFO:root:current train perplexity4.026010513305664
INFO:root:current mean train loss 3526.76832299326
INFO:root:current train perplexity4.021167755126953
INFO:root:current mean train loss 3531.622502888424
INFO:root:current train perplexity4.030154705047607
INFO:root:current mean train loss 3529.2678807520606
INFO:root:current train perplexity4.034301280975342
INFO:root:current mean train loss 3532.004321069116
INFO:root:current train perplexity4.034509658813477
INFO:root:current mean train loss 3532.899032383111
INFO:root:current train perplexity4.033562183380127
INFO:root:current mean train loss 3535.292979421047
INFO:root:current train perplexity4.033524036407471
INFO:root:current mean train loss 3536.367538434302
INFO:root:current train perplexity4.03277587890625
INFO:root:current mean train loss 3539.4546159195024
INFO:root:current train perplexity4.035166263580322

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.93s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.93s/it]
INFO:root:final mean train loss: 3535.044174871137
INFO:root:final train perplexity: 4.03367280960083
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.73s/it]
INFO:root:eval mean loss: 3993.6341717226287
INFO:root:eval perplexity: 5.0274271965026855
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/83
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/200 [3:25:51<5:16:43, 162.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3526.8128681485614
INFO:root:current train perplexity3.9880259037017822
INFO:root:current mean train loss 3527.68380793472
INFO:root:current train perplexity4.008212566375732
INFO:root:current mean train loss 3519.056795649655
INFO:root:current train perplexity4.015218257904053
INFO:root:current mean train loss 3519.1990600249655
INFO:root:current train perplexity4.016852855682373
INFO:root:current mean train loss 3522.2822444907533
INFO:root:current train perplexity4.01475715637207
INFO:root:current mean train loss 3523.9379947858847
INFO:root:current train perplexity4.01725959777832
INFO:root:current mean train loss 3528.54899935485
INFO:root:current train perplexity4.021530628204346
INFO:root:current mean train loss 3531.788514063524
INFO:root:current train perplexity4.025209426879883
INFO:root:current mean train loss 3531.3011744776577
INFO:root:current train perplexity4.023868083953857
INFO:root:current mean train loss 3531.295574184271
INFO:root:current train perplexity4.024130344390869

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:03<00:00, 123.45s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:03<00:00, 123.45s/it]
INFO:root:final mean train loss: 3529.294575598932
INFO:root:final train perplexity: 4.024533271789551
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.37s/it]
INFO:root:eval mean loss: 3995.477469802748
INFO:root:eval perplexity: 5.0311760902404785
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/84
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/200 [3:28:51<5:24:12, 167.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3496.2170444542253
INFO:root:current train perplexity3.9908623695373535
INFO:root:current mean train loss 3509.2202933685126
INFO:root:current train perplexity3.9937198162078857
INFO:root:current mean train loss 3506.038066117966
INFO:root:current train perplexity3.9935522079467773
INFO:root:current mean train loss 3508.786940253327
INFO:root:current train perplexity3.999570846557617
INFO:root:current mean train loss 3512.17083260765
INFO:root:current train perplexity4.000467300415039
INFO:root:current mean train loss 3517.4636217641746
INFO:root:current train perplexity4.003981590270996
INFO:root:current mean train loss 3521.6952091677535
INFO:root:current train perplexity4.008328437805176
INFO:root:current mean train loss 3525.3309805903455
INFO:root:current train perplexity4.0098443031311035
INFO:root:current mean train loss 3525.897814675122
INFO:root:current train perplexity4.012236595153809
INFO:root:current mean train loss 3524.54836548983
INFO:root:current train perplexity4.012726783752441

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.59s/it]
INFO:root:final mean train loss: 3521.647016156104
INFO:root:final train perplexity: 4.012408256530762
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.83s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.83s/it]
INFO:root:eval mean loss: 3994.78965120789
INFO:root:eval perplexity: 5.0297770500183105
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/85
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/200 [3:31:35<5:19:28, 166.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3477.2314144086236
INFO:root:current train perplexity4.00490665435791
INFO:root:current mean train loss 3503.022481396212
INFO:root:current train perplexity4.000434875488281
INFO:root:current mean train loss 3515.8209967937946
INFO:root:current train perplexity4.002233982086182
INFO:root:current mean train loss 3518.8162941643304
INFO:root:current train perplexity3.999387264251709
INFO:root:current mean train loss 3518.693788022736
INFO:root:current train perplexity4.000931739807129
INFO:root:current mean train loss 3517.898903854976
INFO:root:current train perplexity4.003598213195801
INFO:root:current mean train loss 3516.642659025796
INFO:root:current train perplexity4.004662036895752
INFO:root:current mean train loss 3519.1201607504613
INFO:root:current train perplexity4.005649089813232
INFO:root:current mean train loss 3520.7747876337635
INFO:root:current train perplexity4.005932807922363
INFO:root:current mean train loss 3521.4727617367053
INFO:root:current train perplexity4.006379127502441

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.67s/it]
INFO:root:final mean train loss: 3518.227978675596
INFO:root:final train perplexity: 4.006999492645264
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.21s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.21s/it]
INFO:root:eval mean loss: 3995.2824499251997
INFO:root:eval perplexity: 5.03078031539917
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/86
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/200 [3:33:47<4:56:46, 156.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3477.9153140714798
INFO:root:current train perplexity3.96213960647583
INFO:root:current mean train loss 3486.2242477335394
INFO:root:current train perplexity3.9671826362609863
INFO:root:current mean train loss 3491.092155855292
INFO:root:current train perplexity3.9838027954101562
INFO:root:current mean train loss 3496.132585392442
INFO:root:current train perplexity3.9841599464416504
INFO:root:current mean train loss 3499.6747335007058
INFO:root:current train perplexity3.984140396118164
INFO:root:current mean train loss 3505.9116398098117
INFO:root:current train perplexity3.9881300926208496
INFO:root:current mean train loss 3506.863470307951
INFO:root:current train perplexity3.9885165691375732
INFO:root:current mean train loss 3510.8565569905495
INFO:root:current train perplexity3.994173049926758
INFO:root:current mean train loss 3511.9738408962794
INFO:root:current train perplexity3.9944071769714355
INFO:root:current mean train loss 3513.704462455278
INFO:root:current train perplexity3.99540114402771

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:03<00:00, 123.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:03<00:00, 123.63s/it]
INFO:root:final mean train loss: 3511.1305164829378
INFO:root:final train perplexity: 3.995795488357544
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.94s/it]
INFO:root:eval mean loss: 3995.444341131981
INFO:root:eval perplexity: 5.031108379364014
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/87
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/200 [3:36:34<5:00:02, 159.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3498.9110788445723
INFO:root:current train perplexity3.95967173576355
INFO:root:current mean train loss 3514.947511017628
INFO:root:current train perplexity3.9780561923980713
INFO:root:current mean train loss 3512.908649198888
INFO:root:current train perplexity3.9837706089019775
INFO:root:current mean train loss 3504.3342581833467
INFO:root:current train perplexity3.979546546936035
INFO:root:current mean train loss 3504.042458767361
INFO:root:current train perplexity3.9793412685394287
INFO:root:current mean train loss 3505.870991580226
INFO:root:current train perplexity3.9776697158813477
INFO:root:current mean train loss 3504.8654606002697
INFO:root:current train perplexity3.979403257369995
INFO:root:current mean train loss 3506.37112968013
INFO:root:current train perplexity3.9835400581359863
INFO:root:current mean train loss 3508.4170371966657
INFO:root:current train perplexity3.9872982501983643

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.96s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.96s/it]
INFO:root:final mean train loss: 3506.538702934019
INFO:root:final train perplexity: 3.988563299179077
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.86s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.86s/it]
INFO:root:eval mean loss: 3997.7825313054077
INFO:root:eval perplexity: 5.035868167877197
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/88
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/200 [3:39:24<5:03:33, 162.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3703.2561848958335
INFO:root:current train perplexity4.091028690338135
INFO:root:current mean train loss 3513.1373468787924
INFO:root:current train perplexity3.9783241748809814
INFO:root:current mean train loss 3499.4206374595906
INFO:root:current train perplexity3.9661436080932617
INFO:root:current mean train loss 3494.4374701874485
INFO:root:current train perplexity3.9604530334472656
INFO:root:current mean train loss 3499.6729636369805
INFO:root:current train perplexity3.970794916152954
INFO:root:current mean train loss 3499.8370921929363
INFO:root:current train perplexity3.9737637042999268
INFO:root:current mean train loss 3500.771453604374
INFO:root:current train perplexity3.972919464111328
INFO:root:current mean train loss 3496.780744701836
INFO:root:current train perplexity3.9710757732391357
INFO:root:current mean train loss 3499.2098189528524
INFO:root:current train perplexity3.9717185497283936
INFO:root:current mean train loss 3500.3014739280525
INFO:root:current train perplexity3.9766643047332764

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.34s/it]
INFO:root:final mean train loss: 3499.6209413466913
INFO:root:final train perplexity: 3.977692127227783
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.49s/it]
INFO:root:eval mean loss: 3998.2789471132537
INFO:root:eval perplexity: 5.036879539489746
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/89
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/200 [3:42:06<5:00:31, 162.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3490.9660200639205
INFO:root:current train perplexity3.912938356399536
INFO:root:current mean train loss 3487.7555800429336
INFO:root:current train perplexity3.958866834640503
INFO:root:current mean train loss 3485.379695368039
INFO:root:current train perplexity3.9581539630889893
INFO:root:current mean train loss 3482.3485284050944
INFO:root:current train perplexity3.956907033920288
INFO:root:current mean train loss 3481.507851705064
INFO:root:current train perplexity3.9517061710357666
INFO:root:current mean train loss 3483.8749603450647
INFO:root:current train perplexity3.9535470008850098
INFO:root:current mean train loss 3486.3105348877352
INFO:root:current train perplexity3.960416555404663
INFO:root:current mean train loss 3492.5270663018637
INFO:root:current train perplexity3.9623050689697266
INFO:root:current mean train loss 3492.09138144459
INFO:root:current train perplexity3.9633092880249023
INFO:root:current mean train loss 3494.8914983856166
INFO:root:current train perplexity3.9671783447265625

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:04<00:00, 124.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:04<00:00, 124.35s/it]
INFO:root:final mean train loss: 3494.627463863742
INFO:root:final train perplexity: 3.9698636531829834
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.29s/it]
INFO:root:eval mean loss: 4001.133603792664
INFO:root:eval perplexity: 5.042695999145508
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/90
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/200 [3:45:02<5:05:23, 166.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3447.850765830592
INFO:root:current train perplexity3.9905078411102295
INFO:root:current mean train loss 3485.609809939601
INFO:root:current train perplexity3.9393205642700195
INFO:root:current mean train loss 3500.8923774614727
INFO:root:current train perplexity3.9496114253997803
INFO:root:current mean train loss 3496.3700467770377
INFO:root:current train perplexity3.954251766204834
INFO:root:current mean train loss 3496.6665720791693
INFO:root:current train perplexity3.9528307914733887
INFO:root:current mean train loss 3495.014140399205
INFO:root:current train perplexity3.954103708267212
INFO:root:current mean train loss 3491.560121699566
INFO:root:current train perplexity3.9547548294067383
INFO:root:current mean train loss 3488.6421241388866
INFO:root:current train perplexity3.95845627784729
INFO:root:current mean train loss 3490.2305379082723
INFO:root:current train perplexity3.960298538208008
INFO:root:current mean train loss 3493.8126500973376
INFO:root:current train perplexity3.963263511657715

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.82s/it]
INFO:root:final mean train loss: 3490.1028497142174
INFO:root:final train perplexity: 3.962783098220825
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.98s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.98s/it]
INFO:root:eval mean loss: 4001.226510555186
INFO:root:eval perplexity: 5.042886257171631
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/91
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/200 [3:47:51<5:03:37, 167.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3476.6231825086807
INFO:root:current train perplexity3.976505994796753
INFO:root:current mean train loss 3478.3563607283463
INFO:root:current train perplexity3.944223403930664
INFO:root:current mean train loss 3477.8059813377618
INFO:root:current train perplexity3.947688341140747
INFO:root:current mean train loss 3479.627784845661
INFO:root:current train perplexity3.9470903873443604
INFO:root:current mean train loss 3474.361573980899
INFO:root:current train perplexity3.9451282024383545
INFO:root:current mean train loss 3480.566465084648
INFO:root:current train perplexity3.950359582901001
INFO:root:current mean train loss 3482.6756733920206
INFO:root:current train perplexity3.9493274688720703
INFO:root:current mean train loss 3485.2834204000815
INFO:root:current train perplexity3.951805591583252
INFO:root:current mean train loss 3485.9282037626585
INFO:root:current train perplexity3.953669548034668
INFO:root:current mean train loss 3486.01195235597
INFO:root:current train perplexity3.9534716606140137

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.57s/it]
INFO:root:final mean train loss: 3484.308827861663
INFO:root:final train perplexity: 3.9537346363067627
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.71s/it]
INFO:root:eval mean loss: 4004.4740241300974
INFO:root:eval perplexity: 5.049513339996338
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/92
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/200 [3:50:00<4:40:18, 155.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3464.3111886160714
INFO:root:current train perplexity3.9272875785827637
INFO:root:current mean train loss 3473.1745442708334
INFO:root:current train perplexity3.932441234588623
INFO:root:current mean train loss 3480.58575049867
INFO:root:current train perplexity3.9436235427856445
INFO:root:current mean train loss 3482.963917473181
INFO:root:current train perplexity3.944350481033325
INFO:root:current mean train loss 3486.0576929552803
INFO:root:current train perplexity3.9480650424957275
INFO:root:current mean train loss 3486.498150463639
INFO:root:current train perplexity3.949237585067749
INFO:root:current mean train loss 3482.066487373893
INFO:root:current train perplexity3.943551778793335
INFO:root:current mean train loss 3479.093241788903
INFO:root:current train perplexity3.942312240600586
INFO:root:current mean train loss 3481.0261324031626
INFO:root:current train perplexity3.9445972442626953
INFO:root:current mean train loss 3480.3026343687334
INFO:root:current train perplexity3.943488597869873

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:03<00:00, 123.10s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:03<00:00, 123.10s/it]
INFO:root:final mean train loss: 3480.031446703019
INFO:root:final train perplexity: 3.947068214416504
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.88s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.88s/it]
INFO:root:eval mean loss: 4002.1173658438606
INFO:root:eval perplexity: 5.044703006744385
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/93
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/200 [3:52:52<4:46:37, 160.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3433.9961164607557
INFO:root:current train perplexity3.9149413108825684
INFO:root:current mean train loss 3475.24848223066
INFO:root:current train perplexity3.9149842262268066
INFO:root:current mean train loss 3467.3678988233023
INFO:root:current train perplexity3.9276106357574463
INFO:root:current mean train loss 3468.24856433924
INFO:root:current train perplexity3.9277563095092773
INFO:root:current mean train loss 3467.042921905862
INFO:root:current train perplexity3.9271433353424072
INFO:root:current mean train loss 3467.4114619302486
INFO:root:current train perplexity3.9274954795837402
INFO:root:current mean train loss 3469.5485752515065
INFO:root:current train perplexity3.932227611541748
INFO:root:current mean train loss 3475.1436644357755
INFO:root:current train perplexity3.9348323345184326
INFO:root:current mean train loss 3475.625702881728
INFO:root:current train perplexity3.9360098838806152
INFO:root:current mean train loss 3476.421936876574
INFO:root:current train perplexity3.937185049057007

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.59s/it]
INFO:root:final mean train loss: 3473.8810534323416
INFO:root:final train perplexity: 3.937502384185791
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.25s/it]
INFO:root:eval mean loss: 4002.5846371481603
INFO:root:eval perplexity: 5.045655250549316
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/94
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/200 [3:55:02<4:27:43, 151.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3455.7097311580883
INFO:root:current train perplexity3.9302544593811035
INFO:root:current mean train loss 3469.251249805981
INFO:root:current train perplexity3.9226877689361572
INFO:root:current mean train loss 3470.8926987363047
INFO:root:current train perplexity3.9264907836914062
INFO:root:current mean train loss 3470.0374536758814
INFO:root:current train perplexity3.9163057804107666
INFO:root:current mean train loss 3467.7612602419968
INFO:root:current train perplexity3.9177980422973633
INFO:root:current mean train loss 3470.9342304652055
INFO:root:current train perplexity3.924927234649658
INFO:root:current mean train loss 3470.6779653897847
INFO:root:current train perplexity3.928192615509033
INFO:root:current mean train loss 3470.750093950254
INFO:root:current train perplexity3.927044153213501
INFO:root:current mean train loss 3470.583301584533
INFO:root:current train perplexity3.928413152694702
INFO:root:current mean train loss 3471.880938444729
INFO:root:current train perplexity3.931419849395752

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.60s/it]
INFO:root:final mean train loss: 3468.222480835453
INFO:root:final train perplexity: 3.9287214279174805
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.02s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.02s/it]
INFO:root:eval mean loss: 4006.139617270612
INFO:root:eval perplexity: 5.052913665771484
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/95
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/200 [3:57:42<4:29:36, 154.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3434.124855170816
INFO:root:current train perplexity3.87416934967041
INFO:root:current mean train loss 3445.1413958087655
INFO:root:current train perplexity3.880948543548584
INFO:root:current mean train loss 3448.7212328818773
INFO:root:current train perplexity3.8898849487304688
INFO:root:current mean train loss 3456.4126282588354
INFO:root:current train perplexity3.8990721702575684
INFO:root:current mean train loss 3457.5665206078093
INFO:root:current train perplexity3.9027762413024902
INFO:root:current mean train loss 3462.2438283521074
INFO:root:current train perplexity3.9073774814605713
INFO:root:current mean train loss 3461.544686255216
INFO:root:current train perplexity3.9098775386810303
INFO:root:current mean train loss 3464.387945564682
INFO:root:current train perplexity3.9138123989105225
INFO:root:current mean train loss 3467.117883542364
INFO:root:current train perplexity3.91833758354187
INFO:root:current mean train loss 3467.797561088618
INFO:root:current train perplexity3.920649528503418

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.64s/it]
INFO:root:final mean train loss: 3463.4016891602546
INFO:root:final train perplexity: 3.9212565422058105
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.09s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.09s/it]
INFO:root:eval mean loss: 4004.2840325105276
INFO:root:eval perplexity: 5.0491251945495605
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/96
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/200 [4:00:43<4:40:58, 162.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3466.760454320196
INFO:root:current train perplexity3.944608688354492
INFO:root:current mean train loss 3461.0550851422154
INFO:root:current train perplexity3.915476083755493
INFO:root:current mean train loss 3468.6206008968284
INFO:root:current train perplexity3.9201600551605225
INFO:root:current mean train loss 3465.3752860503237
INFO:root:current train perplexity3.913179874420166
INFO:root:current mean train loss 3462.2771597296573
INFO:root:current train perplexity3.9116086959838867
INFO:root:current mean train loss 3459.430205922068
INFO:root:current train perplexity3.910813093185425
INFO:root:current mean train loss 3460.2594457165947
INFO:root:current train perplexity3.910517692565918
INFO:root:current mean train loss 3458.4862857903154
INFO:root:current train perplexity3.9129555225372314
INFO:root:current mean train loss 3462.39628162846
INFO:root:current train perplexity3.915461301803589
INFO:root:current mean train loss 3462.0101322146456
INFO:root:current train perplexity3.915714740753174

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.25s/it]
INFO:root:final mean train loss: 3459.56705745574
INFO:root:final train perplexity: 3.9153287410736084
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.83s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.83s/it]
INFO:root:eval mean loss: 4008.5442431294327
INFO:root:eval perplexity: 5.057829856872559
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/97
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/200 [4:03:36<4:44:01, 165.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3446.2031022135416
INFO:root:current train perplexity3.915689468383789
INFO:root:current mean train loss 3445.437003348214
INFO:root:current train perplexity3.9002106189727783
INFO:root:current mean train loss 3456.4754181463068
INFO:root:current train perplexity3.8978216648101807
INFO:root:current mean train loss 3460.5386555989585
INFO:root:current train perplexity3.8995659351348877
INFO:root:current mean train loss 3457.782763671875
INFO:root:current train perplexity3.9023849964141846
INFO:root:current mean train loss 3454.0183296535324
INFO:root:current train perplexity3.9044575691223145
INFO:root:current mean train loss 3458.4886414930556
INFO:root:current train perplexity3.906252145767212
INFO:root:current mean train loss 3456.8741031376007
INFO:root:current train perplexity3.9058470726013184
INFO:root:current mean train loss 3458.515896763393
INFO:root:current train perplexity3.9073755741119385
INFO:root:current mean train loss 3457.191500400641
INFO:root:current train perplexity3.9087722301483154

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.87s/it]
INFO:root:final mean train loss: 3455.5442318454866
INFO:root:final train perplexity: 3.9091198444366455
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.50s/it]
INFO:root:eval mean loss: 4006.758259225399
INFO:root:eval perplexity: 5.054178237915039
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/98
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [4:06:28<4:44:24, 167.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3432.831295886672
INFO:root:current train perplexity3.875830888748169
INFO:root:current mean train loss 3436.202201801571
INFO:root:current train perplexity3.8878989219665527
INFO:root:current mean train loss 3450.582751594247
INFO:root:current train perplexity3.8898422718048096
INFO:root:current mean train loss 3451.406437408208
INFO:root:current train perplexity3.8875973224639893
INFO:root:current mean train loss 3451.4984224370796
INFO:root:current train perplexity3.8908445835113525
INFO:root:current mean train loss 3451.04455964234
INFO:root:current train perplexity3.892625093460083
INFO:root:current mean train loss 3452.4866283857978
INFO:root:current train perplexity3.896172046661377
INFO:root:current mean train loss 3450.1503251466715
INFO:root:current train perplexity3.8956477642059326
INFO:root:current mean train loss 3451.3077980119265
INFO:root:current train perplexity3.896275043487549
INFO:root:current mean train loss 3451.7319343388385
INFO:root:current train perplexity3.8986473083496094

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.09s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.09s/it]
INFO:root:final mean train loss: 3448.9323939046553
INFO:root:final train perplexity: 3.898935556411743
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.73s/it]
INFO:root:eval mean loss: 4010.2141979028147
INFO:root:eval perplexity: 5.061247825622559
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/99
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/200 [4:08:41<4:24:06, 156.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3415.372354696085
INFO:root:current train perplexity3.845491886138916
INFO:root:current mean train loss 3428.48992760144
INFO:root:current train perplexity3.8546671867370605
INFO:root:current mean train loss 3433.312347307238
INFO:root:current train perplexity3.8674392700195312
INFO:root:current mean train loss 3438.9701449108857
INFO:root:current train perplexity3.875699281692505
INFO:root:current mean train loss 3440.3390566326693
INFO:root:current train perplexity3.878767251968384
INFO:root:current mean train loss 3440.782419065937
INFO:root:current train perplexity3.8787810802459717
INFO:root:current mean train loss 3442.6881864909324
INFO:root:current train perplexity3.8820626735687256
INFO:root:current mean train loss 3443.820451082984
INFO:root:current train perplexity3.88480281829834
INFO:root:current mean train loss 3442.892084089594
INFO:root:current train perplexity3.886644124984741
INFO:root:current mean train loss 3446.891757753374
INFO:root:current train perplexity3.8916823863983154

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.88s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.88s/it]
INFO:root:final mean train loss: 3444.157082773024
INFO:root:final train perplexity: 3.891597032546997
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.28s/it]
INFO:root:eval mean loss: 4010.049822002438
INFO:root:eval perplexity: 5.060910224914551
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/100
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/200 [4:11:14<4:19:55, 155.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3421.3540828203913
INFO:root:current train perplexity3.8533899784088135
INFO:root:current mean train loss 3425.310445047503
INFO:root:current train perplexity3.871100664138794
INFO:root:current mean train loss 3428.6173467221465
INFO:root:current train perplexity3.8728435039520264
INFO:root:current mean train loss 3426.453277358435
INFO:root:current train perplexity3.8694183826446533
INFO:root:current mean train loss 3432.287776236066
INFO:root:current train perplexity3.8724396228790283
INFO:root:current mean train loss 3434.412963255817
INFO:root:current train perplexity3.875877618789673
INFO:root:current mean train loss 3439.4261333154504
INFO:root:current train perplexity3.8800389766693115
INFO:root:current mean train loss 3442.7553310656876
INFO:root:current train perplexity3.8824260234832764
INFO:root:current mean train loss 3442.55152127581
INFO:root:current train perplexity3.884087324142456

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.99s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.99s/it]
INFO:root:final mean train loss: 3439.6813813486406
INFO:root:final train perplexity: 3.8847315311431885
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.52s/it]
INFO:root:eval mean loss: 4010.588160738032
INFO:root:eval perplexity: 5.06201171875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/101
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/200 [4:13:59<4:21:21, 158.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3390.4329659598216
INFO:root:current train perplexity3.850339651107788
INFO:root:current mean train loss 3455.410092362734
INFO:root:current train perplexity3.874041795730591
INFO:root:current mean train loss 3442.1694512850995
INFO:root:current train perplexity3.8665223121643066
INFO:root:current mean train loss 3434.4089005560363
INFO:root:current train perplexity3.873012065887451
INFO:root:current mean train loss 3440.524642007064
INFO:root:current train perplexity3.876694440841675
INFO:root:current mean train loss 3439.475724620932
INFO:root:current train perplexity3.8783118724823
INFO:root:current mean train loss 3437.9179792074237
INFO:root:current train perplexity3.8751816749572754
INFO:root:current mean train loss 3436.87451966109
INFO:root:current train perplexity3.8757688999176025
INFO:root:current mean train loss 3437.891695346383
INFO:root:current train perplexity3.877589702606201
INFO:root:current mean train loss 3437.245598201058
INFO:root:current train perplexity3.878685474395752

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.60s/it]
INFO:root:final mean train loss: 3435.673939304967
INFO:root:final train perplexity: 3.878594398498535
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.66s/it]
INFO:root:eval mean loss: 4015.3475696753103
INFO:root:eval perplexity: 5.07176399230957
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/102
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/200 [4:16:37<4:18:37, 158.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3420.0660807291665
INFO:root:current train perplexity3.830491542816162
INFO:root:current mean train loss 3409.6801184612773
INFO:root:current train perplexity3.821244955062866
INFO:root:current mean train loss 3411.3582916969476
INFO:root:current train perplexity3.835954427719116
INFO:root:current mean train loss 3427.848275514633
INFO:root:current train perplexity3.8530514240264893
INFO:root:current mean train loss 3433.2239699030497
INFO:root:current train perplexity3.8580546379089355
INFO:root:current mean train loss 3429.139343143204
INFO:root:current train perplexity3.8560454845428467
INFO:root:current mean train loss 3431.450148866235
INFO:root:current train perplexity3.860316753387451
INFO:root:current mean train loss 3429.7150882320802
INFO:root:current train perplexity3.8628292083740234
INFO:root:current mean train loss 3429.622358188746
INFO:root:current train perplexity3.865138053894043
INFO:root:current mean train loss 3430.5841591423327
INFO:root:current train perplexity3.867412567138672

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.77s/it]
INFO:root:final mean train loss: 3430.367020084012
INFO:root:final train perplexity: 3.8704824447631836
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.73s/it]
INFO:root:eval mean loss: 4011.4774853861923
INFO:root:eval perplexity: 5.063833236694336
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/103
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/200 [4:19:22<4:19:24, 160.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3371.231646993886
INFO:root:current train perplexity3.820939302444458
INFO:root:current mean train loss 3414.265865170859
INFO:root:current train perplexity3.8298752307891846
INFO:root:current mean train loss 3419.105443569577
INFO:root:current train perplexity3.8425381183624268
INFO:root:current mean train loss 3417.28564679881
INFO:root:current train perplexity3.845189332962036
INFO:root:current mean train loss 3416.260468034316
INFO:root:current train perplexity3.847370147705078
INFO:root:current mean train loss 3420.3815278441684
INFO:root:current train perplexity3.8551876544952393
INFO:root:current mean train loss 3423.735407993078
INFO:root:current train perplexity3.8594324588775635
INFO:root:current mean train loss 3425.83772583852
INFO:root:current train perplexity3.863279342651367
INFO:root:current mean train loss 3426.5517756113304
INFO:root:current train perplexity3.8632240295410156
INFO:root:current mean train loss 3427.1516967641182
INFO:root:current train perplexity3.861764430999756

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.94s/it]
INFO:root:final mean train loss: 3426.157045179798
INFO:root:final train perplexity: 3.864058017730713
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.46s/it]
INFO:root:eval mean loss: 4012.5937707779253
INFO:root:eval perplexity: 5.06611967086792
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/104
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/200 [4:21:43<4:07:05, 154.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3373.211654170867
INFO:root:current train perplexity3.810736894607544
INFO:root:current mean train loss 3394.243711981155
INFO:root:current train perplexity3.823991537094116
INFO:root:current mean train loss 3390.9330610795455
INFO:root:current train perplexity3.831016778945923
INFO:root:current mean train loss 3401.300590215493
INFO:root:current train perplexity3.84127140045166
INFO:root:current mean train loss 3403.297383107055
INFO:root:current train perplexity3.8419082164764404
INFO:root:current mean train loss 3408.6543566457744
INFO:root:current train perplexity3.8451743125915527
INFO:root:current mean train loss 3413.8694803325575
INFO:root:current train perplexity3.847966432571411
INFO:root:current mean train loss 3416.4911571664456
INFO:root:current train perplexity3.8505125045776367
INFO:root:current mean train loss 3422.5505476858643
INFO:root:current train perplexity3.8563952445983887
INFO:root:current mean train loss 3422.301307817535
INFO:root:current train perplexity3.854494333267212

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.87s/it]
INFO:root:final mean train loss: 3421.092893354354
INFO:root:final train perplexity: 3.8563461303710938
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.61s/it]
INFO:root:eval mean loss: 4013.361809480275
INFO:root:eval perplexity: 5.06769323348999
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/105
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/200 [4:24:34<4:12:33, 159.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3401.840519831731
INFO:root:current train perplexity3.844622850418091
INFO:root:current mean train loss 3417.6994119548112
INFO:root:current train perplexity3.841634511947632
INFO:root:current mean train loss 3414.0370041595843
INFO:root:current train perplexity3.8451623916625977
INFO:root:current mean train loss 3424.562505041252
INFO:root:current train perplexity3.8487040996551514
INFO:root:current mean train loss 3420.344934554741
INFO:root:current train perplexity3.84486985206604
INFO:root:current mean train loss 3419.304768578241
INFO:root:current train perplexity3.844850778579712
INFO:root:current mean train loss 3421.876325771469
INFO:root:current train perplexity3.8460769653320312
INFO:root:current mean train loss 3422.6739206275374
INFO:root:current train perplexity3.8476006984710693
INFO:root:current mean train loss 3420.105392219625
INFO:root:current train perplexity3.846780300140381
INFO:root:current mean train loss 3419.5755587934304
INFO:root:current train perplexity3.848363161087036

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.92s/it]
INFO:root:final mean train loss: 3416.2575753119686
INFO:root:final train perplexity: 3.84899640083313
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.99s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.99s/it]
INFO:root:eval mean loss: 4015.50501960051
INFO:root:eval perplexity: 5.072087287902832
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/106
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/200 [4:27:26<4:15:38, 163.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3345.3847864029253
INFO:root:current train perplexity3.7967517375946045
INFO:root:current mean train loss 3393.145725379996
INFO:root:current train perplexity3.828644037246704
INFO:root:current mean train loss 3394.2962335130946
INFO:root:current train perplexity3.8241066932678223
INFO:root:current mean train loss 3397.6265570121127
INFO:root:current train perplexity3.8292391300201416
INFO:root:current mean train loss 3403.3450111201414
INFO:root:current train perplexity3.8279037475585938
INFO:root:current mean train loss 3410.3351555358777
INFO:root:current train perplexity3.8338420391082764
INFO:root:current mean train loss 3412.4480350264444
INFO:root:current train perplexity3.8359808921813965
INFO:root:current mean train loss 3411.3034203872785
INFO:root:current train perplexity3.83667254447937
INFO:root:current mean train loss 3414.0589027449823
INFO:root:current train perplexity3.8414933681488037
INFO:root:current mean train loss 3413.365057521284
INFO:root:current train perplexity3.8415017127990723

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.58s/it]
INFO:root:final mean train loss: 3412.0145243367842
INFO:root:final train perplexity: 3.8425586223602295
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.06s/it]
INFO:root:eval mean loss: 4018.046345162899
INFO:root:eval perplexity: 5.077301502227783
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/107
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/200 [4:30:12<4:14:33, 164.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3391.140176669034
INFO:root:current train perplexity3.8139634132385254
INFO:root:current mean train loss 3399.2175560735886
INFO:root:current train perplexity3.8160932064056396
INFO:root:current mean train loss 3393.0583745021445
INFO:root:current train perplexity3.8187577724456787
INFO:root:current mean train loss 3402.945958956866
INFO:root:current train perplexity3.825150489807129
INFO:root:current mean train loss 3406.7159013349933
INFO:root:current train perplexity3.824397325515747
INFO:root:current mean train loss 3404.994383445946
INFO:root:current train perplexity3.827604055404663
INFO:root:current mean train loss 3405.3463893278863
INFO:root:current train perplexity3.8304595947265625
INFO:root:current mean train loss 3408.352704948779
INFO:root:current train perplexity3.83007550239563
INFO:root:current mean train loss 3407.5217593544407
INFO:root:current train perplexity3.829859972000122
INFO:root:current mean train loss 3410.3922199668687
INFO:root:current train perplexity3.834852695465088

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.08s/it]
INFO:root:final mean train loss: 3406.6368499878913
INFO:root:final train perplexity: 3.8344147205352783
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.83s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.83s/it]
INFO:root:eval mean loss: 4020.0240902731603
INFO:root:eval perplexity: 5.081364154815674
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/108
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/200 [4:33:03<4:14:49, 166.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3353.6213882688494
INFO:root:current train perplexity3.8118953704833984
INFO:root:current mean train loss 3377.7299654907974
INFO:root:current train perplexity3.823972702026367
INFO:root:current mean train loss 3384.4178220799668
INFO:root:current train perplexity3.817704439163208
INFO:root:current mean train loss 3393.0870472301135
INFO:root:current train perplexity3.825108051300049
INFO:root:current mean train loss 3397.2429204491764
INFO:root:current train perplexity3.8196561336517334
INFO:root:current mean train loss 3401.4456971407362
INFO:root:current train perplexity3.823981523513794
INFO:root:current mean train loss 3404.2147701027525
INFO:root:current train perplexity3.8250625133514404
INFO:root:current mean train loss 3404.506093596412
INFO:root:current train perplexity3.8250925540924072
INFO:root:current mean train loss 3404.4560382794393
INFO:root:current train perplexity3.827349901199341
INFO:root:current mean train loss 3406.492283837941
INFO:root:current train perplexity3.8307175636291504

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.34s/it]
INFO:root:final mean train loss: 3404.0323615535613
INFO:root:final train perplexity: 3.8304765224456787
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.47s/it]
INFO:root:eval mean loss: 4017.650262494459
INFO:root:eval perplexity: 5.0764875411987305
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/109
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 109/200 [4:35:55<4:14:45, 167.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3378.9822671379843
INFO:root:current train perplexity3.7890312671661377
INFO:root:current mean train loss 3379.3295498675075
INFO:root:current train perplexity3.8051016330718994
INFO:root:current mean train loss 3381.4160985066883
INFO:root:current train perplexity3.806682825088501
INFO:root:current mean train loss 3386.5420553613544
INFO:root:current train perplexity3.8032386302948
INFO:root:current mean train loss 3390.8433591676617
INFO:root:current train perplexity3.8122706413269043
INFO:root:current mean train loss 3394.8795599995897
INFO:root:current train perplexity3.8188600540161133
INFO:root:current mean train loss 3396.4402583160627
INFO:root:current train perplexity3.8203797340393066
INFO:root:current mean train loss 3398.257303002898
INFO:root:current train perplexity3.821120262145996
INFO:root:current mean train loss 3399.708606811944
INFO:root:current train perplexity3.8222951889038086
INFO:root:current mean train loss 3401.8832115228342
INFO:root:current train perplexity3.824909210205078

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.60s/it]
INFO:root:final mean train loss: 3400.0882313020766
INFO:root:final train perplexity: 3.8245203495025635
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.03s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.03s/it]
INFO:root:eval mean loss: 4021.2849051834
INFO:root:eval perplexity: 5.08395528793335
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/110
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/200 [4:38:48<4:14:06, 169.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3387.4259481309336
INFO:root:current train perplexity3.7930376529693604
INFO:root:current mean train loss 3392.544226278806
INFO:root:current train perplexity3.7971482276916504
INFO:root:current mean train loss 3399.4293226016466
INFO:root:current train perplexity3.810396671295166
INFO:root:current mean train loss 3401.3346246804913
INFO:root:current train perplexity3.81514048576355
INFO:root:current mean train loss 3402.794868357744
INFO:root:current train perplexity3.812415838241577
INFO:root:current mean train loss 3398.840213966699
INFO:root:current train perplexity3.808626413345337
INFO:root:current mean train loss 3399.7996392903165
INFO:root:current train perplexity3.8105010986328125
INFO:root:current mean train loss 3396.6368121439746
INFO:root:current train perplexity3.811893939971924
INFO:root:current mean train loss 3396.565339697099
INFO:root:current train perplexity3.81571626663208
INFO:root:current mean train loss 3398.302507192049
INFO:root:current train perplexity3.818389415740967

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.05s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.05s/it]
INFO:root:final mean train loss: 3395.7566301899574
INFO:root:final train perplexity: 3.8179898262023926
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.66s/it]
INFO:root:eval mean loss: 4020.885439176086
INFO:root:eval perplexity: 5.083133697509766
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/111
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/200 [4:41:39<4:11:59, 169.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3399.9762061108117
INFO:root:current train perplexity3.798588514328003
INFO:root:current mean train loss 3393.846880744485
INFO:root:current train perplexity3.798766851425171
INFO:root:current mean train loss 3393.6744720777983
INFO:root:current train perplexity3.8060882091522217
INFO:root:current mean train loss 3390.7358663396317
INFO:root:current train perplexity3.801924467086792
INFO:root:current mean train loss 3389.708635960761
INFO:root:current train perplexity3.8050930500030518
INFO:root:current mean train loss 3390.406080723621
INFO:root:current train perplexity3.808889150619507
INFO:root:current mean train loss 3394.1129503985853
INFO:root:current train perplexity3.8121631145477295
INFO:root:current mean train loss 3391.7738085192977
INFO:root:current train perplexity3.8119471073150635
INFO:root:current mean train loss 3393.5959458894094
INFO:root:current train perplexity3.8125102519989014
INFO:root:current mean train loss 3394.208927235705
INFO:root:current train perplexity3.811396837234497

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.28s/it]
INFO:root:final mean train loss: 3391.2993407095632
INFO:root:final train perplexity: 3.8112823963165283
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.97s/it]
INFO:root:eval mean loss: 4024.030921016179
INFO:root:eval perplexity: 5.089603900909424
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/112
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/200 [4:44:32<4:10:30, 170.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3373.7833701685854
INFO:root:current train perplexity3.785484552383423
INFO:root:current mean train loss 3362.091039413061
INFO:root:current train perplexity3.7901828289031982
INFO:root:current mean train loss 3369.7223045219807
INFO:root:current train perplexity3.790252447128296
INFO:root:current mean train loss 3379.6969986155063
INFO:root:current train perplexity3.799028158187866
INFO:root:current mean train loss 3381.1154198232325
INFO:root:current train perplexity3.8001742362976074
INFO:root:current mean train loss 3379.6636378184085
INFO:root:current train perplexity3.7984275817871094
INFO:root:current mean train loss 3383.975716262927
INFO:root:current train perplexity3.8025240898132324
INFO:root:current mean train loss 3387.398759335692
INFO:root:current train perplexity3.8052735328674316
INFO:root:current mean train loss 3388.3294943697624
INFO:root:current train perplexity3.806485414505005

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.15s/it]
INFO:root:final mean train loss: 3388.078150964552
INFO:root:final train perplexity: 3.8064417839050293
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.79s/it]
INFO:root:eval mean loss: 4025.227629100177
INFO:root:eval perplexity: 5.092066764831543
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/113
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/200 [4:47:22<4:07:22, 170.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3327.8243001302085
INFO:root:current train perplexity3.8456156253814697
INFO:root:current mean train loss 3362.0650528102246
INFO:root:current train perplexity3.772385358810425
INFO:root:current mean train loss 3378.3170328375154
INFO:root:current train perplexity3.7903356552124023
INFO:root:current mean train loss 3376.395035646143
INFO:root:current train perplexity3.789729356765747
INFO:root:current mean train loss 3376.414806432227
INFO:root:current train perplexity3.791480302810669
INFO:root:current mean train loss 3380.727444900907
INFO:root:current train perplexity3.7949349880218506
INFO:root:current mean train loss 3381.684739550943
INFO:root:current train perplexity3.7950706481933594
INFO:root:current mean train loss 3382.4412569178967
INFO:root:current train perplexity3.795936346054077
INFO:root:current mean train loss 3386.3411699534945
INFO:root:current train perplexity3.799901008605957
INFO:root:current mean train loss 3385.1383596021074
INFO:root:current train perplexity3.797292709350586

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.91s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.91s/it]
INFO:root:final mean train loss: 3382.6232032160606
INFO:root:final train perplexity: 3.7982583045959473
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.92s/it]
INFO:root:eval mean loss: 4022.777387037345
INFO:root:eval perplexity: 5.087024688720703
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/114
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/200 [4:50:16<4:06:06, 171.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3352.647283380682
INFO:root:current train perplexity3.787889242172241
INFO:root:current mean train loss 3374.5863575978324
INFO:root:current train perplexity3.8045332431793213
INFO:root:current mean train loss 3369.833018226081
INFO:root:current train perplexity3.796564817428589
INFO:root:current mean train loss 3377.243397212872
INFO:root:current train perplexity3.7937190532684326
INFO:root:current mean train loss 3377.753193430657
INFO:root:current train perplexity3.7919363975524902
INFO:root:current mean train loss 3375.3263032618333
INFO:root:current train perplexity3.7896218299865723
INFO:root:current mean train loss 3380.8018077594365
INFO:root:current train perplexity3.788756847381592
INFO:root:current mean train loss 3379.6139319482904
INFO:root:current train perplexity3.7892038822174072
INFO:root:current mean train loss 3377.029849578067
INFO:root:current train perplexity3.7869491577148438
INFO:root:current mean train loss 3378.712658008027
INFO:root:current train perplexity3.7884933948516846

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.09s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.09s/it]
INFO:root:final mean train loss: 3378.543968077629
INFO:root:final train perplexity: 3.7921507358551025
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.00s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.00s/it]
INFO:root:eval mean loss: 4025.5438656637853
INFO:root:eval perplexity: 5.092717170715332
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/115
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/200 [4:52:50<3:55:28, 166.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3330.1883095189146
INFO:root:current train perplexity3.7459521293640137
INFO:root:current mean train loss 3345.224418576024
INFO:root:current train perplexity3.769871473312378
INFO:root:current mean train loss 3361.41981151006
INFO:root:current train perplexity3.7737557888031006
INFO:root:current mean train loss 3368.599822137049
INFO:root:current train perplexity3.7783098220825195
INFO:root:current mean train loss 3368.8591582450777
INFO:root:current train perplexity3.7755022048950195
INFO:root:current mean train loss 3371.09961501987
INFO:root:current train perplexity3.7750473022460938
INFO:root:current mean train loss 3373.6479985201686
INFO:root:current train perplexity3.7800519466400146
INFO:root:current mean train loss 3377.2460278761737
INFO:root:current train perplexity3.782276153564453
INFO:root:current mean train loss 3377.792015140892
INFO:root:current train perplexity3.7834038734436035
INFO:root:current mean train loss 3376.0111308466235
INFO:root:current train perplexity3.785409688949585

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.29s/it]
INFO:root:final mean train loss: 3375.2556505510884
INFO:root:final train perplexity: 3.78723406791687
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.79s/it]
INFO:root:eval mean loss: 4026.16660433289
INFO:root:eval perplexity: 5.094000816345215
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/116
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/200 [4:55:44<3:56:08, 168.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3383.6989384403937
INFO:root:current train perplexity3.7922587394714355
INFO:root:current mean train loss 3380.2666400098424
INFO:root:current train perplexity3.7789154052734375
INFO:root:current mean train loss 3377.584418880782
INFO:root:current train perplexity3.774071216583252
INFO:root:current mean train loss 3371.135469675793
INFO:root:current train perplexity3.772555351257324
INFO:root:current mean train loss 3373.9458173622293
INFO:root:current train perplexity3.776515483856201
INFO:root:current mean train loss 3373.1256939708846
INFO:root:current train perplexity3.774916648864746
INFO:root:current mean train loss 3373.250218830991
INFO:root:current train perplexity3.7748091220855713
INFO:root:current mean train loss 3373.304967573289
INFO:root:current train perplexity3.776681661605835
INFO:root:current mean train loss 3372.561970389019
INFO:root:current train perplexity3.77659010887146
INFO:root:current mean train loss 3373.2109743712917
INFO:root:current train perplexity3.7801015377044678

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.01s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.01s/it]
INFO:root:final mean train loss: 3370.4631961084183
INFO:root:final train perplexity: 3.780080556869507
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.59s/it]
INFO:root:eval mean loss: 4028.0477303579346
INFO:root:eval perplexity: 5.0978779792785645
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/117
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 117/200 [4:58:38<3:55:28, 170.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3366.2604422433037
INFO:root:current train perplexity3.7531912326812744
INFO:root:current mean train loss 3365.6522063078705
INFO:root:current train perplexity3.7558536529541016
INFO:root:current mean train loss 3361.6940533577126
INFO:root:current train perplexity3.7616255283355713
INFO:root:current mean train loss 3367.6625932835823
INFO:root:current train perplexity3.766693353652954
INFO:root:current mean train loss 3371.30140310704
INFO:root:current train perplexity3.769909143447876
INFO:root:current mean train loss 3371.0854752299942
INFO:root:current train perplexity3.771005392074585
INFO:root:current mean train loss 3370.4312796044537
INFO:root:current train perplexity3.7740650177001953
INFO:root:current mean train loss 3371.8226323341837
INFO:root:current train perplexity3.7741382122039795
INFO:root:current mean train loss 3371.2787825715754
INFO:root:current train perplexity3.7751553058624268
INFO:root:current mean train loss 3371.138085415274
INFO:root:current train perplexity3.777047634124756

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.02s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.02s/it]
INFO:root:final mean train loss: 3368.1130737796907
INFO:root:final train perplexity: 3.7765772342681885
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.59s/it]
INFO:root:eval mean loss: 4028.9343452183066
INFO:root:eval perplexity: 5.099705696105957
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/118
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/200 [5:01:29<3:52:49, 170.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3359.3967909702037
INFO:root:current train perplexity3.7538838386535645
INFO:root:current mean train loss 3348.5173271552667
INFO:root:current train perplexity3.741570472717285
INFO:root:current mean train loss 3349.1001951115613
INFO:root:current train perplexity3.748365640640259
INFO:root:current mean train loss 3353.4860298890762
INFO:root:current train perplexity3.7511487007141113
INFO:root:current mean train loss 3360.849302959227
INFO:root:current train perplexity3.7548093795776367
INFO:root:current mean train loss 3362.0620022768476
INFO:root:current train perplexity3.7570927143096924
INFO:root:current mean train loss 3364.3873239757486
INFO:root:current train perplexity3.7620487213134766
INFO:root:current mean train loss 3365.5379006140647
INFO:root:current train perplexity3.7648980617523193
INFO:root:current mean train loss 3364.524558867141
INFO:root:current train perplexity3.7692055702209473
INFO:root:current mean train loss 3365.366217409945
INFO:root:current train perplexity3.768970489501953

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.96s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.96s/it]
INFO:root:final mean train loss: 3363.100766643401
INFO:root:final train perplexity: 3.769116163253784
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.19s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.19s/it]
INFO:root:eval mean loss: 4029.8725811031695
INFO:root:eval perplexity: 5.1016411781311035
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/119
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 119/200 [5:04:27<3:53:16, 172.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3363.601921530331
INFO:root:current train perplexity3.759413003921509
INFO:root:current mean train loss 3351.3447265625
INFO:root:current train perplexity3.7489941120147705
INFO:root:current mean train loss 3349.711409245829
INFO:root:current train perplexity3.7541165351867676
INFO:root:current mean train loss 3344.3927450587607
INFO:root:current train perplexity3.752530336380005
INFO:root:current mean train loss 3345.913781548815
INFO:root:current train perplexity3.751983642578125
INFO:root:current mean train loss 3351.8537797045146
INFO:root:current train perplexity3.755431652069092
INFO:root:current mean train loss 3356.6520084785425
INFO:root:current train perplexity3.759246826171875
INFO:root:current mean train loss 3360.6365942415323
INFO:root:current train perplexity3.762683868408203
INFO:root:current mean train loss 3360.0669332609614
INFO:root:current train perplexity3.763113021850586
INFO:root:current mean train loss 3361.696947035604
INFO:root:current train perplexity3.763746976852417

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.59s/it]
INFO:root:final mean train loss: 3359.757362119613
INFO:root:final train perplexity: 3.764148235321045
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.82s/it]
INFO:root:eval mean loss: 4031.243965744127
INFO:root:eval perplexity: 5.104470252990723
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/120
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/200 [5:07:21<3:50:42, 173.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3342.135671841896
INFO:root:current train perplexity3.7308828830718994
INFO:root:current mean train loss 3338.6284287170793
INFO:root:current train perplexity3.732501268386841
INFO:root:current mean train loss 3339.392894847973
INFO:root:current train perplexity3.7292611598968506
INFO:root:current mean train loss 3346.3401586166
INFO:root:current train perplexity3.7409157752990723
INFO:root:current mean train loss 3348.965635744315
INFO:root:current train perplexity3.746159315109253
INFO:root:current mean train loss 3347.431072419359
INFO:root:current train perplexity3.749549150466919
INFO:root:current mean train loss 3353.0111056199025
INFO:root:current train perplexity3.7535762786865234
INFO:root:current mean train loss 3355.1782644721675
INFO:root:current train perplexity3.7557413578033447
INFO:root:current mean train loss 3356.89459178778
INFO:root:current train perplexity3.7562167644500732
INFO:root:current mean train loss 3356.822628908287
INFO:root:current train perplexity3.7562763690948486

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.30s/it]
INFO:root:final mean train loss: 3354.650874845443
INFO:root:final train perplexity: 3.7565720081329346
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.33s/it]
INFO:root:eval mean loss: 4032.10439176086
INFO:root:eval perplexity: 5.106247425079346
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/121
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/200 [5:10:16<3:48:43, 173.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3340.5743025594684
INFO:root:current train perplexity3.7471108436584473
INFO:root:current mean train loss 3348.840772069143
INFO:root:current train perplexity3.7466564178466797
INFO:root:current mean train loss 3344.0440065762523
INFO:root:current train perplexity3.738781690597534
INFO:root:current mean train loss 3344.0434583617166
INFO:root:current train perplexity3.7422709465026855
INFO:root:current mean train loss 3347.88045160265
INFO:root:current train perplexity3.7413055896759033
INFO:root:current mean train loss 3347.8390342537477
INFO:root:current train perplexity3.742675542831421
INFO:root:current mean train loss 3350.8242930536685
INFO:root:current train perplexity3.744722604751587
INFO:root:current mean train loss 3350.5628841952207
INFO:root:current train perplexity3.745631694793701
INFO:root:current mean train loss 3352.2846513547975
INFO:root:current train perplexity3.747107744216919
INFO:root:current mean train loss 3355.1072305010666
INFO:root:current train perplexity3.7523224353790283

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.15s/it]
INFO:root:final mean train loss: 3352.967963126398
INFO:root:final train perplexity: 3.7540783882141113
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.78s/it]
INFO:root:eval mean loss: 4032.6460307236257
INFO:root:eval perplexity: 5.107364654541016
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/122
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/200 [5:13:09<3:45:30, 173.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3348.6801920572916
INFO:root:current train perplexity3.7397618293762207
INFO:root:current mean train loss 3336.8501130022323
INFO:root:current train perplexity3.7472734451293945
INFO:root:current mean train loss 3344.5290092329546
INFO:root:current train perplexity3.7479443550109863
INFO:root:current mean train loss 3346.100018229167
INFO:root:current train perplexity3.74554181098938
INFO:root:current mean train loss 3350.675713404605
INFO:root:current train perplexity3.749427556991577
INFO:root:current mean train loss 3350.8146055536686
INFO:root:current train perplexity3.745936155319214
INFO:root:current mean train loss 3351.8564366319442
INFO:root:current train perplexity3.7468485832214355
INFO:root:current mean train loss 3352.041218182964
INFO:root:current train perplexity3.7470929622650146
INFO:root:current mean train loss 3350.6165728236606
INFO:root:current train perplexity3.7462961673736572
INFO:root:current mean train loss 3350.0776830428686
INFO:root:current train perplexity3.746150016784668

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.10s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.10s/it]
INFO:root:final mean train loss: 3347.9223863540155
INFO:root:final train perplexity: 3.7466132640838623
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.01s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.01s/it]
INFO:root:eval mean loss: 4033.2895715591753
INFO:root:eval perplexity: 5.108694553375244
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/123
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 123/200 [5:16:00<3:41:43, 172.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3328.2811029273344
INFO:root:current train perplexity3.7323250770568848
INFO:root:current mean train loss 3321.852904606387
INFO:root:current train perplexity3.729034423828125
INFO:root:current mean train loss 3327.286046543728
INFO:root:current train perplexity3.7266902923583984
INFO:root:current mean train loss 3338.170670232947
INFO:root:current train perplexity3.7375218868255615
INFO:root:current mean train loss 3340.2524247258348
INFO:root:current train perplexity3.7417190074920654
INFO:root:current mean train loss 3349.5440839240728
INFO:root:current train perplexity3.744072675704956
INFO:root:current mean train loss 3351.4334236022146
INFO:root:current train perplexity3.744986057281494
INFO:root:current mean train loss 3349.227799728608
INFO:root:current train perplexity3.741405487060547
INFO:root:current mean train loss 3348.833186148517
INFO:root:current train perplexity3.7427563667297363
INFO:root:current mean train loss 3348.808522221516
INFO:root:current train perplexity3.743896484375

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:57<00:00, 117.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:57<00:00, 117.96s/it]
INFO:root:final mean train loss: 3346.027405892649
INFO:root:final train perplexity: 3.7438130378723145
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.94s/it]
INFO:root:eval mean loss: 4035.3861248476287
INFO:root:eval perplexity: 5.113027572631836
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/124
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/200 [5:18:42<3:34:44, 169.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3309.2105458018546
INFO:root:current train perplexity3.677945137023926
INFO:root:current mean train loss 3335.855494314463
INFO:root:current train perplexity3.7194178104400635
INFO:root:current mean train loss 3337.0558444413123
INFO:root:current train perplexity3.720395565032959
INFO:root:current mean train loss 3338.6469720318496
INFO:root:current train perplexity3.720277786254883
INFO:root:current mean train loss 3337.897250608611
INFO:root:current train perplexity3.72192645072937
INFO:root:current mean train loss 3341.25511951737
INFO:root:current train perplexity3.728984832763672
INFO:root:current mean train loss 3342.961063986749
INFO:root:current train perplexity3.730834484100342
INFO:root:current mean train loss 3346.044986073799
INFO:root:current train perplexity3.7326107025146484
INFO:root:current mean train loss 3344.144218333509
INFO:root:current train perplexity3.735137701034546
INFO:root:current mean train loss 3343.6859097601064
INFO:root:current train perplexity3.736429452896118

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.73s/it]
INFO:root:final mean train loss: 3340.9851287718743
INFO:root:final train perplexity: 3.736372947692871
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.55s/it]
INFO:root:eval mean loss: 4034.715948443041
INFO:root:eval perplexity: 5.111641883850098
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/125
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 125/200 [5:21:32<3:31:56, 169.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3342.729827572601
INFO:root:current train perplexity3.7277119159698486
INFO:root:current mean train loss 3330.5764380986966
INFO:root:current train perplexity3.7250020503997803
INFO:root:current mean train loss 3334.340713347878
INFO:root:current train perplexity3.726867914199829
INFO:root:current mean train loss 3335.0566075834117
INFO:root:current train perplexity3.727210521697998
INFO:root:current mean train loss 3334.5815008924096
INFO:root:current train perplexity3.7245922088623047
INFO:root:current mean train loss 3337.1104000645605
INFO:root:current train perplexity3.7252445220947266
INFO:root:current mean train loss 3336.870957883472
INFO:root:current train perplexity3.727187395095825
INFO:root:current mean train loss 3339.8881652602863
INFO:root:current train perplexity3.729466199874878
INFO:root:current mean train loss 3341.3380790939586
INFO:root:current train perplexity3.732980489730835

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.69s/it]
INFO:root:final mean train loss: 3338.9850204221666
INFO:root:final train perplexity: 3.7334256172180176
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.83s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.83s/it]
INFO:root:eval mean loss: 4036.1516147911125
INFO:root:eval perplexity: 5.1146111488342285
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/126
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/200 [5:24:27<3:31:11, 171.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3345.7210867745534
INFO:root:current train perplexity3.7847769260406494
INFO:root:current mean train loss 3325.509113062208
INFO:root:current train perplexity3.713512659072876
INFO:root:current mean train loss 3332.794711937651
INFO:root:current train perplexity3.718369722366333
INFO:root:current mean train loss 3330.69515186024
INFO:root:current train perplexity3.71714448928833
INFO:root:current mean train loss 3324.4123661125614
INFO:root:current train perplexity3.7180142402648926
INFO:root:current mean train loss 3329.408606173724
INFO:root:current train perplexity3.721906900405884
INFO:root:current mean train loss 3331.317040130766
INFO:root:current train perplexity3.7210378646850586
INFO:root:current mean train loss 3335.5373956445587
INFO:root:current train perplexity3.7255868911743164
INFO:root:current mean train loss 3336.620908602463
INFO:root:current train perplexity3.726438045501709
INFO:root:current mean train loss 3336.4657411754065
INFO:root:current train perplexity3.7246921062469482

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.84s/it]
INFO:root:final mean train loss: 3334.179830428093
INFO:root:final train perplexity: 3.7263543605804443
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.11s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.11s/it]
INFO:root:eval mean loss: 4038.481478210882
INFO:root:eval perplexity: 5.119431018829346
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/127
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 127/200 [5:26:43<3:15:25, 160.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3302.112890625
INFO:root:current train perplexity3.7426321506500244
INFO:root:current mean train loss 3314.8729110054346
INFO:root:current train perplexity3.731454372406006
INFO:root:current mean train loss 3319.762071902253
INFO:root:current train perplexity3.7271785736083984
INFO:root:current mean train loss 3325.92580140129
INFO:root:current train perplexity3.7207894325256348
INFO:root:current mean train loss 3328.319379471009
INFO:root:current train perplexity3.71907901763916
INFO:root:current mean train loss 3327.047094489533
INFO:root:current train perplexity3.7190892696380615
INFO:root:current mean train loss 3328.804098386687
INFO:root:current train perplexity3.717555284500122
INFO:root:current mean train loss 3330.3121776660837
INFO:root:current train perplexity3.7192490100860596
INFO:root:current mean train loss 3329.666661773869
INFO:root:current train perplexity3.717616319656372
INFO:root:current mean train loss 3332.2522036693135
INFO:root:current train perplexity3.721158504486084

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.51s/it]
INFO:root:final mean train loss: 3331.6369685511436
INFO:root:final train perplexity: 3.7226173877716064
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.67s/it]
INFO:root:eval mean loss: 4038.657623074579
INFO:root:eval perplexity: 5.1197967529296875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/128
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/200 [5:29:26<3:13:50, 161.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3286.473685886549
INFO:root:current train perplexity3.702094316482544
INFO:root:current mean train loss 3331.604738313008
INFO:root:current train perplexity3.7118775844573975
INFO:root:current mean train loss 3328.887448982273
INFO:root:current train perplexity3.7105600833892822
INFO:root:current mean train loss 3329.3917497097523
INFO:root:current train perplexity3.717925786972046
INFO:root:current mean train loss 3326.8724656702498
INFO:root:current train perplexity3.7112295627593994
INFO:root:current mean train loss 3325.92734085579
INFO:root:current train perplexity3.7137436866760254
INFO:root:current mean train loss 3327.124810722437
INFO:root:current train perplexity3.714754581451416
INFO:root:current mean train loss 3328.210740971862
INFO:root:current train perplexity3.7155215740203857
INFO:root:current mean train loss 3326.5028246744
INFO:root:current train perplexity3.7135064601898193
INFO:root:current mean train loss 3327.4829559160853
INFO:root:current train perplexity3.7134807109832764

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:57<00:00, 117.95s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:57<00:00, 117.95s/it]
INFO:root:final mean train loss: 3326.3731943561183
INFO:root:final train perplexity: 3.714895248413086
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.83s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.83s/it]
INFO:root:eval mean loss: 4039.955253005873
INFO:root:eval perplexity: 5.122483253479004
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/129
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 129/200 [5:32:16<3:13:52, 163.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3326.8932239163305
INFO:root:current train perplexity3.6821978092193604
INFO:root:current mean train loss 3318.909690332777
INFO:root:current train perplexity3.703075408935547
INFO:root:current mean train loss 3329.9464983258927
INFO:root:current train perplexity3.713085889816284
INFO:root:current mean train loss 3330.0969857852624
INFO:root:current train perplexity3.7139172554016113
INFO:root:current mean train loss 3327.144063927458
INFO:root:current train perplexity3.714998960494995
INFO:root:current mean train loss 3329.505707649188
INFO:root:current train perplexity3.7187373638153076
INFO:root:current mean train loss 3328.3528230549227
INFO:root:current train perplexity3.7144792079925537
INFO:root:current mean train loss 3328.9823109930744
INFO:root:current train perplexity3.713512659072876
INFO:root:current mean train loss 3325.7456442492103
INFO:root:current train perplexity3.711015462875366
INFO:root:current mean train loss 3325.67955821823
INFO:root:current train perplexity3.7105791568756104

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.12s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.12s/it]
INFO:root:final mean train loss: 3324.5189978691837
INFO:root:final train perplexity: 3.712178945541382
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.81s/it]
INFO:root:eval mean loss: 4040.8993759696364
INFO:root:eval perplexity: 5.124438285827637
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/130
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/200 [5:35:07<3:13:43, 166.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3323.990215594952
INFO:root:current train perplexity3.70560884475708
INFO:root:current mean train loss 3325.8516678844426
INFO:root:current train perplexity3.710923433303833
INFO:root:current mean train loss 3322.2406082472544
INFO:root:current train perplexity3.7020678520202637
INFO:root:current mean train loss 3325.357313127996
INFO:root:current train perplexity3.706570625305176
INFO:root:current mean train loss 3325.283084113397
INFO:root:current train perplexity3.709089756011963
INFO:root:current mean train loss 3318.584138831314
INFO:root:current train perplexity3.7056071758270264
INFO:root:current mean train loss 3318.4623954665494
INFO:root:current train perplexity3.705871343612671
INFO:root:current mean train loss 3319.0554869862144
INFO:root:current train perplexity3.7042105197906494
INFO:root:current mean train loss 3323.1240400239312
INFO:root:current train perplexity3.7070095539093018
INFO:root:current mean train loss 3323.7017800165568
INFO:root:current train perplexity3.708472490310669

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.96s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.96s/it]
INFO:root:final mean train loss: 3321.666844460272
INFO:root:final train perplexity: 3.7080044746398926
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.89s/it]
INFO:root:eval mean loss: 4043.7335767813606
INFO:root:eval perplexity: 5.130314826965332
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/131
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/200 [5:37:26<3:01:41, 157.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3280.584415516955
INFO:root:current train perplexity3.6914947032928467
INFO:root:current mean train loss 3313.526353900935
INFO:root:current train perplexity3.689053773880005
INFO:root:current mean train loss 3314.9907216678266
INFO:root:current train perplexity3.700608730316162
INFO:root:current mean train loss 3313.8117225493065
INFO:root:current train perplexity3.693648099899292
INFO:root:current mean train loss 3316.318619900902
INFO:root:current train perplexity3.6950511932373047
INFO:root:current mean train loss 3317.5428281571353
INFO:root:current train perplexity3.6967201232910156
INFO:root:current mean train loss 3318.108338440036
INFO:root:current train perplexity3.6997313499450684
INFO:root:current mean train loss 3318.983396476531
INFO:root:current train perplexity3.7012250423431396
INFO:root:current mean train loss 3318.245019761843
INFO:root:current train perplexity3.7005491256713867
INFO:root:current mean train loss 3319.8128093651003
INFO:root:current train perplexity3.701643228530884

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.53s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.53s/it]
INFO:root:final mean train loss: 3318.2127022897043
INFO:root:final train perplexity: 3.702954053878784
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.87s/it]
INFO:root:eval mean loss: 4042.604112990359
INFO:root:eval perplexity: 5.1279730796813965
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/132
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/200 [5:40:12<3:01:45, 160.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3284.484978693182
INFO:root:current train perplexity3.6739373207092285
INFO:root:current mean train loss 3287.7670914188507
INFO:root:current train perplexity3.6797752380371094
INFO:root:current mean train loss 3292.6480899586395
INFO:root:current train perplexity3.686859369277954
INFO:root:current mean train loss 3294.8001815580988
INFO:root:current train perplexity3.6818788051605225
INFO:root:current mean train loss 3300.1859621823487
INFO:root:current train perplexity3.682893753051758
INFO:root:current mean train loss 3304.3461245425115
INFO:root:current train perplexity3.6859560012817383
INFO:root:current mean train loss 3306.77356870229
INFO:root:current train perplexity3.6899659633636475
INFO:root:current mean train loss 3309.197456410389
INFO:root:current train perplexity3.6916370391845703
INFO:root:current mean train loss 3312.3913788377195
INFO:root:current train perplexity3.693082094192505
INFO:root:current mean train loss 3314.4055687070518
INFO:root:current train perplexity3.695554256439209

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.80s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.80s/it]
INFO:root:final mean train loss: 3314.47662298141
INFO:root:final train perplexity: 3.697499990463257
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.80s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.80s/it]
INFO:root:eval mean loss: 4045.129623088431
INFO:root:eval perplexity: 5.133212089538574
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/133
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/200 [5:43:01<3:02:01, 163.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3299.5891152033732
INFO:root:current train perplexity3.6893370151519775
INFO:root:current mean train loss 3301.551605037385
INFO:root:current train perplexity3.688164472579956
INFO:root:current mean train loss 3308.168468170746
INFO:root:current train perplexity3.6952273845672607
INFO:root:current mean train loss 3312.151739115229
INFO:root:current train perplexity3.698500633239746
INFO:root:current mean train loss 3313.964777310003
INFO:root:current train perplexity3.696887254714966
INFO:root:current mean train loss 3319.953077299345
INFO:root:current train perplexity3.6980791091918945
INFO:root:current mean train loss 3317.5269633619673
INFO:root:current train perplexity3.6955718994140625
INFO:root:current mean train loss 3315.347341075012
INFO:root:current train perplexity3.6939449310302734
INFO:root:current mean train loss 3315.035719499113
INFO:root:current train perplexity3.694448232650757
INFO:root:current mean train loss 3316.547294323566
INFO:root:current train perplexity3.6962642669677734

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.40s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.40s/it]
INFO:root:final mean train loss: 3313.942599758025
INFO:root:final train perplexity: 3.696721076965332
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.12s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.12s/it]
INFO:root:eval mean loss: 4047.3522395140735
INFO:root:eval perplexity: 5.137828826904297
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/134
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 134/200 [5:45:55<3:02:54, 166.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3310.2112332196302
INFO:root:current train perplexity3.688992977142334
INFO:root:current mean train loss 3296.985287314967
INFO:root:current train perplexity3.6723504066467285
INFO:root:current mean train loss 3297.7113780341906
INFO:root:current train perplexity3.674572467803955
INFO:root:current mean train loss 3300.9177134223382
INFO:root:current train perplexity3.6781346797943115
INFO:root:current mean train loss 3305.0692405619693
INFO:root:current train perplexity3.6824257373809814
INFO:root:current mean train loss 3310.913634078098
INFO:root:current train perplexity3.6849915981292725
INFO:root:current mean train loss 3314.937820184426
INFO:root:current train perplexity3.691605567932129
INFO:root:current mean train loss 3313.110969038789
INFO:root:current train perplexity3.6906254291534424
INFO:root:current mean train loss 3313.92941617035
INFO:root:current train perplexity3.6922099590301514
INFO:root:current mean train loss 3312.108414277726
INFO:root:current train perplexity3.690282106399536

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.37s/it]
INFO:root:final mean train loss: 3309.9992320768297
INFO:root:final train perplexity: 3.690974235534668
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.12s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.12s/it]
INFO:root:eval mean loss: 4045.8098594719636
INFO:root:eval perplexity: 5.134624481201172
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/135
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/200 [5:48:46<3:01:42, 167.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3314.3396675979034
INFO:root:current train perplexity3.6833345890045166
INFO:root:current mean train loss 3306.400521560754
INFO:root:current train perplexity3.695014715194702
INFO:root:current mean train loss 3304.867679281474
INFO:root:current train perplexity3.6877360343933105
INFO:root:current mean train loss 3307.8023092224603
INFO:root:current train perplexity3.6869916915893555
INFO:root:current mean train loss 3310.26231457545
INFO:root:current train perplexity3.687954902648926
INFO:root:current mean train loss 3311.60666331026
INFO:root:current train perplexity3.6877079010009766
INFO:root:current mean train loss 3314.5316868642994
INFO:root:current train perplexity3.688292980194092
INFO:root:current mean train loss 3313.7212443712892
INFO:root:current train perplexity3.6860592365264893
INFO:root:current mean train loss 3312.3266165497903
INFO:root:current train perplexity3.6861202716827393
INFO:root:current mean train loss 3309.0749135158644
INFO:root:current train perplexity3.685319662094116

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:57<00:00, 117.20s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:57<00:00, 117.20s/it]
INFO:root:final mean train loss: 3306.5522780264578
INFO:root:final train perplexity: 3.6859586238861084
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.64s/it]
INFO:root:eval mean loss: 4048.156411028923
INFO:root:eval perplexity: 5.139498233795166
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/136
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/200 [5:51:37<2:59:49, 168.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3283.3621194773705
INFO:root:current train perplexity3.6676411628723145
INFO:root:current mean train loss 3298.797924674131
INFO:root:current train perplexity3.675274133682251
INFO:root:current mean train loss 3297.323214966246
INFO:root:current train perplexity3.672139883041382
INFO:root:current mean train loss 3305.3902370245883
INFO:root:current train perplexity3.678210973739624
INFO:root:current mean train loss 3308.9854874189873
INFO:root:current train perplexity3.6805224418640137
INFO:root:current mean train loss 3308.2877016343696
INFO:root:current train perplexity3.678792715072632
INFO:root:current mean train loss 3307.780341668941
INFO:root:current train perplexity3.679098129272461
INFO:root:current mean train loss 3304.9714550905337
INFO:root:current train perplexity3.677586078643799
INFO:root:current mean train loss 3305.2905347753135
INFO:root:current train perplexity3.678957223892212
INFO:root:current mean train loss 3306.4091913132443
INFO:root:current train perplexity3.6822268962860107

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.63s/it]
INFO:root:final mean train loss: 3304.094563391901
INFO:root:final train perplexity: 3.6823861598968506
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.11s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.11s/it]
INFO:root:eval mean loss: 4048.017806682181
INFO:root:eval perplexity: 5.139211177825928
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/137
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 137/200 [5:54:31<2:58:56, 170.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3281.9716282894738
INFO:root:current train perplexity3.656769275665283
INFO:root:current mean train loss 3294.1815317007213
INFO:root:current train perplexity3.661759614944458
INFO:root:current mean train loss 3297.12824748411
INFO:root:current train perplexity3.6682801246643066
INFO:root:current mean train loss 3297.3315145371835
INFO:root:current train perplexity3.6718344688415527
INFO:root:current mean train loss 3296.7495763297034
INFO:root:current train perplexity3.6734097003936768
INFO:root:current mean train loss 3299.8196998916756
INFO:root:current train perplexity3.6765542030334473
INFO:root:current mean train loss 3298.0231462876573
INFO:root:current train perplexity3.674874782562256
INFO:root:current mean train loss 3302.017934662441
INFO:root:current train perplexity3.6785576343536377
INFO:root:current mean train loss 3302.5930203059534
INFO:root:current train perplexity3.6780319213867188

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.50s/it]
INFO:root:final mean train loss: 3301.2109805691625
INFO:root:final train perplexity: 3.678199052810669
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.83s/it]
INFO:root:eval mean loss: 4048.50590958832
INFO:root:eval perplexity: 5.140226364135742
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/138
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/200 [5:57:24<2:56:40, 170.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3358.0906575520835
INFO:root:current train perplexity3.5373928546905518
INFO:root:current mean train loss 3275.2902666110435
INFO:root:current train perplexity3.6403229236602783
INFO:root:current mean train loss 3279.449474917257
INFO:root:current train perplexity3.6479077339172363
INFO:root:current mean train loss 3285.3074503983603
INFO:root:current train perplexity3.65726375579834
INFO:root:current mean train loss 3290.5438356612517
INFO:root:current train perplexity3.6560707092285156
INFO:root:current mean train loss 3291.974759839401
INFO:root:current train perplexity3.657050609588623
INFO:root:current mean train loss 3294.0634417431074
INFO:root:current train perplexity3.659783363342285
INFO:root:current mean train loss 3295.072002383757
INFO:root:current train perplexity3.6633729934692383
INFO:root:current mean train loss 3297.780955693493
INFO:root:current train perplexity3.666734457015991
INFO:root:current mean train loss 3298.183935492802
INFO:root:current train perplexity3.6691412925720215

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.51s/it]
INFO:root:final mean train loss: 3297.315495460264
INFO:root:final train perplexity: 3.6725504398345947
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.32s/it]
INFO:root:eval mean loss: 4051.428503158245
INFO:root:eval perplexity: 5.146304130554199
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/139
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/200 [6:00:17<2:54:25, 171.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3348.5685591264205
INFO:root:current train perplexity3.611881732940674
INFO:root:current mean train loss 3294.8392740885415
INFO:root:current train perplexity3.668339967727661
INFO:root:current mean train loss 3288.512239429058
INFO:root:current train perplexity3.6547834873199463
INFO:root:current mean train loss 3279.724444521202
INFO:root:current train perplexity3.6459288597106934
INFO:root:current mean train loss 3284.485128806455
INFO:root:current train perplexity3.6510815620422363
INFO:root:current mean train loss 3286.1628695075524
INFO:root:current train perplexity3.6585278511047363
INFO:root:current mean train loss 3289.076140308536
INFO:root:current train perplexity3.659743547439575
INFO:root:current mean train loss 3291.7388196642055
INFO:root:current train perplexity3.6603474617004395
INFO:root:current mean train loss 3293.8171037516377
INFO:root:current train perplexity3.6648812294006348
INFO:root:current mean train loss 3296.0862649861074
INFO:root:current train perplexity3.6660470962524414

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.71s/it]
INFO:root:final mean train loss: 3293.829595258159
INFO:root:final train perplexity: 3.6675026416778564
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.70s/it]
INFO:root:eval mean loss: 4052.624686599623
INFO:root:eval perplexity: 5.148794651031494
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/140
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/200 [6:03:07<2:51:20, 171.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3286.8171900699012
INFO:root:current train perplexity3.6627678871154785
INFO:root:current mean train loss 3272.3884523535976
INFO:root:current train perplexity3.647336959838867
INFO:root:current mean train loss 3272.8546337667667
INFO:root:current train perplexity3.657956600189209
INFO:root:current mean train loss 3280.451672401548
INFO:root:current train perplexity3.6583359241485596
INFO:root:current mean train loss 3284.975176899985
INFO:root:current train perplexity3.660658121109009
INFO:root:current mean train loss 3288.165648238048
INFO:root:current train perplexity3.662412405014038
INFO:root:current mean train loss 3286.8519640107534
INFO:root:current train perplexity3.6613337993621826
INFO:root:current mean train loss 3289.4355526474487
INFO:root:current train perplexity3.6620547771453857
INFO:root:current mean train loss 3290.698494078621
INFO:root:current train perplexity3.66225004196167
INFO:root:current mean train loss 3292.4452880593717
INFO:root:current train perplexity3.6624886989593506

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.45s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.45s/it]
INFO:root:final mean train loss: 3292.550885538901
INFO:root:final train perplexity: 3.6656532287597656
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.86s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.86s/it]
INFO:root:eval mean loss: 4051.834420711436
INFO:root:eval perplexity: 5.147148609161377
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/141
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/200 [6:06:11<2:52:01, 174.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3240.2704445167824
INFO:root:current train perplexity3.659965753555298
INFO:root:current mean train loss 3285.5287278543306
INFO:root:current train perplexity3.6628897190093994
INFO:root:current mean train loss 3286.1278716099946
INFO:root:current train perplexity3.662309408187866
INFO:root:current mean train loss 3285.860382173404
INFO:root:current train perplexity3.6636245250701904
INFO:root:current mean train loss 3290.7800910467286
INFO:root:current train perplexity3.663952589035034
INFO:root:current mean train loss 3290.6895365126898
INFO:root:current train perplexity3.6615257263183594
INFO:root:current mean train loss 3292.1554342510217
INFO:root:current train perplexity3.664198398590088
INFO:root:current mean train loss 3291.9028844190593
INFO:root:current train perplexity3.6638996601104736
INFO:root:current mean train loss 3292.5228027934177
INFO:root:current train perplexity3.664976119995117
INFO:root:current mean train loss 3291.7717485314693
INFO:root:current train perplexity3.66205096244812

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.82s/it]
INFO:root:final mean train loss: 3289.829840998496
INFO:root:final train perplexity: 3.6617205142974854
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.72s/it]
INFO:root:eval mean loss: 4053.634571697695
INFO:root:eval perplexity: 5.1508965492248535
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/142
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 142/200 [6:08:49<2:44:12, 169.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3343.6972865513394
INFO:root:current train perplexity3.6539392471313477
INFO:root:current mean train loss 3308.254859302662
INFO:root:current train perplexity3.6393492221832275
INFO:root:current mean train loss 3297.2726250831115
INFO:root:current train perplexity3.6414644718170166
INFO:root:current mean train loss 3295.2629503847947
INFO:root:current train perplexity3.6435747146606445
INFO:root:current mean train loss 3288.9720590876436
INFO:root:current train perplexity3.646393060684204
INFO:root:current mean train loss 3293.699418625876
INFO:root:current train perplexity3.651348829269409
INFO:root:current mean train loss 3289.8087698388285
INFO:root:current train perplexity3.647794723510742
INFO:root:current mean train loss 3287.5586757945366
INFO:root:current train perplexity3.649371385574341
INFO:root:current mean train loss 3286.189661302395
INFO:root:current train perplexity3.651972532272339
INFO:root:current mean train loss 3286.9030202936997
INFO:root:current train perplexity3.6545934677124023

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.28s/it]
INFO:root:final mean train loss: 3285.72724816107
INFO:root:final train perplexity: 3.6557981967926025
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.06s/it]
INFO:root:eval mean loss: 4054.9543058787676
INFO:root:eval perplexity: 5.153645992279053
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/143
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 143/200 [6:11:19<2:35:50, 164.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3278.5105605014537
INFO:root:current train perplexity3.6298601627349854
INFO:root:current mean train loss 3272.6943154501746
INFO:root:current train perplexity3.63065767288208
INFO:root:current mean train loss 3271.171804671425
INFO:root:current train perplexity3.633687734603882
INFO:root:current mean train loss 3276.2392314766307
INFO:root:current train perplexity3.63510799407959
INFO:root:current mean train loss 3277.8648414353484
INFO:root:current train perplexity3.63920521736145
INFO:root:current mean train loss 3280.4191678716334
INFO:root:current train perplexity3.643198251724243
INFO:root:current mean train loss 3282.661581605997
INFO:root:current train perplexity3.6454079151153564
INFO:root:current mean train loss 3280.2684702404736
INFO:root:current train perplexity3.646449089050293
INFO:root:current mean train loss 3282.737169150356
INFO:root:current train perplexity3.649414539337158
INFO:root:current mean train loss 3284.281880416142
INFO:root:current train perplexity3.652883529663086

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.90s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.90s/it]
INFO:root:final mean train loss: 3283.8828008097985
INFO:root:final train perplexity: 3.6531386375427246
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.88s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.88s/it]
INFO:root:eval mean loss: 4056.1094962045654
INFO:root:eval perplexity: 5.156054496765137
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/144
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/200 [6:13:47<2:28:26, 159.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3282.916446461397
INFO:root:current train perplexity3.6244962215423584
INFO:root:current mean train loss 3277.293222591577
INFO:root:current train perplexity3.6380138397216797
INFO:root:current mean train loss 3277.0872175361055
INFO:root:current train perplexity3.6357944011688232
INFO:root:current mean train loss 3271.920883830796
INFO:root:current train perplexity3.6350300312042236
INFO:root:current mean train loss 3275.7749624315757
INFO:root:current train perplexity3.637075901031494
INFO:root:current mean train loss 3277.610717551894
INFO:root:current train perplexity3.641946792602539
INFO:root:current mean train loss 3281.9621698288693
INFO:root:current train perplexity3.645174026489258
INFO:root:current mean train loss 3284.0932887010026
INFO:root:current train perplexity3.6481103897094727
INFO:root:current mean train loss 3283.563222093952
INFO:root:current train perplexity3.648268461227417
INFO:root:current mean train loss 3282.052133907154
INFO:root:current train perplexity3.6482536792755127

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.56s/it]
INFO:root:final mean train loss: 3280.8616416685045
INFO:root:final train perplexity: 3.648787260055542
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.21s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.21s/it]
INFO:root:eval mean loss: 4056.398092932735
INFO:root:eval perplexity: 5.156655788421631
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/145
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 145/200 [6:16:03<2:19:29, 152.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3282.1653452727755
INFO:root:current train perplexity3.6294355392456055
INFO:root:current mean train loss 3273.477932144261
INFO:root:current train perplexity3.628370523452759
INFO:root:current mean train loss 3268.8088680547175
INFO:root:current train perplexity3.6312007904052734
INFO:root:current mean train loss 3272.394644139537
INFO:root:current train perplexity3.6377079486846924
INFO:root:current mean train loss 3275.4643022790715
INFO:root:current train perplexity3.639371871948242
INFO:root:current mean train loss 3277.1173036742507
INFO:root:current train perplexity3.6415321826934814
INFO:root:current mean train loss 3281.6307163730557
INFO:root:current train perplexity3.6435279846191406
INFO:root:current mean train loss 3281.4270929831605
INFO:root:current train perplexity3.6453564167022705
INFO:root:current mean train loss 3282.0528307238615
INFO:root:current train perplexity3.6461806297302246
INFO:root:current mean train loss 3281.3314207711483
INFO:root:current train perplexity3.6463165283203125

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.59s/it]
INFO:root:final mean train loss: 3279.429405950731
INFO:root:final train perplexity: 3.646726131439209
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.06s/it]
INFO:root:eval mean loss: 4058.30039339539
INFO:root:eval perplexity: 5.160623550415039
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/146
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/200 [6:18:53<2:21:57, 157.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3272.5162408173974
INFO:root:current train perplexity3.6333625316619873
INFO:root:current mean train loss 3284.505795050524
INFO:root:current train perplexity3.6270389556884766
INFO:root:current mean train loss 3280.885055484843
INFO:root:current train perplexity3.6328959465026855
INFO:root:current mean train loss 3281.9452040669703
INFO:root:current train perplexity3.6307437419891357
INFO:root:current mean train loss 3278.455541835352
INFO:root:current train perplexity3.631909132003784
INFO:root:current mean train loss 3275.4963620032795
INFO:root:current train perplexity3.633385419845581
INFO:root:current mean train loss 3276.6764785331943
INFO:root:current train perplexity3.634636402130127
INFO:root:current mean train loss 3276.940648681959
INFO:root:current train perplexity3.6351656913757324
INFO:root:current mean train loss 3278.177841943303
INFO:root:current train perplexity3.6390013694763184
INFO:root:current mean train loss 3278.993523582924
INFO:root:current train perplexity3.6410486698150635

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.50s/it]
INFO:root:final mean train loss: 3275.8012355066116
INFO:root:final train perplexity: 3.641509532928467
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00, 10.00s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00, 10.00s/it]
INFO:root:eval mean loss: 4057.891544423205
INFO:root:eval perplexity: 5.159770965576172
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/147
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 147/200 [6:21:03<2:11:47, 149.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3250.8246549479168
INFO:root:current train perplexity3.610874891281128
INFO:root:current mean train loss 3270.722403738839
INFO:root:current train perplexity3.634040355682373
INFO:root:current mean train loss 3276.0219770951703
INFO:root:current train perplexity3.6393353939056396
INFO:root:current mean train loss 3270.676507161458
INFO:root:current train perplexity3.6340343952178955
INFO:root:current mean train loss 3269.578792660362
INFO:root:current train perplexity3.630366325378418
INFO:root:current mean train loss 3270.9945571501357
INFO:root:current train perplexity3.630504846572876
INFO:root:current mean train loss 3274.115593894676
INFO:root:current train perplexity3.632802963256836
INFO:root:current mean train loss 3274.812425970262
INFO:root:current train perplexity3.6324656009674072
INFO:root:current mean train loss 3278.700345424107
INFO:root:current train perplexity3.637622117996216
INFO:root:current mean train loss 3277.1613619290865
INFO:root:current train perplexity3.6387367248535156

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:57<00:00, 117.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:57<00:00, 117.74s/it]
INFO:root:final mean train loss: 3274.5122713888845
INFO:root:final train perplexity: 3.6396586894989014
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.58s/it]
INFO:root:eval mean loss: 4057.90658764129
INFO:root:eval perplexity: 5.15980339050293
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/148
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/200 [6:23:49<2:13:38, 154.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3275.2354486304594
INFO:root:current train perplexity3.6364870071411133
INFO:root:current mean train loss 3268.631614476605
INFO:root:current train perplexity3.6346840858459473
INFO:root:current mean train loss 3269.7113904110533
INFO:root:current train perplexity3.631303071975708
INFO:root:current mean train loss 3272.2701546691414
INFO:root:current train perplexity3.631101369857788
INFO:root:current mean train loss 3272.608533397224
INFO:root:current train perplexity3.630384922027588
INFO:root:current mean train loss 3271.259364865861
INFO:root:current train perplexity3.6326704025268555
INFO:root:current mean train loss 3271.398312391334
INFO:root:current train perplexity3.632153272628784
INFO:root:current mean train loss 3271.226815994672
INFO:root:current train perplexity3.632108211517334
INFO:root:current mean train loss 3272.1596488909436
INFO:root:current train perplexity3.633622407913208
INFO:root:current mean train loss 3274.1706813684195
INFO:root:current train perplexity3.6354074478149414

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.68s/it]
INFO:root:final mean train loss: 3271.889472715316
INFO:root:final train perplexity: 3.6358940601348877
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.85s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.85s/it]
INFO:root:eval mean loss: 4060.1519541638963
INFO:root:eval perplexity: 5.164489269256592
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/149
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 149/200 [6:26:39<2:15:08, 159.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3260.2867659684066
INFO:root:current train perplexity3.609830856323242
INFO:root:current mean train loss 3256.9255869600784
INFO:root:current train perplexity3.6103122234344482
INFO:root:current mean train loss 3260.7589669243985
INFO:root:current train perplexity3.6173417568206787
INFO:root:current mean train loss 3262.489821021819
INFO:root:current train perplexity3.6181154251098633
INFO:root:current mean train loss 3263.702298104156
INFO:root:current train perplexity3.620967149734497
INFO:root:current mean train loss 3262.7827768083757
INFO:root:current train perplexity3.621392250061035
INFO:root:current mean train loss 3266.9066435928457
INFO:root:current train perplexity3.6247265338897705
INFO:root:current mean train loss 3267.1681082589284
INFO:root:current train perplexity3.6282126903533936
INFO:root:current mean train loss 3268.936564264608
INFO:root:current train perplexity3.6290924549102783
INFO:root:current mean train loss 3270.25243450823
INFO:root:current train perplexity3.6299309730529785

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:57<00:00, 117.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:57<00:00, 117.47s/it]
INFO:root:final mean train loss: 3267.7140792108353
INFO:root:final train perplexity: 3.6299097537994385
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.72s/it]
INFO:root:eval mean loss: 4062.46294083832
INFO:root:eval perplexity: 5.169317722320557
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/150
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/200 [6:29:26<2:14:27, 161.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3265.0496468592173
INFO:root:current train perplexity3.640148162841797
INFO:root:current mean train loss 3266.86526627277
INFO:root:current train perplexity3.6255173683166504
INFO:root:current mean train loss 3265.96697814329
INFO:root:current train perplexity3.6238839626312256
INFO:root:current mean train loss 3263.4404676241384
INFO:root:current train perplexity3.6214141845703125
INFO:root:current mean train loss 3264.7574333236785
INFO:root:current train perplexity3.621638774871826
INFO:root:current mean train loss 3266.6221497906668
INFO:root:current train perplexity3.6266236305236816
INFO:root:current mean train loss 3265.6008737370353
INFO:root:current train perplexity3.628032684326172
INFO:root:current mean train loss 3262.8728482624765
INFO:root:current train perplexity3.6262128353118896
INFO:root:current mean train loss 3267.0166216586135
INFO:root:current train perplexity3.6275813579559326

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.89s/it]
INFO:root:final mean train loss: 3267.0790416348364
INFO:root:final train perplexity: 3.629000425338745
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.41s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.41s/it]
INFO:root:eval mean loss: 4062.267292428524
INFO:root:eval perplexity: 5.168909072875977
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/151
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 151/200 [6:31:39<2:04:53, 152.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3254.6592145647323
INFO:root:current train perplexity3.619565010070801
INFO:root:current mean train loss 3268.126499069071
INFO:root:current train perplexity3.6267857551574707
INFO:root:current mean train loss 3263.8352829200635
INFO:root:current train perplexity3.6254119873046875
INFO:root:current mean train loss 3261.8538416759975
INFO:root:current train perplexity3.628300189971924
INFO:root:current mean train loss 3262.503084449862
INFO:root:current train perplexity3.6276028156280518
INFO:root:current mean train loss 3263.758336896727
INFO:root:current train perplexity3.623903512954712
INFO:root:current mean train loss 3263.3677594406404
INFO:root:current train perplexity3.624189853668213
INFO:root:current mean train loss 3268.020038523802
INFO:root:current train perplexity3.62459135055542
INFO:root:current mean train loss 3268.848745958217
INFO:root:current train perplexity3.6257572174072266
INFO:root:current mean train loss 3271.677366145259
INFO:root:current train perplexity3.6281893253326416

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.63s/it]
INFO:root:final mean train loss: 3265.830430984497
INFO:root:final train perplexity: 3.6272132396698
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.10s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.10s/it]
INFO:root:eval mean loss: 4062.343833111702
INFO:root:eval perplexity: 5.169069290161133
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/152
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 152/200 [6:33:53<1:57:44, 147.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3283.3597330729167
INFO:root:current train perplexity3.6236836910247803
INFO:root:current mean train loss 3260.352048658288
INFO:root:current train perplexity3.6134982109069824
INFO:root:current mean train loss 3260.0775685864824
INFO:root:current train perplexity3.6168277263641357
INFO:root:current mean train loss 3254.7857042100695
INFO:root:current train perplexity3.6140899658203125
INFO:root:current mean train loss 3259.667978162651
INFO:root:current train perplexity3.6152780055999756
INFO:root:current mean train loss 3262.417123501972
INFO:root:current train perplexity3.6190216541290283
INFO:root:current mean train loss 3263.5750821741617
INFO:root:current train perplexity3.616574287414551
INFO:root:current mean train loss 3266.182581334681
INFO:root:current train perplexity3.6205945014953613
INFO:root:current mean train loss 3265.0328412576687
INFO:root:current train perplexity3.6222116947174072
INFO:root:current mean train loss 3265.3471759733607
INFO:root:current train perplexity3.6232688426971436

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.71s/it]
INFO:root:final mean train loss: 3262.9790613728187
INFO:root:final train perplexity: 3.6231346130371094
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.87s/it]
INFO:root:eval mean loss: 4063.3215366661125
INFO:root:eval perplexity: 5.171112537384033
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/153
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 153/200 [6:36:05<1:51:48, 142.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3217.4385402513585
INFO:root:current train perplexity3.576507568359375
INFO:root:current mean train loss 3248.5711263020835
INFO:root:current train perplexity3.603193998336792
INFO:root:current mean train loss 3249.2776283982626
INFO:root:current train perplexity3.599456787109375
INFO:root:current mean train loss 3251.30908203125
INFO:root:current train perplexity3.601550579071045
INFO:root:current mean train loss 3252.806514803118
INFO:root:current train perplexity3.6056835651397705
INFO:root:current mean train loss 3255.4435012846557
INFO:root:current train perplexity3.60713791847229
INFO:root:current mean train loss 3255.7004664927767
INFO:root:current train perplexity3.6106412410736084
INFO:root:current mean train loss 3259.2939760411264
INFO:root:current train perplexity3.614476442337036
INFO:root:current mean train loss 3261.912258885176
INFO:root:current train perplexity3.6152191162109375
INFO:root:current mean train loss 3262.3280464412073
INFO:root:current train perplexity3.6170425415039062

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:57<00:00, 117.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:57<00:00, 117.58s/it]
INFO:root:final mean train loss: 3259.8614081721153
INFO:root:final train perplexity: 3.6186814308166504
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.97s/it]
INFO:root:eval mean loss: 4063.3241910460993
INFO:root:eval perplexity: 5.171118259429932
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/154
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/200 [6:38:13<1:46:07, 138.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3283.727767452117
INFO:root:current train perplexity3.645210027694702
INFO:root:current mean train loss 3259.6621391937024
INFO:root:current train perplexity3.6228818893432617
INFO:root:current mean train loss 3255.889923227814
INFO:root:current train perplexity3.610619306564331
INFO:root:current mean train loss 3256.4838557401813
INFO:root:current train perplexity3.6126582622528076
INFO:root:current mean train loss 3253.1920310460773
INFO:root:current train perplexity3.6150870323181152
INFO:root:current mean train loss 3252.425900791549
INFO:root:current train perplexity3.6141550540924072
INFO:root:current mean train loss 3256.694385848975
INFO:root:current train perplexity3.6177139282226562
INFO:root:current mean train loss 3259.7620363665997
INFO:root:current train perplexity3.620589017868042
INFO:root:current mean train loss 3262.2688008846644
INFO:root:current train perplexity3.6179239749908447
INFO:root:current mean train loss 3260.6575144963413
INFO:root:current train perplexity3.6172633171081543

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.19s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.19s/it]
INFO:root:final mean train loss: 3258.520492307601
INFO:root:final train perplexity: 3.616767406463623
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.81s/it]
INFO:root:eval mean loss: 4065.0620082557625
INFO:root:eval perplexity: 5.1747541427612305
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/155
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 155/200 [6:40:22<1:41:39, 135.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3299.3441443810098
INFO:root:current train perplexity3.620325803756714
INFO:root:current mean train loss 3255.3893041816546
INFO:root:current train perplexity3.599062919616699
INFO:root:current mean train loss 3258.346830870816
INFO:root:current train perplexity3.6063013076782227
INFO:root:current mean train loss 3256.0869205441095
INFO:root:current train perplexity3.6102938652038574
INFO:root:current mean train loss 3251.2457839861545
INFO:root:current train perplexity3.6113932132720947
INFO:root:current mean train loss 3253.4875542635377
INFO:root:current train perplexity3.6101229190826416
INFO:root:current mean train loss 3253.4912548751713
INFO:root:current train perplexity3.6084487438201904
INFO:root:current mean train loss 3255.480134089035
INFO:root:current train perplexity3.6087310314178467
INFO:root:current mean train loss 3256.3800757388817
INFO:root:current train perplexity3.6080362796783447
INFO:root:current mean train loss 3257.7974831415568
INFO:root:current train perplexity3.6120376586914062

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.01s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.01s/it]
INFO:root:final mean train loss: 3255.472784780687
INFO:root:final train perplexity: 3.6124210357666016
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.91s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.91s/it]
INFO:root:eval mean loss: 4065.132658397052
INFO:root:eval perplexity: 5.174901485443115
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/156
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 156/200 [6:42:32<1:38:07, 133.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3242.4176466921544
INFO:root:current train perplexity3.5560362339019775
INFO:root:current mean train loss 3255.719620269983
INFO:root:current train perplexity3.5883536338806152
INFO:root:current mean train loss 3247.4409881468246
INFO:root:current train perplexity3.591357946395874
INFO:root:current mean train loss 3249.759567920344
INFO:root:current train perplexity3.5952847003936768
INFO:root:current mean train loss 3253.065716429845
INFO:root:current train perplexity3.6017205715179443
INFO:root:current mean train loss 3248.36886345621
INFO:root:current train perplexity3.6006288528442383
INFO:root:current mean train loss 3248.947750510167
INFO:root:current train perplexity3.6027936935424805
INFO:root:current mean train loss 3251.9734308327056
INFO:root:current train perplexity3.6048738956451416
INFO:root:current mean train loss 3255.4060822433958
INFO:root:current train perplexity3.60727858543396
INFO:root:current mean train loss 3256.8730383674597
INFO:root:current train perplexity3.609532594680786

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.85s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.85s/it]
INFO:root:final mean train loss: 3253.9768149468205
INFO:root:final train perplexity: 3.6102895736694336
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.89s/it]
INFO:root:eval mean loss: 4066.5466239334
INFO:root:eval perplexity: 5.177860736846924
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/157
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 157/200 [6:44:45<1:35:37, 133.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3231.1240012428975
INFO:root:current train perplexity3.622384548187256
INFO:root:current mean train loss 3224.68031281502
INFO:root:current train perplexity3.596400022506714
INFO:root:current mean train loss 3238.621750536152
INFO:root:current train perplexity3.6002206802368164
INFO:root:current mean train loss 3244.051115481954
INFO:root:current train perplexity3.603839159011841
INFO:root:current mean train loss 3245.4815520904876
INFO:root:current train perplexity3.603501558303833
INFO:root:current mean train loss 3249.2012317004505
INFO:root:current train perplexity3.603519916534424
INFO:root:current mean train loss 3252.8507040941076
INFO:root:current train perplexity3.6063411235809326
INFO:root:current mean train loss 3252.8727901231373
INFO:root:current train perplexity3.6058249473571777
INFO:root:current mean train loss 3254.735941783169
INFO:root:current train perplexity3.607818365097046
INFO:root:current mean train loss 3253.543292396106
INFO:root:current train perplexity3.6057212352752686

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.87s/it]
INFO:root:final mean train loss: 3250.999181870491
INFO:root:final train perplexity: 3.606050491333008
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.21s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.21s/it]
INFO:root:eval mean loss: 4066.896602116578
INFO:root:eval perplexity: 5.178593635559082
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/158
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/200 [6:46:54<1:32:39, 132.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3265.0328466021824
INFO:root:current train perplexity3.6001055240631104
INFO:root:current mean train loss 3258.8361726538537
INFO:root:current train perplexity3.59516978263855
INFO:root:current mean train loss 3253.562592829135
INFO:root:current train perplexity3.593444347381592
INFO:root:current mean train loss 3245.8506969105115
INFO:root:current train perplexity3.5958242416381836
INFO:root:current mean train loss 3249.1643071679264
INFO:root:current train perplexity3.602778196334839
INFO:root:current mean train loss 3250.8503947012377
INFO:root:current train perplexity3.6027514934539795
INFO:root:current mean train loss 3249.8614875712906
INFO:root:current train perplexity3.6059353351593018
INFO:root:current mean train loss 3253.7610055266014
INFO:root:current train perplexity3.605525255203247
INFO:root:current mean train loss 3252.0625226318075
INFO:root:current train perplexity3.6046066284179688
INFO:root:current mean train loss 3252.1596418560976
INFO:root:current train perplexity3.6039328575134277

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.83s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.83s/it]
INFO:root:final mean train loss: 3249.757877411381
INFO:root:final train perplexity: 3.604285478591919
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.35s/it]
INFO:root:eval mean loss: 4067.346243351064
INFO:root:eval perplexity: 5.179535865783691
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/159
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 159/200 [6:49:05<1:30:10, 131.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3253.14833778059
INFO:root:current train perplexity3.607346534729004
INFO:root:current mean train loss 3241.1956323099416
INFO:root:current train perplexity3.588989734649658
INFO:root:current mean train loss 3242.0214582492504
INFO:root:current train perplexity3.589359998703003
INFO:root:current mean train loss 3238.6104081557446
INFO:root:current train perplexity3.594449281692505
INFO:root:current mean train loss 3243.0947840988256
INFO:root:current train perplexity3.5961170196533203
INFO:root:current mean train loss 3242.8553515967055
INFO:root:current train perplexity3.5974435806274414
INFO:root:current mean train loss 3245.164456545152
INFO:root:current train perplexity3.5956711769104004
INFO:root:current mean train loss 3247.189503789721
INFO:root:current train perplexity3.5967507362365723
INFO:root:current mean train loss 3248.4688427790434
INFO:root:current train perplexity3.600252866744995
INFO:root:current mean train loss 3248.5365346010876
INFO:root:current train perplexity3.5999531745910645

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.61s/it]
INFO:root:final mean train loss: 3246.775577545166
INFO:root:final train perplexity: 3.6000468730926514
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.19s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.19s/it]
INFO:root:eval mean loss: 4068.2539841672206
INFO:root:eval perplexity: 5.181438446044922
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/160
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 160/200 [6:51:16<1:27:42, 131.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3243.5515167622625
INFO:root:current train perplexity3.584411382675171
INFO:root:current mean train loss 3238.866801512308
INFO:root:current train perplexity3.5974860191345215
INFO:root:current mean train loss 3245.2899113043236
INFO:root:current train perplexity3.6004116535186768
INFO:root:current mean train loss 3250.5587773385964
INFO:root:current train perplexity3.598262310028076
INFO:root:current mean train loss 3247.931678341923
INFO:root:current train perplexity3.5961852073669434
INFO:root:current mean train loss 3249.460342960654
INFO:root:current train perplexity3.5963518619537354
INFO:root:current mean train loss 3245.9261760458853
INFO:root:current train perplexity3.5976040363311768
INFO:root:current mean train loss 3247.1486528075857
INFO:root:current train perplexity3.598048686981201
INFO:root:current mean train loss 3245.7726639713987
INFO:root:current train perplexity3.5985536575317383
INFO:root:current mean train loss 3247.5442829697076
INFO:root:current train perplexity3.598708152770996

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.11s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.11s/it]
INFO:root:final mean train loss: 3245.5382807331703
INFO:root:final train perplexity: 3.598290205001831
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.16s/it]
INFO:root:eval mean loss: 4069.607077307735
INFO:root:eval perplexity: 5.184272289276123
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/161
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/200 [6:53:27<1:25:25, 131.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3234.6828360721984
INFO:root:current train perplexity3.5677990913391113
INFO:root:current mean train loss 3231.5763977377173
INFO:root:current train perplexity3.572089433670044
INFO:root:current mean train loss 3237.8384248421166
INFO:root:current train perplexity3.576087474822998
INFO:root:current mean train loss 3244.1399935148174
INFO:root:current train perplexity3.5861399173736572
INFO:root:current mean train loss 3241.7632021424215
INFO:root:current train perplexity3.5866520404815674
INFO:root:current mean train loss 3242.575669868638
INFO:root:current train perplexity3.5857412815093994
INFO:root:current mean train loss 3241.5870533683587
INFO:root:current train perplexity3.5863513946533203
INFO:root:current mean train loss 3242.876173239954
INFO:root:current train perplexity3.5888092517852783
INFO:root:current mean train loss 3243.2820735273394
INFO:root:current train perplexity3.5910346508026123
INFO:root:current mean train loss 3245.777027133992
INFO:root:current train perplexity3.595141887664795

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.84s/it]
INFO:root:final mean train loss: 3242.94748527773
INFO:root:final train perplexity: 3.594614267349243
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.94s/it]
INFO:root:eval mean loss: 4069.6628037040114
INFO:root:eval perplexity: 5.184390544891357
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/162
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 162/200 [6:55:39<1:23:16, 131.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3239.9382761101974
INFO:root:current train perplexity3.593735456466675
INFO:root:current mean train loss 3247.5415840344554
INFO:root:current train perplexity3.5999810695648193
INFO:root:current mean train loss 3244.002065677966
INFO:root:current train perplexity3.590576171875
INFO:root:current mean train loss 3247.882312475277
INFO:root:current train perplexity3.5888900756835938
INFO:root:current mean train loss 3250.0106874408143
INFO:root:current train perplexity3.5927915573120117
INFO:root:current mean train loss 3248.2929096638654
INFO:root:current train perplexity3.5912764072418213
INFO:root:current mean train loss 3247.0152347262815
INFO:root:current train perplexity3.589655637741089
INFO:root:current mean train loss 3244.9024939195165
INFO:root:current train perplexity3.590857744216919
INFO:root:current mean train loss 3245.226954216131
INFO:root:current train perplexity3.5922086238861084

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.91s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.91s/it]
INFO:root:final mean train loss: 3241.4288890592516
INFO:root:final train perplexity: 3.592460870742798
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.76s/it]
INFO:root:eval mean loss: 4070.235325590093
INFO:root:eval perplexity: 5.1855902671813965
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/163
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/200 [6:57:48<1:20:43, 130.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3252.9140625
INFO:root:current train perplexity3.587021827697754
INFO:root:current mean train loss 3213.0280098035496
INFO:root:current train perplexity3.585207939147949
INFO:root:current mean train loss 3237.445421942349
INFO:root:current train perplexity3.5983057022094727
INFO:root:current mean train loss 3234.4598666653774
INFO:root:current train perplexity3.5932424068450928
INFO:root:current mean train loss 3234.472442399775
INFO:root:current train perplexity3.5953266620635986
INFO:root:current mean train loss 3236.8718819893143
INFO:root:current train perplexity3.5946125984191895
INFO:root:current mean train loss 3239.4772264977196
INFO:root:current train perplexity3.5924274921417236
INFO:root:current mean train loss 3238.7033295502533
INFO:root:current train perplexity3.5891404151916504
INFO:root:current mean train loss 3239.0278329433568
INFO:root:current train perplexity3.589768886566162
INFO:root:current mean train loss 3243.3192391464218
INFO:root:current train perplexity3.591555595397949

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.73s/it]
INFO:root:final mean train loss: 3240.8438401376047
INFO:root:final train perplexity: 3.5916318893432617
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.18s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.18s/it]
INFO:root:eval mean loss: 4071.4273430574026
INFO:root:eval perplexity: 5.1880903244018555
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/164
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/200 [6:59:58<1:18:19, 130.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3254.879949396307
INFO:root:current train perplexity3.5740487575531006
INFO:root:current mean train loss 3239.782734638936
INFO:root:current train perplexity3.5724172592163086
INFO:root:current mean train loss 3242.1626659230596
INFO:root:current train perplexity3.579289674758911
INFO:root:current mean train loss 3242.3319299826667
INFO:root:current train perplexity3.5871193408966064
INFO:root:current mean train loss 3246.782903740876
INFO:root:current train perplexity3.593083620071411
INFO:root:current mean train loss 3243.4628997026357
INFO:root:current train perplexity3.5934300422668457
INFO:root:current mean train loss 3240.5857017504604
INFO:root:current train perplexity3.589726209640503
INFO:root:current mean train loss 3238.9595694883965
INFO:root:current train perplexity3.587902307510376
INFO:root:current mean train loss 3240.4356143071823
INFO:root:current train perplexity3.5879104137420654
INFO:root:current mean train loss 3240.3099497890366
INFO:root:current train perplexity3.587503433227539

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.20s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.21s/it]
INFO:root:final mean train loss: 3237.30312888853
INFO:root:final train perplexity: 3.586618185043335
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.20s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.20s/it]
INFO:root:eval mean loss: 4072.041774019282
INFO:root:eval perplexity: 5.189380645751953
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/165
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 165/200 [7:02:07<1:15:54, 130.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3247.4884225945725
INFO:root:current train perplexity3.5890111923217773
INFO:root:current mean train loss 3234.4610380284926
INFO:root:current train perplexity3.5801384449005127
INFO:root:current mean train loss 3241.7064103078624
INFO:root:current train perplexity3.5917980670928955
INFO:root:current mean train loss 3241.112421017829
INFO:root:current train perplexity3.5866105556488037
INFO:root:current mean train loss 3238.611381731056
INFO:root:current train perplexity3.5869555473327637
INFO:root:current mean train loss 3236.791335500963
INFO:root:current train perplexity3.5867156982421875
INFO:root:current mean train loss 3239.0629819706683
INFO:root:current train perplexity3.586956024169922
INFO:root:current mean train loss 3241.864250342272
INFO:root:current train perplexity3.5877938270568848
INFO:root:current mean train loss 3242.0953266297506
INFO:root:current train perplexity3.5895352363586426
INFO:root:current mean train loss 3240.742608038204
INFO:root:current train perplexity3.58719801902771

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.22s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.22s/it]
INFO:root:final mean train loss: 3237.2555451546946
INFO:root:final train perplexity: 3.586550712585449
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.33s/it]
INFO:root:eval mean loss: 4072.880826476618
INFO:root:eval perplexity: 5.191141128540039
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/166
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/200 [7:04:18<1:13:46, 130.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3231.465910734954
INFO:root:current train perplexity3.590085744857788
INFO:root:current mean train loss 3232.3974051888536
INFO:root:current train perplexity3.571847438812256
INFO:root:current mean train loss 3237.7997229487887
INFO:root:current train perplexity3.581501007080078
INFO:root:current mean train loss 3233.2114213016057
INFO:root:current train perplexity3.5787930488586426
INFO:root:current mean train loss 3234.1706886023494
INFO:root:current train perplexity3.575793743133545
INFO:root:current mean train loss 3237.823802738081
INFO:root:current train perplexity3.5763659477233887
INFO:root:current mean train loss 3238.7716814007676
INFO:root:current train perplexity3.5796890258789062
INFO:root:current mean train loss 3239.494423720663
INFO:root:current train perplexity3.58109712600708
INFO:root:current mean train loss 3236.6835609814275
INFO:root:current train perplexity3.5816359519958496
INFO:root:current mean train loss 3237.3349008899677
INFO:root:current train perplexity3.582916259765625

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.31s/it]
INFO:root:final mean train loss: 3234.857876685358
INFO:root:final train perplexity: 3.5831596851348877
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.83s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.84s/it]
INFO:root:eval mean loss: 4072.8435404892507
INFO:root:eval perplexity: 5.191063404083252
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/167
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 167/200 [7:06:27<1:11:24, 129.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3263.9026506696428
INFO:root:current train perplexity3.5755133628845215
INFO:root:current mean train loss 3235.2058051215276
INFO:root:current train perplexity3.580090284347534
INFO:root:current mean train loss 3223.7730385638297
INFO:root:current train perplexity3.571850538253784
INFO:root:current mean train loss 3230.1920577775186
INFO:root:current train perplexity3.5734007358551025
INFO:root:current mean train loss 3232.4110368399784
INFO:root:current train perplexity3.575423002243042
INFO:root:current mean train loss 3236.5418758214078
INFO:root:current train perplexity3.577483892440796
INFO:root:current mean train loss 3235.6971802718995
INFO:root:current train perplexity3.5791831016540527
INFO:root:current mean train loss 3236.4697165975767
INFO:root:current train perplexity3.577974557876587
INFO:root:current mean train loss 3236.9289255473427
INFO:root:current train perplexity3.5788826942443848
INFO:root:current mean train loss 3236.3969379282253
INFO:root:current train perplexity3.5805299282073975

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.20s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.21s/it]
INFO:root:final mean train loss: 3233.693100283223
INFO:root:final train perplexity: 3.5815134048461914
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.70s/it]
INFO:root:eval mean loss: 4072.0270753684617
INFO:root:eval perplexity: 5.1893486976623535
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/168
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 168/200 [7:08:36<1:09:13, 129.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3236.8756870003635
INFO:root:current train perplexity3.5526654720306396
INFO:root:current mean train loss 3246.952203070367
INFO:root:current train perplexity3.582038164138794
INFO:root:current mean train loss 3231.6600487477494
INFO:root:current train perplexity3.5776898860931396
INFO:root:current mean train loss 3227.0725553195607
INFO:root:current train perplexity3.571915626525879
INFO:root:current mean train loss 3228.775264421381
INFO:root:current train perplexity3.573448896408081
INFO:root:current mean train loss 3227.4076055874484
INFO:root:current train perplexity3.569948434829712
INFO:root:current mean train loss 3231.7994770150904
INFO:root:current train perplexity3.5707786083221436
INFO:root:current mean train loss 3233.2256371971735
INFO:root:current train perplexity3.5753424167633057
INFO:root:current mean train loss 3234.988616617549
INFO:root:current train perplexity3.5769965648651123
INFO:root:current mean train loss 3233.3673912525683
INFO:root:current train perplexity3.5767011642456055

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.37s/it]
INFO:root:final mean train loss: 3229.9918299644223
INFO:root:final train perplexity: 3.5762875080108643
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.87s/it]
INFO:root:eval mean loss: 4074.523479055851
INFO:root:eval perplexity: 5.194590091705322
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/169
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 169/200 [7:10:46<1:07:05, 129.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3235.0189520143995
INFO:root:current train perplexity3.557591438293457
INFO:root:current mean train loss 3223.4875132579677
INFO:root:current train perplexity3.5603575706481934
INFO:root:current mean train loss 3226.6914655829805
INFO:root:current train perplexity3.5724117755889893
INFO:root:current mean train loss 3226.5333408453525
INFO:root:current train perplexity3.571209669113159
INFO:root:current mean train loss 3221.6003315115713
INFO:root:current train perplexity3.5670669078826904
INFO:root:current mean train loss 3222.1531727647175
INFO:root:current train perplexity3.5685009956359863
INFO:root:current mean train loss 3225.732122230823
INFO:root:current train perplexity3.5701839923858643
INFO:root:current mean train loss 3228.7002076658205
INFO:root:current train perplexity3.5733320713043213
INFO:root:current mean train loss 3230.1189254599367
INFO:root:current train perplexity3.572695255279541
INFO:root:current mean train loss 3231.1577990478772
INFO:root:current train perplexity3.5742604732513428

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.39s/it]
INFO:root:final mean train loss: 3229.7991185957385
INFO:root:final train perplexity: 3.576014995574951
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.86s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.86s/it]
INFO:root:eval mean loss: 4075.993357989805
INFO:root:eval perplexity: 5.197678565979004
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/170
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/200 [7:12:58<1:05:16, 130.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3217.4431773040255
INFO:root:current train perplexity3.566180467605591
INFO:root:current mean train loss 3211.2953425953224
INFO:root:current train perplexity3.5516936779022217
INFO:root:current mean train loss 3210.987087883084
INFO:root:current train perplexity3.5600595474243164
INFO:root:current mean train loss 3220.571720218924
INFO:root:current train perplexity3.5697834491729736
INFO:root:current mean train loss 3220.9250924436615
INFO:root:current train perplexity3.568977117538452
INFO:root:current mean train loss 3223.5148772920393
INFO:root:current train perplexity3.5720698833465576
INFO:root:current mean train loss 3227.327446666943
INFO:root:current train perplexity3.5740575790405273
INFO:root:current mean train loss 3229.721010632823
INFO:root:current train perplexity3.5750348567962646
INFO:root:current mean train loss 3230.1842548339278
INFO:root:current train perplexity3.575151205062866
INFO:root:current mean train loss 3230.3056276577977
INFO:root:current train perplexity3.5744781494140625

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.55s/it]
INFO:root:final mean train loss: 3228.7078396581833
INFO:root:final train perplexity: 3.5744760036468506
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.26s/it]
INFO:root:eval mean loss: 4074.4813587378103
INFO:root:eval perplexity: 5.1945013999938965
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/171
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/200 [7:15:09<1:03:06, 130.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3254.554097189832
INFO:root:current train perplexity3.5629334449768066
INFO:root:current mean train loss 3242.566847749813
INFO:root:current train perplexity3.5776994228363037
INFO:root:current mean train loss 3234.5969018829
INFO:root:current train perplexity3.581848621368408
INFO:root:current mean train loss 3234.8865338151395
INFO:root:current train perplexity3.58341383934021
INFO:root:current mean train loss 3225.3616417960384
INFO:root:current train perplexity3.5755367279052734
INFO:root:current mean train loss 3223.5111460744597
INFO:root:current train perplexity3.5710561275482178
INFO:root:current mean train loss 3223.506290921922
INFO:root:current train perplexity3.570861577987671
INFO:root:current mean train loss 3226.3027168681756
INFO:root:current train perplexity3.5719645023345947
INFO:root:current mean train loss 3229.29790168595
INFO:root:current train perplexity3.57261323928833
INFO:root:current mean train loss 3229.6359807737363
INFO:root:current train perplexity3.5728390216827393

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.58s/it]
INFO:root:final mean train loss: 3227.355131456929
INFO:root:final train perplexity: 3.5725693702697754
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.74s/it]
INFO:root:eval mean loss: 4075.8293664810503
INFO:root:eval perplexity: 5.197333812713623
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/172
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 172/200 [7:17:22<1:01:18, 131.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3218.7949739583332
INFO:root:current train perplexity3.577446460723877
INFO:root:current mean train loss 3219.9854617745536
INFO:root:current train perplexity3.56718373298645
INFO:root:current mean train loss 3230.0294841974433
INFO:root:current train perplexity3.571150541305542
INFO:root:current mean train loss 3225.787318359375
INFO:root:current train perplexity3.5675551891326904
INFO:root:current mean train loss 3225.711664268092
INFO:root:current train perplexity3.567540168762207
INFO:root:current mean train loss 3226.848783542799
INFO:root:current train perplexity3.5672802925109863
INFO:root:current mean train loss 3224.4782443576387
INFO:root:current train perplexity3.567450523376465
INFO:root:current mean train loss 3226.2144140625
INFO:root:current train perplexity3.5684597492218018
INFO:root:current mean train loss 3228.200239676339
INFO:root:current train perplexity3.5691068172454834
INFO:root:current mean train loss 3227.3696484375
INFO:root:current train perplexity3.5683560371398926

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.26s/it]
INFO:root:final mean train loss: 3224.475454576554
INFO:root:final train perplexity: 3.568512439727783
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.11s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.11s/it]
INFO:root:eval mean loss: 4076.29791562777
INFO:root:eval perplexity: 5.1983184814453125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/173
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 173/200 [7:19:36<59:22, 131.95s/it]  
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3226.2473350432983
INFO:root:current train perplexity3.5854828357696533
INFO:root:current mean train loss 3229.986644307121
INFO:root:current train perplexity3.567448377609253
INFO:root:current mean train loss 3227.1920015045275
INFO:root:current train perplexity3.5692179203033447
INFO:root:current mean train loss 3227.6319812744778
INFO:root:current train perplexity3.5643579959869385
INFO:root:current mean train loss 3234.7269228980654
INFO:root:current train perplexity3.5688822269439697
INFO:root:current mean train loss 3228.8797555076117
INFO:root:current train perplexity3.5676536560058594
INFO:root:current mean train loss 3227.180955386965
INFO:root:current train perplexity3.566828727722168
INFO:root:current mean train loss 3228.201281629151
INFO:root:current train perplexity3.568098306655884
INFO:root:current mean train loss 3229.562402399048
INFO:root:current train perplexity3.5698680877685547
INFO:root:current mean train loss 3227.3655848149
INFO:root:current train perplexity3.5689523220062256

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.26s/it]
INFO:root:final mean train loss: 3224.9569404971216
INFO:root:final train perplexity: 3.569190740585327
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.21s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.21s/it]
INFO:root:eval mean loss: 4077.4582761940383
INFO:root:eval perplexity: 5.200758457183838
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/174
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 174/200 [7:21:46<56:57, 131.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3207.2422760345125
INFO:root:current train perplexity3.55814528465271
INFO:root:current mean train loss 3207.012750276096
INFO:root:current train perplexity3.550666332244873
INFO:root:current mean train loss 3217.042712024807
INFO:root:current train perplexity3.5583667755126953
INFO:root:current mean train loss 3222.236861987492
INFO:root:current train perplexity3.5621793270111084
INFO:root:current mean train loss 3218.515022852756
INFO:root:current train perplexity3.55735182762146
INFO:root:current mean train loss 3219.6195161306314
INFO:root:current train perplexity3.5569887161254883
INFO:root:current mean train loss 3222.3963713142184
INFO:root:current train perplexity3.5602128505706787
INFO:root:current mean train loss 3222.6399172699707
INFO:root:current train perplexity3.561068534851074
INFO:root:current mean train loss 3225.351089563166
INFO:root:current train perplexity3.56449294090271
INFO:root:current mean train loss 3225.913148019677
INFO:root:current train perplexity3.567000150680542

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.58s/it]
INFO:root:final mean train loss: 3223.380822458575
INFO:root:final train perplexity: 3.566972017288208
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.76s/it]
INFO:root:eval mean loss: 4078.0969792359265
INFO:root:eval perplexity: 5.202101707458496
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/175
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/200 [7:23:55<54:29, 130.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3216.8166775173613
INFO:root:current train perplexity3.5652554035186768
INFO:root:current mean train loss 3217.872224894001
INFO:root:current train perplexity3.557840347290039
INFO:root:current mean train loss 3219.6974575081
INFO:root:current train perplexity3.5599594116210938
INFO:root:current mean train loss 3220.4429279987075
INFO:root:current train perplexity3.5603160858154297
INFO:root:current mean train loss 3221.630364733373
INFO:root:current train perplexity3.5616698265075684
INFO:root:current mean train loss 3223.0944053891903
INFO:root:current train perplexity3.5651814937591553
INFO:root:current mean train loss 3223.7087433778165
INFO:root:current train perplexity3.5631275177001953
INFO:root:current mean train loss 3221.9543020083697
INFO:root:current train perplexity3.5620298385620117
INFO:root:current mean train loss 3222.4125362816326
INFO:root:current train perplexity3.5629029273986816

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.30s/it]
INFO:root:final mean train loss: 3221.899278886857
INFO:root:final train perplexity: 3.564887523651123
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.97s/it]
INFO:root:eval mean loss: 4078.012890971299
INFO:root:eval perplexity: 5.2019243240356445
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/176
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 176/200 [7:26:05<52:14, 130.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3179.917724609375
INFO:root:current train perplexity3.438572883605957
INFO:root:current mean train loss 3228.515319253797
INFO:root:current train perplexity3.5515530109405518
INFO:root:current mean train loss 3222.0971514568237
INFO:root:current train perplexity3.549886703491211
INFO:root:current mean train loss 3219.316517584487
INFO:root:current train perplexity3.5523064136505127
INFO:root:current mean train loss 3220.714090333231
INFO:root:current train perplexity3.550990581512451
INFO:root:current mean train loss 3218.7078883906556
INFO:root:current train perplexity3.5524935722351074
INFO:root:current mean train loss 3217.703254511172
INFO:root:current train perplexity3.5545129776000977
INFO:root:current mean train loss 3219.9038224065152
INFO:root:current train perplexity3.5594799518585205
INFO:root:current mean train loss 3221.1026969824584
INFO:root:current train perplexity3.5608177185058594
INFO:root:current mean train loss 3224.278144542017
INFO:root:current train perplexity3.565288543701172

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.84s/it]
INFO:root:final mean train loss: 3221.019089975665
INFO:root:final train perplexity: 3.5636491775512695
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.05s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.05s/it]
INFO:root:eval mean loss: 4078.864450008311
INFO:root:eval perplexity: 5.203716278076172
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/177
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 177/200 [7:28:15<49:58, 130.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3224.1251627604165
INFO:root:current train perplexity3.5229289531707764
INFO:root:current mean train loss 3213.8809442934785
INFO:root:current train perplexity3.535916805267334
INFO:root:current mean train loss 3216.200331577035
INFO:root:current train perplexity3.5586025714874268
INFO:root:current mean train loss 3213.9838998945934
INFO:root:current train perplexity3.5596234798431396
INFO:root:current mean train loss 3217.0087672957457
INFO:root:current train perplexity3.5568926334381104
INFO:root:current mean train loss 3216.875940059921
INFO:root:current train perplexity3.555593490600586
INFO:root:current mean train loss 3220.6579617632115
INFO:root:current train perplexity3.5579617023468018
INFO:root:current mean train loss 3222.9034807965472
INFO:root:current train perplexity3.5601258277893066
INFO:root:current mean train loss 3222.9091701016105
INFO:root:current train perplexity3.5601089000701904
INFO:root:current mean train loss 3221.1594956028007
INFO:root:current train perplexity3.5607268810272217

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.60s/it]
INFO:root:final mean train loss: 3219.3377580950337
INFO:root:final train perplexity: 3.561286449432373
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.93s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.93s/it]
INFO:root:eval mean loss: 4078.484449454233
INFO:root:eval perplexity: 5.202916622161865
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/178
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/200 [7:30:27<48:01, 130.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3234.2096424932065
INFO:root:current train perplexity3.5525622367858887
INFO:root:current mean train loss 3228.3025835238823
INFO:root:current train perplexity3.5469417572021484
INFO:root:current mean train loss 3222.6931447940024
INFO:root:current train perplexity3.5434019565582275
INFO:root:current mean train loss 3225.48146118542
INFO:root:current train perplexity3.5500733852386475
INFO:root:current mean train loss 3229.2191990340575
INFO:root:current train perplexity3.555760622024536
INFO:root:current mean train loss 3224.7161750866394
INFO:root:current train perplexity3.557481050491333
INFO:root:current mean train loss 3223.561256959771
INFO:root:current train perplexity3.5572221279144287
INFO:root:current mean train loss 3220.7543756213263
INFO:root:current train perplexity3.55839467048645
INFO:root:current mean train loss 3219.364364308836
INFO:root:current train perplexity3.557192802429199
INFO:root:current mean train loss 3219.7570284991198
INFO:root:current train perplexity3.560025691986084

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.81s/it]
INFO:root:final mean train loss: 3218.58720121076
INFO:root:final train perplexity: 3.560232400894165
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.97s/it]
INFO:root:eval mean loss: 4078.6703963042996
INFO:root:eval perplexity: 5.203307628631592
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/179
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/200 [7:32:37<45:42, 130.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3197.6820422757055
INFO:root:current train perplexity3.550328016281128
INFO:root:current mean train loss 3209.889491889313
INFO:root:current train perplexity3.5514447689056396
INFO:root:current mean train loss 3210.035305270901
INFO:root:current train perplexity3.559951066970825
INFO:root:current mean train loss 3210.323655235083
INFO:root:current train perplexity3.55886173248291
INFO:root:current mean train loss 3213.2204935379204
INFO:root:current train perplexity3.5580101013183594
INFO:root:current mean train loss 3215.0487554805204
INFO:root:current train perplexity3.5551512241363525
INFO:root:current mean train loss 3216.7707318337707
INFO:root:current train perplexity3.557732105255127
INFO:root:current mean train loss 3218.8667583335114
INFO:root:current train perplexity3.559133291244507
INFO:root:current mean train loss 3219.4409320707355
INFO:root:current train perplexity3.5579209327697754
INFO:root:current mean train loss 3217.7394190869195
INFO:root:current train perplexity3.556077718734741

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.94s/it]
INFO:root:final mean train loss: 3217.1195043133152
INFO:root:final train perplexity: 3.558170795440674
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.91s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.91s/it]
INFO:root:eval mean loss: 4078.816482435727
INFO:root:eval perplexity: 5.203615188598633
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/180
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/200 [7:34:48<43:32, 130.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3225.053704677484
INFO:root:current train perplexity3.5747151374816895
INFO:root:current mean train loss 3210.339861314074
INFO:root:current train perplexity3.5550973415374756
INFO:root:current mean train loss 3210.753548721888
INFO:root:current train perplexity3.5546798706054688
INFO:root:current mean train loss 3215.6121023172473
INFO:root:current train perplexity3.552760362625122
INFO:root:current mean train loss 3215.6602930577305
INFO:root:current train perplexity3.555492401123047
INFO:root:current mean train loss 3215.4741247173583
INFO:root:current train perplexity3.5543417930603027
INFO:root:current mean train loss 3214.2572095987384
INFO:root:current train perplexity3.552534341812134
INFO:root:current mean train loss 3217.039591416293
INFO:root:current train perplexity3.55448579788208
INFO:root:current mean train loss 3218.949739331142
INFO:root:current train perplexity3.557654619216919
INFO:root:current mean train loss 3218.9619015824683
INFO:root:current train perplexity3.556682586669922

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.48s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.48s/it]
INFO:root:final mean train loss: 3216.500715871011
INFO:root:final train perplexity: 3.557302474975586
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.05s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.05s/it]
INFO:root:eval mean loss: 4080.6256458471853
INFO:root:eval perplexity: 5.207424163818359
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/181
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 181/200 [7:36:58<41:20, 130.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3220.806204288564
INFO:root:current train perplexity3.532471179962158
INFO:root:current mean train loss 3213.963682836416
INFO:root:current train perplexity3.557307243347168
INFO:root:current mean train loss 3216.8014533780365
INFO:root:current train perplexity3.5541460514068604
INFO:root:current mean train loss 3224.3745862977307
INFO:root:current train perplexity3.5606601238250732
INFO:root:current mean train loss 3219.6401591119616
INFO:root:current train perplexity3.555209159851074
INFO:root:current mean train loss 3216.8595718300103
INFO:root:current train perplexity3.5547049045562744
INFO:root:current mean train loss 3219.3027411671655
INFO:root:current train perplexity3.5551512241363525
INFO:root:current mean train loss 3217.6719348095507
INFO:root:current train perplexity3.554847240447998
INFO:root:current mean train loss 3217.510085861404
INFO:root:current train perplexity3.5546960830688477
INFO:root:current mean train loss 3217.4652423153707
INFO:root:current train perplexity3.554877281188965

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.03s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.03s/it]
INFO:root:final mean train loss: 3214.222951889038
INFO:root:final train perplexity: 3.5541069507598877
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.73s/it]
INFO:root:eval mean loss: 4080.4824565048757
INFO:root:eval perplexity: 5.207121849060059
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/182
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/200 [7:39:09<39:10, 130.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3211.8286354758525
INFO:root:current train perplexity3.5724079608917236
INFO:root:current mean train loss 3205.366395224294
INFO:root:current train perplexity3.5611777305603027
INFO:root:current mean train loss 3204.6093548943013
INFO:root:current train perplexity3.5471746921539307
INFO:root:current mean train loss 3210.673035871479
INFO:root:current train perplexity3.5484955310821533
INFO:root:current mean train loss 3212.88818359375
INFO:root:current train perplexity3.547011137008667
INFO:root:current mean train loss 3211.1369290188627
INFO:root:current train perplexity3.550386905670166
INFO:root:current mean train loss 3212.7030690899333
INFO:root:current train perplexity3.5490291118621826
INFO:root:current mean train loss 3214.0825108003933
INFO:root:current train perplexity3.552046060562134
INFO:root:current mean train loss 3215.7339812340097
INFO:root:current train perplexity3.5524604320526123
INFO:root:current mean train loss 3216.568969087451
INFO:root:current train perplexity3.552788734436035

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.37s/it]
INFO:root:final mean train loss: 3213.0664235391923
INFO:root:final train perplexity: 3.552485942840576
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.13s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.13s/it]
INFO:root:eval mean loss: 4080.5390902039007
INFO:root:eval perplexity: 5.207241058349609
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/183
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 183/200 [7:41:18<36:53, 130.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3203.4447467137898
INFO:root:current train perplexity3.5567619800567627
INFO:root:current mean train loss 3221.7205773101996
INFO:root:current train perplexity3.5529720783233643
INFO:root:current mean train loss 3215.2970114588284
INFO:root:current train perplexity3.5528481006622314
INFO:root:current mean train loss 3216.3529022469006
INFO:root:current train perplexity3.5562989711761475
INFO:root:current mean train loss 3215.86018651711
INFO:root:current train perplexity3.555140972137451
INFO:root:current mean train loss 3214.3192925732683
INFO:root:current train perplexity3.5540599822998047
INFO:root:current mean train loss 3216.0102874157474
INFO:root:current train perplexity3.5539138317108154
INFO:root:current mean train loss 3217.1223880472844
INFO:root:current train perplexity3.5524680614471436
INFO:root:current mean train loss 3214.179597255667
INFO:root:current train perplexity3.551405429840088
INFO:root:current mean train loss 3214.4760881623993
INFO:root:current train perplexity3.552503824234009

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.38s/it]
INFO:root:final mean train loss: 3213.274865427325
INFO:root:final train perplexity: 3.552778482437134
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.13s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.13s/it]
INFO:root:eval mean loss: 4080.6135288536125
INFO:root:eval perplexity: 5.207397937774658
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/184
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 184/200 [7:43:28<34:43, 130.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3215.1005343584948
INFO:root:current train perplexity3.5475282669067383
INFO:root:current mean train loss 3218.828374851517
INFO:root:current train perplexity3.543910026550293
INFO:root:current mean train loss 3215.564467539207
INFO:root:current train perplexity3.542012929916382
INFO:root:current mean train loss 3212.9450907334485
INFO:root:current train perplexity3.5418689250946045
INFO:root:current mean train loss 3212.559739811405
INFO:root:current train perplexity3.5425338745117188
INFO:root:current mean train loss 3212.7829376060367
INFO:root:current train perplexity3.5439767837524414
INFO:root:current mean train loss 3212.8893595438244
INFO:root:current train perplexity3.5463900566101074
INFO:root:current mean train loss 3211.218984007681
INFO:root:current train perplexity3.5465333461761475
INFO:root:current mean train loss 3214.7358771235467
INFO:root:current train perplexity3.547882556915283
INFO:root:current mean train loss 3213.4432670993983
INFO:root:current train perplexity3.5486671924591064

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.76s/it]
INFO:root:final mean train loss: 3210.0327857232865
INFO:root:final train perplexity: 3.548236608505249
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.19s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.19s/it]
INFO:root:eval mean loss: 4081.1611189605496
INFO:root:eval perplexity: 5.208551406860352
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/185
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 185/200 [7:45:38<32:31, 130.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3201.0750593354433
INFO:root:current train perplexity3.531528949737549
INFO:root:current mean train loss 3197.6102315380585
INFO:root:current train perplexity3.5417182445526123
INFO:root:current mean train loss 3202.1684876582103
INFO:root:current train perplexity3.551208019256592
INFO:root:current mean train loss 3206.523947038877
INFO:root:current train perplexity3.5476107597351074
INFO:root:current mean train loss 3209.5350079307477
INFO:root:current train perplexity3.5483107566833496
INFO:root:current mean train loss 3211.7405447161054
INFO:root:current train perplexity3.5473179817199707
INFO:root:current mean train loss 3213.615757173923
INFO:root:current train perplexity3.5494232177734375
INFO:root:current mean train loss 3213.8137272845797
INFO:root:current train perplexity3.5505104064941406
INFO:root:current mean train loss 3214.953022510932
INFO:root:current train perplexity3.5505049228668213
INFO:root:current mean train loss 3214.1953691087047
INFO:root:current train perplexity3.5499510765075684

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.77s/it]
INFO:root:final mean train loss: 3211.2453652043496
INFO:root:final train perplexity: 3.5499348640441895
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.26s/it]
INFO:root:eval mean loss: 4081.0610334247563
INFO:root:eval perplexity: 5.208339691162109
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/186
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 186/200 [7:47:52<30:37, 131.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3214.8733639771913
INFO:root:current train perplexity3.560656785964966
INFO:root:current mean train loss 3212.4178421102106
INFO:root:current train perplexity3.5547714233398438
INFO:root:current mean train loss 3214.8951471308796
INFO:root:current train perplexity3.5476183891296387
INFO:root:current mean train loss 3215.596031800105
INFO:root:current train perplexity3.546292781829834
INFO:root:current mean train loss 3215.6955571419403
INFO:root:current train perplexity3.545100688934326
INFO:root:current mean train loss 3213.991422221039
INFO:root:current train perplexity3.54654598236084
INFO:root:current mean train loss 3215.5890175098934
INFO:root:current train perplexity3.5479536056518555
INFO:root:current mean train loss 3214.0043917392986
INFO:root:current train perplexity3.5481584072113037
INFO:root:current mean train loss 3212.4933366399205
INFO:root:current train perplexity3.547466516494751
INFO:root:current mean train loss 3212.2925356291953
INFO:root:current train perplexity3.547893524169922

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.75s/it]
INFO:root:final mean train loss: 3209.347859597975
INFO:root:final train perplexity: 3.54727840423584
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.01s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.01s/it]
INFO:root:eval mean loss: 4083.070876966977
INFO:root:eval perplexity: 5.212575435638428
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/187
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 187/200 [7:50:06<28:35, 131.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3216.3393863075657
INFO:root:current train perplexity3.5365259647369385
INFO:root:current mean train loss 3213.9389685997594
INFO:root:current train perplexity3.5470187664031982
INFO:root:current mean train loss 3211.979474807998
INFO:root:current train perplexity3.5466198921203613
INFO:root:current mean train loss 3210.2124332476264
INFO:root:current train perplexity3.546187400817871
INFO:root:current mean train loss 3207.6738468671088
INFO:root:current train perplexity3.5432701110839844
INFO:root:current mean train loss 3211.0354705554096
INFO:root:current train perplexity3.547693967819214
INFO:root:current mean train loss 3214.4109933537543
INFO:root:current train perplexity3.550075054168701
INFO:root:current mean train loss 3213.0308710446147
INFO:root:current train perplexity3.5481441020965576
INFO:root:current mean train loss 3212.2075031642808
INFO:root:current train perplexity3.5483672618865967

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:03<00:00, 123.19s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:03<00:00, 123.19s/it]
INFO:root:final mean train loss: 3209.2099928086805
INFO:root:final train perplexity: 3.5470855236053467
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.94s/it]
INFO:root:eval mean loss: 4081.9867817763743
INFO:root:eval perplexity: 5.210290431976318
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/188
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/200 [7:52:20<26:30, 132.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3170.6854654947915
INFO:root:current train perplexity3.4781551361083984
INFO:root:current mean train loss 3208.2314405719053
INFO:root:current train perplexity3.527815580368042
INFO:root:current mean train loss 3207.616364878387
INFO:root:current train perplexity3.5385377407073975
INFO:root:current mean train loss 3211.9720978689666
INFO:root:current train perplexity3.5461478233337402
INFO:root:current mean train loss 3208.763719733832
INFO:root:current train perplexity3.54551362991333
INFO:root:current mean train loss 3208.7359412858787
INFO:root:current train perplexity3.546877145767212
INFO:root:current mean train loss 3204.9223450618006
INFO:root:current train perplexity3.5452325344085693
INFO:root:current mean train loss 3206.400707347973
INFO:root:current train perplexity3.541940689086914
INFO:root:current mean train loss 3206.3908904231203
INFO:root:current train perplexity3.5418739318847656
INFO:root:current mean train loss 3210.7613037379742
INFO:root:current train perplexity3.543929100036621

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.45s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.45s/it]
INFO:root:final mean train loss: 3207.794740738407
INFO:root:final train perplexity: 3.54510498046875
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.28s/it]
INFO:root:eval mean loss: 4082.382062763187
INFO:root:eval perplexity: 5.211123466491699
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/189
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 189/200 [7:54:29<24:08, 131.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3196.004216974432
INFO:root:current train perplexity3.5592079162597656
INFO:root:current mean train loss 3192.931011577984
INFO:root:current train perplexity3.5327694416046143
INFO:root:current mean train loss 3208.4066121612113
INFO:root:current train perplexity3.540792465209961
INFO:root:current mean train loss 3210.064922565816
INFO:root:current train perplexity3.5430233478546143
INFO:root:current mean train loss 3205.4691818497186
INFO:root:current train perplexity3.543353796005249
INFO:root:current mean train loss 3204.7106895372126
INFO:root:current train perplexity3.5429306030273438
INFO:root:current mean train loss 3202.7452182800994
INFO:root:current train perplexity3.540231943130493
INFO:root:current mean train loss 3204.6379040853553
INFO:root:current train perplexity3.5419132709503174
INFO:root:current mean train loss 3204.6893790700137
INFO:root:current train perplexity3.5405404567718506
INFO:root:current mean train loss 3208.0393835543014
INFO:root:current train perplexity3.5426924228668213

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.67s/it]
INFO:root:final mean train loss: 3207.0959860278713
INFO:root:final train perplexity: 3.544128179550171
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.03s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.03s/it]
INFO:root:eval mean loss: 4082.1449329565603
INFO:root:eval perplexity: 5.2106242179870605
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/190
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/200 [7:56:42<21:59, 131.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3190.6478078741775
INFO:root:current train perplexity3.5501036643981934
INFO:root:current mean train loss 3226.8473854385506
INFO:root:current train perplexity3.5524744987487793
INFO:root:current mean train loss 3221.6074921072345
INFO:root:current train perplexity3.5471878051757812
INFO:root:current mean train loss 3221.037957361873
INFO:root:current train perplexity3.5480332374572754
INFO:root:current mean train loss 3213.9332161769094
INFO:root:current train perplexity3.5469844341278076
INFO:root:current mean train loss 3214.1062265737896
INFO:root:current train perplexity3.5464377403259277
INFO:root:current mean train loss 3210.4602188825224
INFO:root:current train perplexity3.544635534286499
INFO:root:current mean train loss 3210.2516074571886
INFO:root:current train perplexity3.544766426086426
INFO:root:current mean train loss 3208.005166599893
INFO:root:current train perplexity3.544123888015747
INFO:root:current mean train loss 3208.7550493806957
INFO:root:current train perplexity3.54386305809021

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.09s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.09s/it]
INFO:root:final mean train loss: 3207.4018741115447
INFO:root:final train perplexity: 3.544556140899658
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.07s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.07s/it]
INFO:root:eval mean loss: 4083.109837308843
INFO:root:eval perplexity: 5.2126569747924805
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/191
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/200 [7:58:53<19:44, 131.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3260.671875
INFO:root:current train perplexity3.558074474334717
INFO:root:current mean train loss 3223.599349855438
INFO:root:current train perplexity3.538628101348877
INFO:root:current mean train loss 3217.2144511890833
INFO:root:current train perplexity3.543405771255493
INFO:root:current mean train loss 3216.2535590775515
INFO:root:current train perplexity3.543325424194336
INFO:root:current mean train loss 3212.4779421518224
INFO:root:current train perplexity3.542605400085449
INFO:root:current mean train loss 3216.0429974724266
INFO:root:current train perplexity3.5443849563598633
INFO:root:current mean train loss 3210.9776695044607
INFO:root:current train perplexity3.542217254638672
INFO:root:current mean train loss 3208.9289406378953
INFO:root:current train perplexity3.541285753250122
INFO:root:current mean train loss 3207.173776167624
INFO:root:current train perplexity3.5410971641540527
INFO:root:current mean train loss 3207.469894590244
INFO:root:current train perplexity3.541576385498047

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.22s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.22s/it]
INFO:root:final mean train loss: 3206.1832222477083
INFO:root:final train perplexity: 3.5428524017333984
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.94s/it]
INFO:root:eval mean loss: 4082.8022062693926
INFO:root:eval perplexity: 5.212009429931641
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/192
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/200 [8:01:03<17:29, 131.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3190.2372837611606
INFO:root:current train perplexity3.540679693222046
INFO:root:current mean train loss 3204.3704137731484
INFO:root:current train perplexity3.5366907119750977
INFO:root:current mean train loss 3201.394374376662
INFO:root:current train perplexity3.538806438446045
INFO:root:current mean train loss 3204.863956827192
INFO:root:current train perplexity3.5416624546051025
INFO:root:current mean train loss 3209.6479295752515
INFO:root:current train perplexity3.5424554347991943
INFO:root:current mean train loss 3204.313402635806
INFO:root:current train perplexity3.53781795501709
INFO:root:current mean train loss 3207.0215159018207
INFO:root:current train perplexity3.5416979789733887
INFO:root:current mean train loss 3205.0475516183037
INFO:root:current train perplexity3.540987014770508
INFO:root:current mean train loss 3205.0970802535553
INFO:root:current train perplexity3.5398921966552734
INFO:root:current mean train loss 3206.3620639413434
INFO:root:current train perplexity3.5408341884613037

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:07<00:00, 127.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:07<00:00, 127.26s/it]
INFO:root:final mean train loss: 3205.3543639029226
INFO:root:final train perplexity: 3.541693687438965
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.08s/it]
INFO:root:eval mean loss: 4082.984868475731
INFO:root:eval perplexity: 5.212393760681152
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/193
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193/200 [8:03:21<15:32, 133.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3205.7839639353197
INFO:root:current train perplexity3.5589911937713623
INFO:root:current mean train loss 3206.144894900022
INFO:root:current train perplexity3.552197217941284
INFO:root:current mean train loss 3206.538877636317
INFO:root:current train perplexity3.5433008670806885
INFO:root:current mean train loss 3201.416960157389
INFO:root:current train perplexity3.5422935485839844
INFO:root:current mean train loss 3207.0055876789997
INFO:root:current train perplexity3.5414879322052
INFO:root:current mean train loss 3206.7680668558646
INFO:root:current train perplexity3.5392656326293945
INFO:root:current mean train loss 3206.243288980487
INFO:root:current train perplexity3.5401086807250977
INFO:root:current mean train loss 3205.1095652522504
INFO:root:current train perplexity3.538177490234375
INFO:root:current mean train loss 3208.2590992340415
INFO:root:current train perplexity3.5402486324310303
INFO:root:current mean train loss 3208.1984839980446
INFO:root:current train perplexity3.5402140617370605

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:00<00:00, 120.08s/it]
INFO:root:final mean train loss: 3204.644889523906
INFO:root:final train perplexity: 3.5407023429870605
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.32s/it]
INFO:root:eval mean loss: 4083.3683891566934
INFO:root:eval perplexity: 5.213202476501465
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/194
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/200 [8:05:32<13:15, 132.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3174.3742867264095
INFO:root:current train perplexity3.5054337978363037
INFO:root:current mean train loss 3191.351113022558
INFO:root:current train perplexity3.5234546661376953
INFO:root:current mean train loss 3197.390184379669
INFO:root:current train perplexity3.5351037979125977
INFO:root:current mean train loss 3198.2848981982283
INFO:root:current train perplexity3.5355288982391357
INFO:root:current mean train loss 3201.8034965701218
INFO:root:current train perplexity3.5373194217681885
INFO:root:current mean train loss 3204.2649616996086
INFO:root:current train perplexity3.5402846336364746
INFO:root:current mean train loss 3208.135930824573
INFO:root:current train perplexity3.543398141860962
INFO:root:current mean train loss 3205.8488519213965
INFO:root:current train perplexity3.541612148284912
INFO:root:current mean train loss 3206.7522993973084
INFO:root:current train perplexity3.5420825481414795
INFO:root:current mean train loss 3207.409894652422
INFO:root:current train perplexity3.542280912399292

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 119.00s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.00s/it]
INFO:root:final mean train loss: 3205.013136586835
INFO:root:final train perplexity: 3.541217088699341
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.05s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.05s/it]
INFO:root:eval mean loss: 4082.9151169797206
INFO:root:eval perplexity: 5.212247371673584
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/195
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 195/200 [8:07:42<10:59, 131.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3215.9198060116523
INFO:root:current train perplexity3.5540285110473633
INFO:root:current mean train loss 3212.6824574980346
INFO:root:current train perplexity3.542577028274536
INFO:root:current mean train loss 3214.6845637141046
INFO:root:current train perplexity3.5411016941070557
INFO:root:current mean train loss 3211.228287125696
INFO:root:current train perplexity3.5384910106658936
INFO:root:current mean train loss 3208.5368865102464
INFO:root:current train perplexity3.538363218307495
INFO:root:current mean train loss 3203.7680419485127
INFO:root:current train perplexity3.5372776985168457
INFO:root:current mean train loss 3202.77833661324
INFO:root:current train perplexity3.537754535675049
INFO:root:current mean train loss 3203.632723078269
INFO:root:current train perplexity3.5383551120758057
INFO:root:current mean train loss 3205.676742180679
INFO:root:current train perplexity3.5383803844451904
INFO:root:current mean train loss 3205.477395734896
INFO:root:current train perplexity3.5384552478790283

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.84s/it]
INFO:root:final mean train loss: 3203.1963601266184
INFO:root:final train perplexity: 3.538680076599121
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.26s/it]
INFO:root:eval mean loss: 4082.948368586547
INFO:root:eval perplexity: 5.21231746673584
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/196
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 196/200 [8:09:52<08:45, 131.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3217.0603756121736
INFO:root:current train perplexity3.5462727546691895
INFO:root:current mean train loss 3207.583646671501
INFO:root:current train perplexity3.544032096862793
INFO:root:current mean train loss 3211.9484451808285
INFO:root:current train perplexity3.5438530445098877
INFO:root:current mean train loss 3211.921530409145
INFO:root:current train perplexity3.542694330215454
INFO:root:current mean train loss 3212.4771276306546
INFO:root:current train perplexity3.543398857116699
INFO:root:current mean train loss 3211.448918202987
INFO:root:current train perplexity3.5432486534118652
INFO:root:current mean train loss 3207.958834669579
INFO:root:current train perplexity3.5406978130340576
INFO:root:current mean train loss 3208.0668916664968
INFO:root:current train perplexity3.5417089462280273
INFO:root:current mean train loss 3206.795161791739
INFO:root:current train perplexity3.5407004356384277
INFO:root:current mean train loss 3205.894723633822
INFO:root:current train perplexity3.5398030281066895

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:59<00:00, 119.61s/it]
INFO:root:final mean train loss: 3203.2643718719482
INFO:root:final train perplexity: 3.5387744903564453
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.96s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.96s/it]
INFO:root:eval mean loss: 4083.419284685284
INFO:root:eval perplexity: 5.2133097648620605
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/197
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 197/200 [8:12:03<06:33, 131.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3200.6048274739583
INFO:root:current train perplexity3.5314316749572754
INFO:root:current mean train loss 3204.37638671875
INFO:root:current train perplexity3.533733606338501
INFO:root:current mean train loss 3203.2674085582385
INFO:root:current train perplexity3.5369882583618164
INFO:root:current mean train loss 3200.003671875
INFO:root:current train perplexity3.5352890491485596
INFO:root:current mean train loss 3202.8245852179275
INFO:root:current train perplexity3.533465623855591
INFO:root:current mean train loss 3203.765516304348
INFO:root:current train perplexity3.53275728225708
INFO:root:current mean train loss 3205.5058984375
INFO:root:current train perplexity3.5354652404785156
INFO:root:current mean train loss 3203.4312118825605
INFO:root:current train perplexity3.534788131713867
INFO:root:current mean train loss 3205.195855747768
INFO:root:current train perplexity3.535383462905884
INFO:root:current mean train loss 3204.5944626402243
INFO:root:current train perplexity3.537416458129883

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.99s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.99s/it]
INFO:root:final mean train loss: 3202.3497152020855
INFO:root:final train perplexity: 3.5374977588653564
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.74s/it]
INFO:root:eval mean loss: 4083.6162230579566
INFO:root:eval perplexity: 5.213724613189697
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/198
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 198/200 [8:14:16<04:23, 131.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3200.242131612387
INFO:root:current train perplexity3.5433993339538574
INFO:root:current mean train loss 3211.576537418887
INFO:root:current train perplexity3.5430796146392822
INFO:root:current mean train loss 3213.570769724492
INFO:root:current train perplexity3.5488359928131104
INFO:root:current mean train loss 3208.439160538716
INFO:root:current train perplexity3.5436599254608154
INFO:root:current mean train loss 3201.772689914111
INFO:root:current train perplexity3.541996717453003
INFO:root:current mean train loss 3206.152566952321
INFO:root:current train perplexity3.5458531379699707
INFO:root:current mean train loss 3207.019105523083
INFO:root:current train perplexity3.5423312187194824
INFO:root:current mean train loss 3206.3218571649504
INFO:root:current train perplexity3.5393617153167725
INFO:root:current mean train loss 3206.0760222939375
INFO:root:current train perplexity3.5371499061584473
INFO:root:current mean train loss 3205.87340973304
INFO:root:current train perplexity3.5383219718933105

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:58<00:00, 118.49s/it]
INFO:root:final mean train loss: 3202.7035736576204
INFO:root:final train perplexity: 3.537992000579834
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.00s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.00s/it]
INFO:root:eval mean loss: 4083.620160474845
INFO:root:eval perplexity: 5.213733196258545
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/199
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/200 [8:16:26<02:11, 131.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3198.5327926468062
INFO:root:current train perplexity3.5414204597473145
INFO:root:current mean train loss 3199.5855565894963
INFO:root:current train perplexity3.5305469036102295
INFO:root:current mean train loss 3197.5323700265785
INFO:root:current train perplexity3.5335161685943604
INFO:root:current mean train loss 3201.459222271619
INFO:root:current train perplexity3.5345449447631836
INFO:root:current mean train loss 3204.098562205639
INFO:root:current train perplexity3.5372707843780518
INFO:root:current mean train loss 3203.7001758969172
INFO:root:current train perplexity3.539565086364746
INFO:root:current mean train loss 3202.831523889743
INFO:root:current train perplexity3.538121461868286
INFO:root:current mean train loss 3204.2428051047923
INFO:root:current train perplexity3.537569046020508
INFO:root:current mean train loss 3205.50277048918
INFO:root:current train perplexity3.5387184619903564
INFO:root:current mean train loss 3205.402266639994
INFO:root:current train perplexity3.538212299346924

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:01<00:00, 121.37s/it]
INFO:root:final mean train loss: 3202.8399548684397
INFO:root:final train perplexity: 3.538181781768799
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.98s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.98s/it]
INFO:root:eval mean loss: 4083.646963998781
INFO:root:eval perplexity: 5.213788986206055
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_132/200
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [8:18:38<00:00, 131.45s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [8:18:38<00:00, 149.59s/it]
INFO:root:evaluating final model
INFO:root:start evaluating
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:14<00:00, 14.09s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:14<00:00, 14.09s/it]
INFO:root:eval mean loss: 4083.646963998781
INFO:root:eval perplexity: 5.213788986206055
INFO:root:evalaution complete
INFO:root:save model final: small_val_132/final
Fatal error condition occurred in /opt/vcpkg/buildtrees/aws-c-io/src/9e6648842a-364b708815.clean/source/event_loop.c:72: aws_thread_launch(&cleanup_thread, s_event_loop_destroy_async_thread_fn, el_group, &thread_options) == AWS_OP_SUCCESS
Exiting Application
################################################################################
Stack trace:
################################################################################
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/pyarrow/libarrow.so.900(+0x200af06) [0x148ec5725f06]
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/pyarrow/libarrow.so.900(+0x20028e5) [0x148ec571d8e5]
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/pyarrow/libarrow.so.900(+0x1f27e09) [0x148ec5642e09]
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/pyarrow/libarrow.so.900(+0x200ba3d) [0x148ec5726a3d]
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/pyarrow/libarrow.so.900(+0x1f25948) [0x148ec5640948]
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/pyarrow/libarrow.so.900(+0x200ba3d) [0x148ec5726a3d]
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/pyarrow/libarrow.so.900(+0x1ee0b46) [0x148ec55fbb46]
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/pyarrow/libarrow.so.900(+0x194546a) [0x148ec506046a]
/lib/x86_64-linux-gnu/libc.so.6(+0x49a27) [0x148fc187ca27]
/lib/x86_64-linux-gnu/libc.so.6(on_exit+0) [0x148fc187cbe0]
python(+0x24a989) [0x5560b9be8989]
python(+0x24a9bd) [0x5560b9be89bd]
python(+0x24aa14) [0x5560b9be8a14]
python(+0x108f75) [0x5560b9aa6f75]
python(Py_RunMain+0x313) [0x5560b9beb983]
python(Py_BytesMain+0x39) [0x5560b9bebbc9]
/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf3) [0x148fc185a0b3]
python(+0x1d6e13) [0x5560b9b74e13]
/opt/slurm/data/slurmd/job26151390/slurm_script: line 142: 2162969 Aborted                 singularity exec --nv --overlay /scratch/zw2374/overlay-50G-10M.ext3:ro /scratch/work/public/singularity/cuda11.3.0-cudnn8-devel-ubuntu20.04.sif /bin/bash -c "
source /ext3/env.sh
conda activate rblm
python train_script.py --model_path sentence-transformers/multi-qa-MiniLM-L6-cos-v1 --data_config data_config.json --data_folder fast_processed_data_132_final  --output small_val_132 --batch_size 128 --epochs 200 --save_head  --save_epochs 1 --external_embedding
"
