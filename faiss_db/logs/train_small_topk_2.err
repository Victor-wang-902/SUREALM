INFO:root:Output: small_topk_2
INFO:root:Steps per epochs:248
INFO:root:Total steps:49600
/scratch/zw2374/public/faiss_db/models.py:432: UserWarning: Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.
  warnings.warn("Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.")
Some weights of RetrievalGenerationModel were not initialized from the model checkpoint at sentence-transformers/multi-qa-MiniLM-L6-cos-v1 and are newly initialized: ['encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.0.crossattention.self.value.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.4.crossattention.self.value.bias', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.1.crossattention.self.value.weight', 'cls.predictions.transform.dense.weight', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'cls.predictions.decoder.weight', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.weight', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.output.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/scratch/zw2374/public/faiss_db/models.py:446: UserWarning: Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.
  warnings.warn("Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.")
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:root:started training

  0%|          | 0/200 [00:00<?, ?it/s]

  0%|          | 0/1 [00:00<?, ?it/s][A/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
INFO:root:current mean train loss 97776.96022727272
INFO:root:current train perplexity15095.001953125
INFO:root:current mean train loss 81346.91891096106
INFO:root:current train perplexity3022.02197265625


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:05<00:00, 245.01s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:05<00:00, 245.01s/it]
INFO:root:final mean train loss: 74986.31531943045
INFO:root:final train perplexity: 1629.5653076171875
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.31s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.31s/it]
INFO:root:eval mean loss: 44151.766322544645
INFO:root:eval perplexity: 96.49755096435547
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/1

  0%|          | 1/200 [04:49<16:00:14, 289.52s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 42883.11703431373
INFO:root:current train perplexity69.38507080078125
INFO:root:current mean train loss 39076.95333195364
INFO:root:current train perplexity47.0894889831543


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:57<00:00, 237.92s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:57<00:00, 237.92s/it]
INFO:root:final mean train loss: 36486.408211000504
INFO:root:final train perplexity: 36.5520133972168
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.40s/it]
INFO:root:eval mean loss: 31749.026227678572
INFO:root:eval perplexity: 26.732698440551758
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/2

  1%|          | 2/200 [09:29<15:37:14, 284.01s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 31168.870442708332
INFO:root:current train perplexity21.961467742919922
INFO:root:current mean train loss 29698.584344660194
INFO:root:current train perplexity18.66781234741211
INFO:root:current mean train loss 28793.548770397167
INFO:root:current train perplexity17.07528305053711


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:03<00:00, 243.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:03<00:00, 243.63s/it]
INFO:root:final mean train loss: 28406.98828125
INFO:root:final train perplexity: 16.475008010864258
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.42s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.43s/it]
INFO:root:eval mean loss: 28529.84533110119
INFO:root:eval perplexity: 19.157922744750977
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/3

  2%|â–         | 3/200 [14:14<15:33:37, 284.35s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 26369.568359375
INFO:root:current train perplexity13.40141487121582
INFO:root:current mean train loss 25884.058089717742
INFO:root:current train perplexity12.814699172973633


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:06<00:00, 246.10s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:06<00:00, 246.10s/it]
INFO:root:final mean train loss: 25520.32291141633
INFO:root:final train perplexity: 12.392934799194336
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.43s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.43s/it]
INFO:root:eval mean loss: 27121.44298735119
INFO:root:eval perplexity: 16.559375762939453
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/4

  2%|â–         | 4/200 [19:00<15:31:37, 285.19s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 24855.966796875
INFO:root:current train perplexity11.291229248046875
INFO:root:current mean train loss 24379.074748101637
INFO:root:current train perplexity11.019123077392578
INFO:root:current mean train loss 24095.99481053744
INFO:root:current train perplexity10.760444641113281


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:10<00:00, 250.00s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:10<00:00, 250.00s/it]
INFO:root:final mean train loss: 23983.375826927924
INFO:root:final train perplexity: 10.6497220993042
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.52s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.52s/it]
INFO:root:eval mean loss: 26316.662434895832
INFO:root:eval perplexity: 15.236001968383789
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/5

  2%|â–Ž         | 5/200 [24:02<15:45:36, 290.95s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 23341.045219809323
INFO:root:current train perplexity9.989551544189453
INFO:root:current mean train loss 23109.15505847091
INFO:root:current train perplexity9.754905700683594


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:06<00:00, 246.53s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:06<00:00, 246.53s/it]
INFO:root:final mean train loss: 22963.53601467994
INFO:root:final train perplexity: 9.630593299865723
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.50s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.50s/it]
INFO:root:eval mean loss: 25752.622256324405
INFO:root:eval perplexity: 14.372050285339355
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/6

  3%|â–Ž         | 6/200 [28:50<15:37:32, 289.96s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 22324.305397727272
INFO:root:current train perplexity9.095175743103027
INFO:root:current mean train loss 22398.469189893018
INFO:root:current train perplexity9.100518226623535
INFO:root:current mean train loss 22274.88299763033
INFO:root:current train perplexity8.98727035522461


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:11<00:00, 251.43s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:11<00:00, 251.43s/it]
INFO:root:final mean train loss: 22226.973294165826
INFO:root:final train perplexity: 8.9557523727417
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.78s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.78s/it]
INFO:root:eval mean loss: 25322.454496837796
INFO:root:eval perplexity: 13.746234893798828
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/7

  4%|â–Ž         | 7/200 [33:44<15:37:04, 291.32s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 21861.150421626986
INFO:root:current train perplexity8.624757766723633
INFO:root:current mean train loss 21776.357721434048
INFO:root:current train perplexity8.537494659423828


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:10<00:00, 250.23s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:10<00:00, 250.23s/it]
INFO:root:final mean train loss: 21654.375484343498
INFO:root:final train perplexity: 8.463979721069336
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.18s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.18s/it]
INFO:root:eval mean loss: 25015.28641183036
INFO:root:eval perplexity: 13.316105842590332
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/8

  4%|â–         | 8/200 [38:35<15:32:18, 291.35s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 21429.873958333334
INFO:root:current train perplexity8.202234268188477
INFO:root:current mean train loss 21347.895737092393
INFO:root:current train perplexity8.180355072021484
INFO:root:current mean train loss 21232.170930232558
INFO:root:current train perplexity8.10593318939209


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:09<00:00, 249.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:09<00:00, 249.45s/it]
INFO:root:final mean train loss: 21195.50246503276
INFO:root:final train perplexity: 8.08944320678711
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.59s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.59s/it]
INFO:root:eval mean loss: 24728.02978515625
INFO:root:eval perplexity: 12.926046371459961
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/9

  4%|â–         | 9/200 [43:27<15:28:04, 291.54s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 20956.31156716418
INFO:root:current train perplexity7.861790180206299
INFO:root:current mean train loss 20896.162109375
INFO:root:current train perplexity7.83264684677124


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:09<00:00, 249.19s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:09<00:00, 249.19s/it]
INFO:root:final mean train loss: 20803.504103137602
INFO:root:final train perplexity: 7.782643795013428
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.21s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.21s/it]
INFO:root:eval mean loss: 24506.133277529763
INFO:root:eval perplexity: 12.632579803466797
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/10

  5%|â–Œ         | 10/200 [48:19<15:23:04, 291.50s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 20781.927837171053
INFO:root:current train perplexity7.6335768699646
INFO:root:current mean train loss 20590.851250656513
INFO:root:current train perplexity7.573883056640625
INFO:root:current mean train loss 20501.399632562785
INFO:root:current train perplexity7.5421881675720215


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:11<00:00, 251.84s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:11<00:00, 251.84s/it]
INFO:root:final mean train loss: 20476.26888545867
INFO:root:final train perplexity: 7.535460472106934
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.80s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.80s/it]
INFO:root:eval mean loss: 24308.118675595237
INFO:root:eval perplexity: 12.376325607299805
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/11

  6%|â–Œ         | 11/200 [53:12<15:20:14, 292.14s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 20295.313820422536
INFO:root:current train perplexity7.354383945465088
INFO:root:current mean train loss 20236.67755162646
INFO:root:current train perplexity7.345143795013428


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:03<00:00, 243.95s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:03<00:00, 243.95s/it]
INFO:root:final mean train loss: 20188.350826140373
INFO:root:final train perplexity: 7.324480056762695
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.17s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.17s/it]
INFO:root:eval mean loss: 24144.304640997023
INFO:root:eval perplexity: 12.168267250061035
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/12

  6%|â–Œ         | 12/200 [57:58<15:09:35, 290.30s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 20057.306046195652
INFO:root:current train perplexity7.2011237144470215
INFO:root:current mean train loss 19970.688055767278
INFO:root:current train perplexity7.173870086669922
INFO:root:current mean train loss 19941.26390835202
INFO:root:current train perplexity7.14486026763916


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:05<00:00, 245.50s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:05<00:00, 245.50s/it]
INFO:root:final mean train loss: 19936.49699155746
INFO:root:final train perplexity: 7.144773006439209
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.29s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.29s/it]
INFO:root:eval mean loss: 23990.279250372023
INFO:root:eval perplexity: 11.975829124450684
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/13

  6%|â–‹         | 13/200 [1:02:45<15:01:20, 289.20s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 19804.983463541666
INFO:root:current train perplexity7.008291721343994
INFO:root:current mean train loss 19740.000491071427
INFO:root:current train perplexity6.998111724853516


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.65s/it]
INFO:root:final mean train loss: 19704.165865990424
INFO:root:final train perplexity: 6.982909202575684
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.03s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.03s/it]
INFO:root:eval mean loss: 23851.492280505954
INFO:root:eval perplexity: 11.80504035949707
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/14

  7%|â–‹         | 14/200 [1:07:25<14:48:27, 286.60s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 19503.234013310186
INFO:root:current train perplexity6.873668193817139
INFO:root:current mean train loss 19478.132428026576
INFO:root:current train perplexity6.85795259475708
INFO:root:current mean train loss 19521.39219094163
INFO:root:current train perplexity6.850664138793945


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.31s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.31s/it]
INFO:root:final mean train loss: 19499.901902721773
INFO:root:final train perplexity: 6.843634128570557
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.40s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.40s/it]
INFO:root:eval mean loss: 23743.585100446428
INFO:root:eval perplexity: 11.673937797546387
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/15

  8%|â–Š         | 15/200 [1:12:06<14:38:08, 284.80s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 19318.062673061708
INFO:root:current train perplexity6.729578495025635
INFO:root:current mean train loss 19328.266028718575
INFO:root:current train perplexity6.726326942443848


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:57<00:00, 237.33s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:57<00:00, 237.33s/it]
INFO:root:final mean train loss: 19314.085220829133
INFO:root:final train perplexity: 6.719348907470703
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.97s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.97s/it]
INFO:root:eval mean loss: 23636.03185453869
INFO:root:eval perplexity: 11.544713020324707
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/16

  8%|â–Š         | 16/200 [1:16:54<14:35:50, 285.60s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 19101.1990297379
INFO:root:current train perplexity6.642737865447998
INFO:root:current mean train loss 19149.205763955153
INFO:root:current train perplexity6.607794761657715
INFO:root:current mean train loss 19146.126488095237
INFO:root:current train perplexity6.604774475097656


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:00<00:00, 240.15s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:00<00:00, 240.15s/it]
INFO:root:final mean train loss: 19143.528962166078
INFO:root:final train perplexity: 6.607259273529053
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.92s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.92s/it]
INFO:root:eval mean loss: 23517.169828869046
INFO:root:eval perplexity: 11.403560638427734
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/17

  8%|â–Š         | 17/200 [1:21:49<14:39:56, 288.50s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18985.177499058736
INFO:root:current train perplexity6.50240421295166
INFO:root:current mean train loss 18999.49625384221
INFO:root:current train perplexity6.497294902801514


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:07<00:00, 247.27s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:07<00:00, 247.27s/it]
INFO:root:final mean train loss: 18983.859327746977
INFO:root:final train perplexity: 6.504019737243652
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.70s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.70s/it]
INFO:root:eval mean loss: 23452.477841331845
INFO:root:eval perplexity: 11.327466011047363
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/18

  9%|â–‰         | 18/200 [1:26:43<14:40:22, 290.24s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18926.254575892857
INFO:root:current train perplexity6.45723819732666
INFO:root:current mean train loss 18916.482884837962
INFO:root:current train perplexity6.4432692527771
INFO:root:current mean train loss 18839.437416888297
INFO:root:current train perplexity6.40792179107666


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:58<00:00, 238.27s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:58<00:00, 238.27s/it]
INFO:root:final mean train loss: 18838.56781202747
INFO:root:final train perplexity: 6.411479473114014
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.89s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.89s/it]
INFO:root:eval mean loss: 23379.931640625
INFO:root:eval perplexity: 11.2427339553833
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/19

 10%|â–‰         | 19/200 [1:31:23<14:26:22, 287.20s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18698.884473778737
INFO:root:current train perplexity6.3088812828063965
INFO:root:current mean train loss 18731.994109291445
INFO:root:current train perplexity6.324851036071777


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:00<00:00, 240.68s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:00<00:00, 240.68s/it]
INFO:root:final mean train loss: 18700.177214591735
INFO:root:final train perplexity: 6.324558258056641
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.59s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.60s/it]
INFO:root:eval mean loss: 23291.213332403273
INFO:root:eval perplexity: 11.139979362487793
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/20

 10%|â–ˆ         | 20/200 [1:36:18<14:28:48, 289.60s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18718.02123397436
INFO:root:current train perplexity6.268912315368652
INFO:root:current mean train loss 18611.186628821943
INFO:root:current train perplexity6.254940032958984
INFO:root:current mean train loss 18600.770667167886
INFO:root:current train perplexity6.252020359039307


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:00<00:00, 240.25s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:00<00:00, 240.25s/it]
INFO:root:final mean train loss: 18577.81939500378
INFO:root:final train perplexity: 6.2486891746521
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.16s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.16s/it]
INFO:root:eval mean loss: 23236.212867373513
INFO:root:eval perplexity: 11.076742172241211
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/21

 10%|â–ˆ         | 21/200 [1:41:09<14:25:03, 289.96s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18447.692071600275
INFO:root:current train perplexity6.1717448234558105
INFO:root:current mean train loss 18473.630869600784
INFO:root:current train perplexity6.170950889587402


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:08<00:00, 248.47s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:08<00:00, 248.47s/it]
INFO:root:final mean train loss: 18459.051072643648
INFO:root:final train perplexity: 6.175917148590088
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.81s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.81s/it]
INFO:root:eval mean loss: 23169.26199776786
INFO:root:eval perplexity: 11.00025749206543
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/22

 11%|â–ˆ         | 22/200 [1:45:59<14:20:30, 290.06s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18377.640397892443
INFO:root:current train perplexity6.118587493896484
INFO:root:current mean train loss 18362.484853037586
INFO:root:current train perplexity6.110879898071289
INFO:root:current mean train loss 18370.545902456277
INFO:root:current train perplexity6.1103434562683105


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:09<00:00, 249.49s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:09<00:00, 249.50s/it]
INFO:root:final mean train loss: 18348.003969254034
INFO:root:final train perplexity: 6.108641624450684
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.55s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.55s/it]
INFO:root:eval mean loss: 23128.36560639881
INFO:root:eval perplexity: 10.95379638671875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/23

 12%|â–ˆâ–        | 23/200 [1:50:52<14:17:28, 290.67s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18296.275226151316
INFO:root:current train perplexity6.047379493713379
INFO:root:current mean train loss 18265.815845352565
INFO:root:current train perplexity6.044286251068115


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:08<00:00, 248.54s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:08<00:00, 248.54s/it]
INFO:root:final mean train loss: 18244.60002283896
INFO:root:final train perplexity: 6.046658039093018
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.69s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.69s/it]
INFO:root:eval mean loss: 23046.67138671875
INFO:root:eval perplexity: 10.861574172973633
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/24

 12%|â–ˆâ–        | 24/200 [1:55:43<14:13:06, 290.83s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18092.256565824468
INFO:root:current train perplexity5.975226402282715
INFO:root:current mean train loss 18143.9320923682
INFO:root:current train perplexity5.986406326293945
INFO:root:current mean train loss 18156.906574202934
INFO:root:current train perplexity5.987340450286865


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:05<00:00, 245.29s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:05<00:00, 245.29s/it]
INFO:root:final mean train loss: 18144.01218734249
INFO:root:final train perplexity: 5.986963748931885
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.87s/it]
INFO:root:eval mean loss: 23050.895949590773
INFO:root:eval perplexity: 10.866321563720703
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/25

 12%|â–ˆâ–Ž        | 25/200 [2:00:29<14:04:06, 289.41s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18099.30141256313
INFO:root:current train perplexity5.948810577392578
INFO:root:current mean train loss 18081.873478721733
INFO:root:current train perplexity5.932164669036865


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:01<00:00, 241.31s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:01<00:00, 241.32s/it]
INFO:root:final mean train loss: 18051.712040070564
INFO:root:final train perplexity: 5.932708740234375
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.51s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.51s/it]
INFO:root:eval mean loss: 23000.85791015625
INFO:root:eval perplexity: 10.81019401550293
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/26

 13%|â–ˆâ–Ž        | 26/200 [2:05:13<13:54:26, 287.74s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17975.798598345587
INFO:root:current train perplexity5.898330211639404
INFO:root:current mean train loss 17991.983870550495
INFO:root:current train perplexity5.890273571014404


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:02<00:00, 242.83s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:02<00:00, 242.84s/it]
INFO:root:final mean train loss: 17963.117821478074
INFO:root:final train perplexity: 5.881092548370361
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.56s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.56s/it]
INFO:root:eval mean loss: 22941.780947730655
INFO:root:eval perplexity: 10.74429988861084
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/27

 14%|â–ˆâ–Ž        | 27/200 [2:09:57<13:46:46, 286.74s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18096.436197916668
INFO:root:current train perplexity5.858996391296387
INFO:root:current mean train loss 17911.46509026092
INFO:root:current train perplexity5.847263813018799
INFO:root:current mean train loss 17907.68813500616
INFO:root:current train perplexity5.834621906280518


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:09<00:00, 249.33s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:09<00:00, 249.33s/it]
INFO:root:final mean train loss: 17880.312047158517
INFO:root:final train perplexity: 5.833255290985107
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.16s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.16s/it]
INFO:root:eval mean loss: 22905.549665178572
INFO:root:eval perplexity: 10.704085350036621
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/28

 14%|â–ˆâ–        | 28/200 [2:14:49<13:46:03, 288.16s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17863.845490056818
INFO:root:current train perplexity5.807662010192871
INFO:root:current mean train loss 17832.652885584677
INFO:root:current train perplexity5.7917585372924805


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:11<00:00, 251.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:11<00:00, 251.45s/it]
INFO:root:final mean train loss: 17795.68591702369
INFO:root:final train perplexity: 5.7847676277160645
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.54s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.54s/it]
INFO:root:eval mean loss: 22885.604073660714
INFO:root:eval perplexity: 10.682015419006348
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/29

 14%|â–ˆâ–        | 29/200 [2:19:42<13:45:20, 289.59s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17824.160993303572
INFO:root:current train perplexity5.742432117462158
INFO:root:current mean train loss 17825.517377336448
INFO:root:current train perplexity5.748927593231201
INFO:root:current mean train loss 17765.272380736715
INFO:root:current train perplexity5.743096828460693


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:06<00:00, 246.27s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:06<00:00, 246.27s/it]
INFO:root:final mean train loss: 17719.1107453377
INFO:root:final train perplexity: 5.741241931915283
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.17s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.18s/it]
INFO:root:eval mean loss: 22842.113304501487
INFO:root:eval perplexity: 10.634039878845215
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/30

 15%|â–ˆâ–Œ        | 30/200 [2:24:30<13:39:31, 289.25s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17668.88244835805
INFO:root:current train perplexity5.704522132873535
INFO:root:current mean train loss 17663.181849449684
INFO:root:current train perplexity5.7010884284973145


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:04<00:00, 244.21s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:04<00:00, 244.21s/it]
INFO:root:final mean train loss: 17644.949817288307
INFO:root:final train perplexity: 5.699398994445801
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.28s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.28s/it]
INFO:root:eval mean loss: 22811.424851190477
INFO:root:eval perplexity: 10.600319862365723
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/31

 16%|â–ˆâ–Œ        | 31/200 [2:29:17<13:33:10, 288.70s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17499.844815340908
INFO:root:current train perplexity5.6273298263549805
INFO:root:current mean train loss 17588.18378730293
INFO:root:current train perplexity5.647151470184326
INFO:root:current mean train loss 17588.18289025474
INFO:root:current train perplexity5.663141250610352


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:04<00:00, 244.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:04<00:00, 244.88s/it]
INFO:root:final mean train loss: 17576.364387758316
INFO:root:final train perplexity: 5.660974502563477
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.97s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.97s/it]
INFO:root:eval mean loss: 22797.900716145832
INFO:root:eval perplexity: 10.585490226745605
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/32

 16%|â–ˆâ–Œ        | 32/200 [2:34:04<13:26:49, 288.15s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17507.23235987103
INFO:root:current train perplexity5.60442590713501
INFO:root:current mean train loss 17528.963357937115
INFO:root:current train perplexity5.625335693359375


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:04<00:00, 244.53s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:04<00:00, 244.53s/it]
INFO:root:final mean train loss: 17507.47545205393
INFO:root:final train perplexity: 5.6226396560668945
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.39s/it]
INFO:root:eval mean loss: 22766.56375558036
INFO:root:eval perplexity: 10.551214218139648
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/33

 16%|â–ˆâ–‹        | 33/200 [2:38:51<13:20:59, 287.78s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17293.240494791666
INFO:root:current train perplexity5.54196310043335
INFO:root:current mean train loss 17441.790760869564
INFO:root:current train perplexity5.586451053619385
INFO:root:current mean train loss 17428.37179324128
INFO:root:current train perplexity5.576659202575684


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:09<00:00, 249.26s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:09<00:00, 249.26s/it]
INFO:root:final mean train loss: 17442.85433467742
INFO:root:final train perplexity: 5.586917877197266
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.82s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.82s/it]
INFO:root:eval mean loss: 22694.278390066964
INFO:root:eval perplexity: 10.472574234008789
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/34

 17%|â–ˆâ–‹        | 34/200 [2:43:43<13:19:44, 289.06s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17369.674935867537
INFO:root:current train perplexity5.5450053215026855
INFO:root:current mean train loss 17393.98927535554
INFO:root:current train perplexity5.548887252807617


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:03<00:00, 243.08s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:03<00:00, 243.08s/it]
INFO:root:final mean train loss: 17375.55212796119
INFO:root:final train perplexity: 5.549953460693359
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.91s/it]
INFO:root:eval mean loss: 22702.94945126488
INFO:root:eval perplexity: 10.481979370117188
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/35

 18%|â–ˆâ–Š        | 35/200 [2:48:37<13:19:00, 290.55s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17514.70034950658
INFO:root:current train perplexity5.576613903045654
INFO:root:current mean train loss 17362.485105370273
INFO:root:current train perplexity5.530961990356445
INFO:root:current mean train loss 17334.36771368436
INFO:root:current train perplexity5.517181396484375


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:27<00:00, 267.68s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:27<00:00, 267.68s/it]
INFO:root:final mean train loss: 17315.779796969506
INFO:root:final train perplexity: 5.517330169677734
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.88s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.94s/it]
INFO:root:eval mean loss: 22699.6171875
INFO:root:eval perplexity: 10.478363990783691
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/36

 18%|â–ˆâ–Š        | 36/200 [2:53:47<13:29:46, 296.26s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17232.054577464787
INFO:root:current train perplexity5.47107458114624
INFO:root:current mean train loss 17252.035316154972
INFO:root:current train perplexity5.485900402069092


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:13<00:00, 253.00s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:13<00:00, 253.00s/it]
INFO:root:final mean train loss: 17258.29462260585
INFO:root:final train perplexity: 5.4861369132995605
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.47s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.47s/it]
INFO:root:eval mean loss: 22664.045433407737
INFO:root:eval perplexity: 10.43985652923584
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/37

 18%|â–ˆâ–Š        | 37/200 [2:58:41<13:23:21, 295.71s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17141.038425611412
INFO:root:current train perplexity5.424408912658691
INFO:root:current mean train loss 17150.958206300813
INFO:root:current train perplexity5.444627285003662
INFO:root:current mean train loss 17218.83056859585
INFO:root:current train perplexity5.457818508148193


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:07<00:00, 247.43s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:07<00:00, 247.43s/it]
INFO:root:final mean train loss: 17203.090398973036
INFO:root:final train perplexity: 5.456345558166504
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.89s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.89s/it]
INFO:root:eval mean loss: 22630.12083798363
INFO:root:eval perplexity: 10.403264999389648
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/38

 19%|â–ˆâ–‰        | 38/200 [3:03:31<13:13:13, 293.79s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17170.813815104168
INFO:root:current train perplexity5.4209885597229
INFO:root:current mean train loss 17152.893387276785
INFO:root:current train perplexity5.421747207641602


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:10<00:00, 250.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:10<00:00, 250.61s/it]
INFO:root:final mean train loss: 17148.463426159273
INFO:root:final train perplexity: 5.42702579498291
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.49s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.49s/it]
INFO:root:eval mean loss: 22617.646484375
INFO:root:eval perplexity: 10.38984489440918
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/39

 20%|â–ˆâ–‰        | 39/200 [3:08:23<13:06:57, 293.28s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17016.750614872686
INFO:root:current train perplexity5.36221170425415
INFO:root:current mean train loss 17098.361451156496
INFO:root:current train perplexity5.397491455078125
INFO:root:current mean train loss 17103.85381676762
INFO:root:current train perplexity5.396873950958252


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.41s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.42s/it]
INFO:root:final mean train loss: 17095.054144090223
INFO:root:final train perplexity: 5.3985114097595215
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.24s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.24s/it]
INFO:root:eval mean loss: 22592.720842633928
INFO:root:eval perplexity: 10.363077163696289
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/40

 20%|â–ˆâ–ˆ        | 40/200 [3:13:04<12:52:44, 289.78s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17086.20055379747
INFO:root:current train perplexity5.380041122436523
INFO:root:current mean train loss 17098.052668907123
INFO:root:current train perplexity5.384843826293945


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:16<00:00, 256.76s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:16<00:00, 256.76s/it]
INFO:root:final mean train loss: 17045.376555412047
INFO:root:final train perplexity: 5.372125625610352
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.72s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.72s/it]
INFO:root:eval mean loss: 22588.778273809523
INFO:root:eval perplexity: 10.358847618103027
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/41

 20%|â–ˆâ–ˆ        | 41/200 [3:18:04<12:55:59, 292.83s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16895.756237399193
INFO:root:current train perplexity5.302000999450684
INFO:root:current mean train loss 16967.295167879292
INFO:root:current train perplexity5.327483654022217
INFO:root:current mean train loss 17012.638177252436
INFO:root:current train perplexity5.3439412117004395


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:07<00:00, 247.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:07<00:00, 247.30s/it]
INFO:root:final mean train loss: 16993.31958795363
INFO:root:final train perplexity: 5.344613075256348
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.12s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.12s/it]
INFO:root:eval mean loss: 22556.74483816964
INFO:root:eval perplexity: 10.324565887451172
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/42

 21%|â–ˆâ–ˆ        | 42/200 [3:22:54<12:48:23, 291.79s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16937.3504094503
INFO:root:current train perplexity5.317437648773193
INFO:root:current mean train loss 16971.98267802254
INFO:root:current train perplexity5.321794033050537


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:08<00:00, 248.75s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:08<00:00, 248.75s/it]
INFO:root:final mean train loss: 16948.47258143271
INFO:root:final train perplexity: 5.321023941040039
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.13s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.13s/it]
INFO:root:eval mean loss: 22575.12283761161
INFO:root:eval perplexity: 10.344220161437988
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/43

 22%|â–ˆâ–ˆâ–       | 43/200 [3:27:45<12:43:32, 291.80s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17035.725697544643
INFO:root:current train perplexity5.335039138793945
INFO:root:current mean train loss 16945.924110243057
INFO:root:current train perplexity5.29760217666626
INFO:root:current mean train loss 16919.27770944149
INFO:root:current train perplexity5.298407077789307


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:09<00:00, 249.33s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:09<00:00, 249.33s/it]
INFO:root:final mean train loss: 16902.93058924521
INFO:root:final train perplexity: 5.297175884246826
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.84s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.84s/it]
INFO:root:eval mean loss: 22547.30494326637
INFO:root:eval perplexity: 10.314482688903809
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/44

 22%|â–ˆâ–ˆâ–       | 44/200 [3:32:38<12:38:57, 291.91s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16836.765703573994
INFO:root:current train perplexity5.256521224975586
INFO:root:current mean train loss 16860.4609113887
INFO:root:current train perplexity5.265657901763916


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:13<00:00, 253.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:13<00:00, 253.75s/it]
INFO:root:final mean train loss: 16853.5748015373
INFO:root:final train perplexity: 5.271451950073242
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.47s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.47s/it]
INFO:root:eval mean loss: 22530.43533761161
INFO:root:eval perplexity: 10.296487808227539
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/45

 22%|â–ˆâ–ˆâ–Ž       | 45/200 [3:37:34<12:37:23, 293.19s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16849.73101963141
INFO:root:current train perplexity5.261168003082275
INFO:root:current mean train loss 16818.644854428956
INFO:root:current train perplexity5.251816749572754
INFO:root:current mean train loss 16824.519903079236
INFO:root:current train perplexity5.249008655548096


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:01<00:00, 241.97s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:01<00:00, 241.97s/it]
INFO:root:final mean train loss: 16808.279269310737
INFO:root:final train perplexity: 5.247952938079834
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.05s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.05s/it]
INFO:root:eval mean loss: 22514.615745907737
INFO:root:eval perplexity: 10.279644012451172
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/46

 23%|â–ˆâ–ˆâ–Ž       | 46/200 [3:42:18<12:25:26, 290.43s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16809.900734031595
INFO:root:current train perplexity5.225882053375244
INFO:root:current mean train loss 16799.405109824933
INFO:root:current train perplexity5.23128604888916


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:03<00:00, 243.07s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:03<00:00, 243.07s/it]
INFO:root:final mean train loss: 16767.817437941027
INFO:root:final train perplexity: 5.22705078125
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.60s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.60s/it]
INFO:root:eval mean loss: 22522.431129092263
INFO:root:eval perplexity: 10.287962913513184
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/47

 24%|â–ˆâ–ˆâ–Ž       | 47/200 [3:47:03<12:16:54, 288.98s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16732.445494186046
INFO:root:current train perplexity5.2042951583862305
INFO:root:current mean train loss 16741.552502185314
INFO:root:current train perplexity5.205905437469482
INFO:root:current mean train loss 16738.37579571759
INFO:root:current train perplexity5.206806659698486


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:01<00:00, 241.96s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:01<00:00, 241.96s/it]
INFO:root:final mean train loss: 16726.849522744455
INFO:root:final train perplexity: 5.205972194671631
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.55s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.55s/it]
INFO:root:eval mean loss: 22500.674618675595
INFO:root:eval perplexity: 10.264822006225586
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/48

 24%|â–ˆâ–ˆâ–       | 48/200 [3:51:48<12:08:39, 287.63s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16670.380787417762
INFO:root:current train perplexity5.168906211853027
INFO:root:current mean train loss 16682.177719350962
INFO:root:current train perplexity5.174616813659668


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:00<00:00, 240.35s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:00<00:00, 240.35s/it]
INFO:root:final mean train loss: 16684.44195359753
INFO:root:final train perplexity: 5.1842427253723145
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.97s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.97s/it]
INFO:root:eval mean loss: 22462.716494605655
INFO:root:eval perplexity: 10.224576950073242
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/49

 24%|â–ˆâ–ˆâ–       | 49/200 [3:56:31<12:00:33, 286.31s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16679.36087101064
INFO:root:current train perplexity5.147260665893555
INFO:root:current mean train loss 16653.12242240646
INFO:root:current train perplexity5.161105155944824
INFO:root:current mean train loss 16656.237838436235
INFO:root:current train perplexity5.164012432098389


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:58<00:00, 238.46s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:58<00:00, 238.46s/it]
INFO:root:final mean train loss: 16645.547934255294
INFO:root:final train perplexity: 5.164393424987793
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.61s/it]
INFO:root:eval mean loss: 22497.315755208332
INFO:root:eval perplexity: 10.26125717163086
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/50

 25%|â–ˆâ–ˆâ–Œ       | 50/200 [4:01:11<11:51:04, 284.43s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16633.136985085228
INFO:root:current train perplexity5.137048244476318
INFO:root:current mean train loss 16603.856347165514
INFO:root:current train perplexity5.147014141082764


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:56<00:00, 236.01s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:56<00:00, 236.01s/it]
INFO:root:final mean train loss: 16611.95849609375
INFO:root:final train perplexity: 5.147311687469482
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.47s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.47s/it]
INFO:root:eval mean loss: 22471.318219866072
INFO:root:eval perplexity: 10.233683586120605
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/51

 26%|â–ˆâ–ˆâ–Œ       | 51/200 [4:05:48<11:41:05, 282.32s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16585.08856081495
INFO:root:current train perplexity5.123079776763916
INFO:root:current mean train loss 16597.33628026697
INFO:root:current train perplexity5.128045082092285


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:56<00:00, 236.32s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:56<00:00, 236.32s/it]
INFO:root:final mean train loss: 16574.4837922127
INFO:root:final train perplexity: 5.128321170806885
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.95s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.95s/it]
INFO:root:eval mean loss: 22473.336728050595
INFO:root:eval perplexity: 10.235820770263672
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/52

 26%|â–ˆâ–ˆâ–Œ       | 52/200 [4:10:27<11:33:22, 281.10s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16645.029947916668
INFO:root:current train perplexity5.228565692901611
INFO:root:current mean train loss 16508.627503033982
INFO:root:current train perplexity5.094120025634766
INFO:root:current mean train loss 16543.185590170873
INFO:root:current train perplexity5.106828689575195


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:56<00:00, 236.20s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:56<00:00, 236.20s/it]
INFO:root:final mean train loss: 16533.48763152092
INFO:root:final train perplexity: 5.107626438140869
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.16s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.16s/it]
INFO:root:eval mean loss: 22442.305315290178
INFO:root:eval perplexity: 10.203001022338867
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/53

 26%|â–ˆâ–ˆâ–‹       | 53/200 [4:15:25<11:41:35, 286.37s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16491.879154829545
INFO:root:current train perplexity5.0977301597595215
INFO:root:current mean train loss 16542.571547379033
INFO:root:current train perplexity5.1041259765625


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:58<00:00, 238.10s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:58<00:00, 238.10s/it]
INFO:root:final mean train loss: 16499.710252331148
INFO:root:final train perplexity: 5.090638637542725
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.20s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.20s/it]
INFO:root:eval mean loss: 22442.607328869046
INFO:root:eval perplexity: 10.203319549560547
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/54

 27%|â–ˆâ–ˆâ–‹       | 54/200 [4:20:21<11:43:47, 289.23s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16342.309012276786
INFO:root:current train perplexity5.076267242431641
INFO:root:current mean train loss 16462.157838054907
INFO:root:current train perplexity5.0747971534729
INFO:root:current mean train loss 16464.3310546875
INFO:root:current train perplexity5.075770378112793


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:56<00:00, 236.76s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:56<00:00, 236.76s/it]
INFO:root:final mean train loss: 16462.541417275705
INFO:root:final train perplexity: 5.072010517120361
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.03s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.04s/it]
INFO:root:eval mean loss: 22429.608700706845
INFO:root:eval perplexity: 10.18960189819336
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/55

 28%|â–ˆâ–ˆâ–Š       | 55/200 [4:25:07<11:36:27, 288.19s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16460.504336599577
INFO:root:current train perplexity5.050108909606934
INFO:root:current mean train loss 16412.24628414898
INFO:root:current train perplexity5.049239635467529


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:56<00:00, 236.16s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:56<00:00, 236.17s/it]
INFO:root:final mean train loss: 16432.919969128023
INFO:root:final train perplexity: 5.057214260101318
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.64s/it]
INFO:root:eval mean loss: 22430.491071428572
INFO:root:eval perplexity: 10.190531730651855
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/56

 28%|â–ˆâ–ˆâ–Š       | 56/200 [4:29:51<11:28:21, 286.81s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16544.216264204544
INFO:root:current train perplexity5.082963943481445
INFO:root:current mean train loss 16395.078960796734
INFO:root:current train perplexity5.036872863769531
INFO:root:current mean train loss 16425.099132664396
INFO:root:current train perplexity5.043081760406494


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:55<00:00, 235.89s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:55<00:00, 235.89s/it]
INFO:root:final mean train loss: 16396.926470356604
INFO:root:final train perplexity: 5.039291858673096
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.94s/it]
INFO:root:eval mean loss: 22409.465029761905
INFO:root:eval perplexity: 10.168380737304688
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/57

 28%|â–ˆâ–ˆâ–Š       | 57/200 [4:34:30<11:18:03, 284.50s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16335.104740203373
INFO:root:current train perplexity4.998678207397461
INFO:root:current mean train loss 16367.354827693634
INFO:root:current train perplexity5.020636558532715


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:02<00:00, 242.20s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:02<00:00, 242.20s/it]
INFO:root:final mean train loss: 16365.40236343876
INFO:root:final train perplexity: 5.023647308349609
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.78s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.78s/it]
INFO:root:eval mean loss: 22394.371000744046
INFO:root:eval perplexity: 10.152509689331055
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/58

 29%|â–ˆâ–ˆâ–‰       | 58/200 [4:39:45<11:35:00, 293.66s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16336.488541666668
INFO:root:current train perplexity5.015723705291748
INFO:root:current mean train loss 16278.86539572011
INFO:root:current train perplexity4.981927871704102
INFO:root:current mean train loss 16319.787318313953
INFO:root:current train perplexity5.002790927886963


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:11<00:00, 251.02s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:11<00:00, 251.03s/it]
INFO:root:final mean train loss: 16335.21789157006
INFO:root:final train perplexity: 5.0087127685546875
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.46s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.46s/it]
INFO:root:eval mean loss: 22389.218238467263
INFO:root:eval perplexity: 10.147095680236816
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/59

 30%|â–ˆâ–ˆâ–‰       | 59/200 [4:44:41<11:32:02, 294.49s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16305.639837919776
INFO:root:current train perplexity4.973140239715576
INFO:root:current mean train loss 16307.320370976797
INFO:root:current train perplexity4.99369478225708


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.11s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.11s/it]
INFO:root:final mean train loss: 16299.996274886593
INFO:root:final train perplexity: 4.9913434982299805
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.83s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.83s/it]
INFO:root:eval mean loss: 22406.13829985119
INFO:root:eval perplexity: 10.164880752563477
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/60

 30%|â–ˆâ–ˆâ–ˆ       | 60/200 [4:50:17<11:56:03, 306.88s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16276.58341899671
INFO:root:current train perplexity4.988137245178223
INFO:root:current mean train loss 16245.00053341649
INFO:root:current train perplexity4.970363616943359
INFO:root:current mean train loss 16268.307229238013
INFO:root:current train perplexity4.973012924194336


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:06<00:00, 246.51s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:06<00:00, 246.51s/it]
INFO:root:final mean train loss: 16269.7476530998
INFO:root:final train perplexity: 4.976473808288574
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.69s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.69s/it]
INFO:root:eval mean loss: 22373.1650390625
INFO:root:eval perplexity: 10.130252838134766
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/61

 30%|â–ˆâ–ˆâ–ˆ       | 61/200 [4:55:07<11:39:21, 301.88s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16265.687279929578
INFO:root:current train perplexity4.964346408843994
INFO:root:current mean train loss 16269.87619928728
INFO:root:current train perplexity4.9653449058532715


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:07<00:00, 247.00s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:07<00:00, 247.01s/it]
INFO:root:final mean train loss: 16238.854988344254
INFO:root:final train perplexity: 4.961333274841309
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.21s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.21s/it]
INFO:root:eval mean loss: 22392.289946056546
INFO:root:eval perplexity: 10.150321006774902
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/62

 31%|â–ˆâ–ˆâ–ˆ       | 62/200 [5:00:21<11:42:30, 305.44s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16145.869395380434
INFO:root:current train perplexity4.919213771820068
INFO:root:current mean train loss 16222.538133574695
INFO:root:current train perplexity4.939744472503662
INFO:root:current mean train loss 16223.139446994115
INFO:root:current train perplexity4.948176860809326


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:01<00:00, 241.54s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:01<00:00, 241.54s/it]
INFO:root:final mean train loss: 16208.545362903225
INFO:root:final train perplexity: 4.946523189544678
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.10s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.10s/it]
INFO:root:eval mean loss: 22384.85372488839
INFO:root:eval perplexity: 10.142515182495117
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/63

 32%|â–ˆâ–ˆâ–ˆâ–      | 63/200 [5:05:05<11:22:27, 298.89s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16178.587799479166
INFO:root:current train perplexity4.938745498657227
INFO:root:current mean train loss 16210.267572544642
INFO:root:current train perplexity4.933098316192627


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:00<00:00, 240.67s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:00<00:00, 240.67s/it]
INFO:root:final mean train loss: 16185.371251260081
INFO:root:final train perplexity: 4.935230255126953
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.07s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.07s/it]
INFO:root:eval mean loss: 22358.723284040178
INFO:root:eval perplexity: 10.11512279510498
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/64

 32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [5:09:48<11:07:09, 294.34s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16254.134006076389
INFO:root:current train perplexity4.9274821281433105
INFO:root:current mean train loss 16128.994286724901
INFO:root:current train perplexity4.9047160148620605
INFO:root:current mean train loss 16155.270236784141
INFO:root:current train perplexity4.915589809417725


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:58<00:00, 238.47s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:58<00:00, 238.47s/it]
INFO:root:final mean train loss: 16149.341021137852
INFO:root:final train perplexity: 4.917722702026367
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.76s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.76s/it]
INFO:root:eval mean loss: 22391.159505208332
INFO:root:eval perplexity: 10.14913558959961
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/65

 32%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/200 [5:14:28<10:52:41, 290.08s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16143.524883801425
INFO:root:current train perplexity4.900636196136475
INFO:root:current mean train loss 16136.735307917248
INFO:root:current train perplexity4.903921604156494


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:57<00:00, 237.60s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:57<00:00, 237.60s/it]
INFO:root:final mean train loss: 16126.22218765751
INFO:root:final train perplexity: 4.906521797180176
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.85s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.85s/it]
INFO:root:eval mean loss: 22367.627464657737
INFO:root:eval perplexity: 10.12445068359375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/66

 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/200 [5:19:08<10:40:39, 286.86s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16052.010458669354
INFO:root:current train perplexity4.8621416091918945
INFO:root:current mean train loss 16102.891899749522
INFO:root:current train perplexity4.882416725158691
INFO:root:current mean train loss 16104.38835058171
INFO:root:current train perplexity4.890420913696289


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:57<00:00, 237.73s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:57<00:00, 237.74s/it]
INFO:root:final mean train loss: 16093.695324313256
INFO:root:final train perplexity: 4.890806198120117
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.53s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.53s/it]
INFO:root:eval mean loss: 22366.290550595237
INFO:root:eval perplexity: 10.123047828674316
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/67

 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/200 [5:23:49<10:32:05, 285.15s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16072.612963573041
INFO:root:current train perplexity4.870995044708252
INFO:root:current mean train loss 16085.742955942624
INFO:root:current train perplexity4.875473499298096


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:58<00:00, 238.08s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:58<00:00, 238.08s/it]
INFO:root:final mean train loss: 16074.042956936744
INFO:root:final train perplexity: 4.8813347816467285
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.84s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.84s/it]
INFO:root:eval mean loss: 22352.22486514137
INFO:root:eval perplexity: 10.108322143554688
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/68

 34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [5:28:29<10:23:49, 283.56s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16132.661607142858
INFO:root:current train perplexity4.8889899253845215
INFO:root:current mean train loss 16067.389641203703
INFO:root:current train perplexity4.866662502288818
INFO:root:current mean train loss 16064.95102642952
INFO:root:current train perplexity4.868224143981934


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:57<00:00, 237.96s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:57<00:00, 237.96s/it]
INFO:root:final mean train loss: 16048.500948998237
INFO:root:final train perplexity: 4.869052886962891
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.93s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.93s/it]
INFO:root:eval mean loss: 22362.272553943454
INFO:root:eval perplexity: 10.118839263916016
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/69

 34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [5:33:10<10:17:18, 282.74s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16017.323096264368
INFO:root:current train perplexity4.858346462249756
INFO:root:current mean train loss 16002.63826976103
INFO:root:current train perplexity4.853211879730225


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.42s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.42s/it]
INFO:root:final mean train loss: 16022.877787928428
INFO:root:final train perplexity: 4.8567633628845215
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.51s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.51s/it]
INFO:root:eval mean loss: 22374.369605654763
INFO:root:eval perplexity: 10.131513595581055
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/70

 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [5:37:55<10:14:34, 283.65s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15900.727614182691
INFO:root:current train perplexity4.835951805114746
INFO:root:current mean train loss 15964.251482407824
INFO:root:current train perplexity4.8312087059021
INFO:root:current mean train loss 16006.634630785826
INFO:root:current train perplexity4.84267520904541


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:00<00:00, 240.76s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:00<00:00, 240.76s/it]
INFO:root:final mean train loss: 16000.554636309223
INFO:root:final train perplexity: 4.846081256866455
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.73s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.73s/it]
INFO:root:eval mean loss: 22339.539202008928
INFO:root:eval perplexity: 10.09505844116211
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/71

 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [5:42:39<10:09:43, 283.59s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15942.545984289149
INFO:root:current train perplexity4.814358711242676
INFO:root:current mean train loss 15964.709475212696
INFO:root:current train perplexity4.825390815734863


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.51s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.51s/it]
INFO:root:final mean train loss: 15970.855460874496
INFO:root:final train perplexity: 4.831906795501709
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.96s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.96s/it]
INFO:root:eval mean loss: 22344.646135602678
INFO:root:eval perplexity: 10.100396156311035
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/72

 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/200 [5:47:26<10:07:12, 284.63s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15953.350563226744
INFO:root:current train perplexity4.7987518310546875
INFO:root:current mean train loss 15953.055329436189
INFO:root:current train perplexity4.81671667098999
INFO:root:current mean train loss 15959.984720614711
INFO:root:current train perplexity4.820165634155273


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.83s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.83s/it]
INFO:root:final mean train loss: 15945.489056987148
INFO:root:final train perplexity: 4.819832801818848
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.27s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.27s/it]
INFO:root:eval mean loss: 22336.35811941964
INFO:root:eval perplexity: 10.091734886169434
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/73

 36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/200 [5:52:07<10:00:14, 283.58s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15926.848468338816
INFO:root:current train perplexity4.807372093200684
INFO:root:current mean train loss 15937.223587740385
INFO:root:current train perplexity4.813138961791992


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:56<00:00, 236.53s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:56<00:00, 236.53s/it]
INFO:root:final mean train loss: 15927.922977570564
INFO:root:final train perplexity: 4.811489105224609
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.31s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.31s/it]
INFO:root:eval mean loss: 22353.789155505954
INFO:root:eval perplexity: 10.10995864868164
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/74

 37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/200 [5:56:57<9:59:21, 285.41s/it] 

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15887.629529587766
INFO:root:current train perplexity4.7813801765441895
INFO:root:current mean train loss 15897.998266103315
INFO:root:current train perplexity4.785857200622559
INFO:root:current mean train loss 15913.852278118675
INFO:root:current train perplexity4.798439025878906


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:21<00:00, 261.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:21<00:00, 261.75s/it]
INFO:root:final mean train loss: 15901.182892830142
INFO:root:final train perplexity: 4.798815727233887
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.58s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.58s/it]
INFO:root:eval mean loss: 22338.758882068454
INFO:root:eval perplexity: 10.094243049621582
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/75

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/200 [6:02:00<10:05:45, 290.77s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15887.660728377525
INFO:root:current train perplexity4.786104202270508
INFO:root:current mean train loss 15891.094304530465
INFO:root:current train perplexity4.788132667541504


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:02<00:00, 242.10s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:02<00:00, 242.10s/it]
INFO:root:final mean train loss: 15881.590414724042
INFO:root:final train perplexity: 4.789551258087158
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.54s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.54s/it]
INFO:root:eval mean loss: 22345.62276785714
INFO:root:eval perplexity: 10.10141658782959
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/76

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [6:07:21<10:19:52, 299.94s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15843.430970435049
INFO:root:current train perplexity4.774434566497803
INFO:root:current mean train loss 15870.580647247516
INFO:root:current train perplexity4.775213718414307


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:09<00:00, 249.25s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:09<00:00, 249.25s/it]
INFO:root:final mean train loss: 15853.797127016129
INFO:root:final train perplexity: 4.776439666748047
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.79s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.79s/it]
INFO:root:eval mean loss: 22353.250465029763
INFO:root:eval perplexity: 10.109394073486328
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/77

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/200 [6:12:27<10:18:20, 301.63s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15451.663411458334
INFO:root:current train perplexity4.651185035705566
INFO:root:current mean train loss 15835.571061513956
INFO:root:current train perplexity4.7571516036987305
INFO:root:current mean train loss 15829.018670143165
INFO:root:current train perplexity4.759969234466553


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:09<00:00, 249.95s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:09<00:00, 249.95s/it]
INFO:root:final mean train loss: 15831.274083291331
INFO:root:final train perplexity: 4.765840530395508
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.38s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.38s/it]
INFO:root:eval mean loss: 22335.376906622023
INFO:root:eval perplexity: 10.090710639953613
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/78

 39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [6:17:34<10:16:35, 303.24s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15779.68387784091
INFO:root:current train perplexity4.757380962371826
INFO:root:current mean train loss 15805.938016633065
INFO:root:current train perplexity4.753632068634033


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:12<00:00, 252.36s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:12<00:00, 252.36s/it]
INFO:root:final mean train loss: 15808.529344128025
INFO:root:final train perplexity: 4.755161762237549
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.66s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.66s/it]
INFO:root:eval mean loss: 22352.118350074405
INFO:root:eval perplexity: 10.108210563659668
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/79

 40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/200 [6:22:39<10:12:47, 303.86s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15739.06529017857
INFO:root:current train perplexity4.718741416931152
INFO:root:current mean train loss 15820.214204877337
INFO:root:current train perplexity4.748239994049072
INFO:root:current mean train loss 15821.523069519928
INFO:root:current train perplexity4.75349760055542


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:05<00:00, 245.37s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:05<00:00, 245.37s/it]
INFO:root:final mean train loss: 15792.481740643902
INFO:root:final train perplexity: 4.7476396560668945
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.02s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.02s/it]
INFO:root:eval mean loss: 22329.40013485863
INFO:root:eval perplexity: 10.084471702575684
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/80

 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/200 [6:27:28<9:58:27, 299.23s/it] 

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15714.804505429025
INFO:root:current train perplexity4.712294101715088
INFO:root:current mean train loss 15760.982679834906
INFO:root:current train perplexity4.729169845581055


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:01<00:00, 241.40s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:01<00:00, 241.40s/it]
INFO:root:final mean train loss: 15774.474148658013
INFO:root:final train perplexity: 4.73921537399292
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.40s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.40s/it]
INFO:root:eval mean loss: 22343.104631696428
INFO:root:eval perplexity: 10.098782539367676
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/81

 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/200 [6:32:10<9:43:38, 294.28s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15825.205788352272
INFO:root:current train perplexity4.792892932891846
INFO:root:current mean train loss 15777.094427435248
INFO:root:current train perplexity4.72499418258667
INFO:root:current mean train loss 15781.16945904917
INFO:root:current train perplexity4.728552341461182


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:58<00:00, 238.23s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:58<00:00, 238.23s/it]
INFO:root:final mean train loss: 15752.810672883064
INFO:root:final train perplexity: 4.729099750518799
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.57s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.57s/it]
INFO:root:eval mean loss: 22348.665992373513
INFO:root:eval perplexity: 10.104598999023438
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/82

 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/200 [6:36:51<9:30:43, 290.20s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15724.73587859623
INFO:root:current train perplexity4.715569496154785
INFO:root:current mean train loss 15730.211051332439
INFO:root:current train perplexity4.723438739776611


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.28s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.28s/it]
INFO:root:final mean train loss: 15730.575207125756
INFO:root:final train perplexity: 4.718739986419678
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.46s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.46s/it]
INFO:root:eval mean loss: 22335.343098958332
INFO:root:eval perplexity: 10.090676307678223
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/83

 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/200 [6:41:32<9:20:19, 287.34s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15710.1376953125
INFO:root:current train perplexity4.6769185066223145
INFO:root:current mean train loss 15699.417985733695
INFO:root:current train perplexity4.706602573394775
INFO:root:current mean train loss 15725.853620094476
INFO:root:current train perplexity4.71164083480835


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:14<00:00, 254.16s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:14<00:00, 254.16s/it]
INFO:root:final mean train loss: 15711.415397397934
INFO:root:final train perplexity: 4.709830284118652
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.57s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.58s/it]
INFO:root:eval mean loss: 22346.280343191964
INFO:root:eval perplexity: 10.102106094360352
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/84

 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/200 [6:46:28<9:20:55, 290.14s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15640.314351096082
INFO:root:current train perplexity4.6835246086120605
INFO:root:current mean train loss 15692.820938201721
INFO:root:current train perplexity4.6964287757873535


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.86s/it]
INFO:root:final mean train loss: 15688.761285597278
INFO:root:final train perplexity: 4.699317932128906
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.76s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.77s/it]
INFO:root:eval mean loss: 22332.619931175595
INFO:root:eval perplexity: 10.087833404541016
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/85

 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/200 [6:51:11<9:11:42, 287.85s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15466.7900390625
INFO:root:current train perplexity4.684299945831299
INFO:root:current mean train loss 15692.846663274684
INFO:root:current train perplexity4.690619468688965
INFO:root:current mean train loss 15684.567793057933
INFO:root:current train perplexity4.690349578857422


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.09s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.09s/it]
INFO:root:final mean train loss: 15671.578483335434
INFO:root:final train perplexity: 4.691361427307129
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.40s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.40s/it]
INFO:root:eval mean loss: 22328.81424386161
INFO:root:eval perplexity: 10.083858489990234
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/86

 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/200 [6:55:52<9:03:16, 285.93s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15595.465586487677
INFO:root:current train perplexity4.680535793304443
INFO:root:current mean train loss 15641.679584703947
INFO:root:current train perplexity4.676939487457275


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:13<00:00, 253.11s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:13<00:00, 253.11s/it]
INFO:root:final mean train loss: 15651.596175655242
INFO:root:final train perplexity: 4.682124137878418
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.89s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.89s/it]
INFO:root:eval mean loss: 22340.839750744046
INFO:root:eval perplexity: 10.096416473388672
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/87

 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/200 [7:00:47<9:03:36, 288.64s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15655.220363451086
INFO:root:current train perplexity4.660009860992432
INFO:root:current mean train loss 15636.623245363313
INFO:root:current train perplexity4.669745445251465
INFO:root:current mean train loss 15642.77446661295
INFO:root:current train perplexity4.672938823699951


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:05<00:00, 245.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:05<00:00, 245.95s/it]
INFO:root:final mean train loss: 15632.37894562752
INFO:root:final train perplexity: 4.673257827758789
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.20s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.20s/it]
INFO:root:eval mean loss: 22344.884626116072
INFO:root:eval perplexity: 10.1006441116333
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/88

 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/200 [7:05:36<8:59:01, 288.77s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15615.900091145833
INFO:root:current train perplexity4.652804851531982
INFO:root:current mean train loss 15609.321623883929
INFO:root:current train perplexity4.660384654998779


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:07<00:00, 247.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:07<00:00, 247.61s/it]
INFO:root:final mean train loss: 15615.207940870716
INFO:root:final train perplexity: 4.665349960327148
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.43s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.43s/it]
INFO:root:eval mean loss: 22317.809105282737
INFO:root:eval perplexity: 10.072382926940918
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/89
#####################best###########
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/200 [7:10:26<8:54:54, 289.14s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15568.733181423611
INFO:root:current train perplexity4.6464948654174805
INFO:root:current mean train loss 15559.529727485236
INFO:root:current train perplexity4.6404829025268555
INFO:root:current mean train loss 15597.408887148953
INFO:root:current train perplexity4.6534743309021


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:02<00:00, 242.79s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:02<00:00, 242.80s/it]
INFO:root:final mean train loss: 15600.42072123866
INFO:root:final train perplexity: 4.658550262451172
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.34s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.34s/it]
INFO:root:eval mean loss: 22336.842215401786
INFO:root:eval perplexity: 10.092240333557129
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/90

 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/200 [7:15:10<8:47:17, 287.61s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15636.735821301425
INFO:root:current train perplexity4.650838375091553
INFO:root:current mean train loss 15577.770731494413
INFO:root:current train perplexity4.644865989685059


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:01<00:00, 241.42s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:01<00:00, 241.42s/it]
INFO:root:final mean train loss: 15574.814311365928
INFO:root:final train perplexity: 4.646799087524414
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.61s/it]
INFO:root:eval mean loss: 22324.231352306546
INFO:root:eval perplexity: 10.07907772064209
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/91

 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/200 [7:19:53<8:39:55, 286.20s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15571.639301915322
INFO:root:current train perplexity4.645297050476074
INFO:root:current mean train loss 15559.103224892653
INFO:root:current train perplexity4.643146514892578
INFO:root:current mean train loss 15572.965029761905
INFO:root:current train perplexity4.642650604248047


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:57<00:00, 237.52s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:57<00:00, 237.52s/it]
INFO:root:final mean train loss: 15563.061062720513
INFO:root:final train perplexity: 4.641416072845459
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.31s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.31s/it]
INFO:root:eval mean loss: 22341.070963541668
INFO:root:eval perplexity: 10.096661567687988
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/92

 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/200 [7:24:31<8:30:34, 283.65s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15569.942947571537
INFO:root:current train perplexity4.637568950653076
INFO:root:current mean train loss 15556.00239604679
INFO:root:current train perplexity4.632731914520264


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.32s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.32s/it]
INFO:root:final mean train loss: 15545.12124732233
INFO:root:final train perplexity: 4.633210182189941
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.97s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.97s/it]
INFO:root:eval mean loss: 22339.433430989582
INFO:root:eval perplexity: 10.094947814941406
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/93

 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/200 [7:29:12<8:24:33, 282.93s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15523.308872767857
INFO:root:current train perplexity4.598345756530762
INFO:root:current mean train loss 15514.383101851852
INFO:root:current train perplexity4.619654655456543
INFO:root:current mean train loss 15540.701454454787
INFO:root:current train perplexity4.625640869140625


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:06<00:00, 246.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:06<00:00, 246.06s/it]
INFO:root:final mean train loss: 15530.890794323337
INFO:root:final train perplexity: 4.626712322235107
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.13s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.13s/it]
INFO:root:eval mean loss: 22352.306198846727
INFO:root:eval perplexity: 10.108404159545898
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/94

 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/200 [7:34:01<8:23:08, 284.80s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15541.721230693247
INFO:root:current train perplexity4.611335277557373
INFO:root:current mean train loss 15530.994537516712
INFO:root:current train perplexity4.61950159072876


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.90s/it]
INFO:root:final mean train loss: 15512.299891318044
INFO:root:final train perplexity: 4.618236064910889
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.27s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.27s/it]
INFO:root:eval mean loss: 22346.41906156994
INFO:root:eval perplexity: 10.102250099182129
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/95

 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/200 [7:38:43<8:16:28, 283.70s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15435.104542267629
INFO:root:current train perplexity4.563335418701172
INFO:root:current mean train loss 15492.612276584981
INFO:root:current train perplexity4.592388153076172
INFO:root:current mean train loss 15512.327834891476
INFO:root:current train perplexity4.608198642730713


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:02<00:00, 242.15s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:02<00:00, 242.15s/it]
INFO:root:final mean train loss: 15489.65638782132
INFO:root:final train perplexity: 4.607933044433594
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.46s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.46s/it]
INFO:root:eval mean loss: 22347.78699311756
INFO:root:eval perplexity: 10.103681564331055
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/96

 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/200 [7:43:27<8:12:12, 283.96s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15525.700764079671
INFO:root:current train perplexity4.610311508178711
INFO:root:current mean train loss 15478.790396964987
INFO:root:current train perplexity4.602680206298828


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:17<00:00, 257.78s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:17<00:00, 257.78s/it]
INFO:root:final mean train loss: 15479.61970766129
INFO:root:final train perplexity: 4.603374004364014
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.02s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.02s/it]
INFO:root:eval mean loss: 22349.08203125
INFO:root:eval perplexity: 10.105032920837402
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/97

 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/200 [7:48:27<8:15:34, 288.69s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15466.430164425872
INFO:root:current train perplexity4.605367183685303
INFO:root:current mean train loss 15477.877096536276
INFO:root:current train perplexity4.600423812866211
INFO:root:current mean train loss 15472.203402295525
INFO:root:current train perplexity4.596257209777832


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:13<00:00, 253.07s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:13<00:00, 253.07s/it]
INFO:root:final mean train loss: 15462.903276997227
INFO:root:final train perplexity: 4.595789909362793
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.44s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.44s/it]
INFO:root:eval mean loss: 22342.763578869046
INFO:root:eval perplexity: 10.098428726196289
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/98

 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [7:53:23<8:14:43, 291.01s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15490.407164884868
INFO:root:current train perplexity4.5873541831970215
INFO:root:current mean train loss 15458.394931891025
INFO:root:current train perplexity4.586975574493408


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:09<00:00, 249.41s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:09<00:00, 249.41s/it]
INFO:root:final mean train loss: 15446.573443012852
INFO:root:final train perplexity: 4.588393688201904
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.86s/it]
INFO:root:eval mean loss: 22345.334379650296
INFO:root:eval perplexity: 10.101113319396973
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/99

 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/200 [7:58:15<8:09:58, 291.08s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15358.62834524601
INFO:root:current train perplexity4.534323692321777
INFO:root:current mean train loss 15423.30290710034
INFO:root:current train perplexity4.569192886352539
INFO:root:current mean train loss 15439.586965460527
INFO:root:current train perplexity4.5797295570373535


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:07<00:00, 247.97s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:07<00:00, 247.97s/it]
INFO:root:final mean train loss: 15429.165031186996
INFO:root:final train perplexity: 4.580522537231445
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.01s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.01s/it]
INFO:root:eval mean loss: 22345.397019159227
INFO:root:eval perplexity: 10.101178169250488
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/100

 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/200 [8:03:03<8:04:03, 290.44s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15368.275232796717
INFO:root:current train perplexity4.557492256164551
INFO:root:current mean train loss 15435.05442741049
INFO:root:current train perplexity4.574209213256836


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:09<00:00, 249.78s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:09<00:00, 249.78s/it]
INFO:root:final mean train loss: 15416.039231823337
INFO:root:final train perplexity: 4.574595928192139
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.53s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.53s/it]
INFO:root:eval mean loss: 22342.918805803572
INFO:root:eval perplexity: 10.098589897155762
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/101

 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/200 [8:07:55<7:59:37, 290.68s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15443.725949754902
INFO:root:current train perplexity4.5589599609375
INFO:root:current mean train loss 15426.654555567053
INFO:root:current train perplexity4.569444179534912


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:09<00:00, 249.47s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:09<00:00, 249.47s/it]
INFO:root:final mean train loss: 15399.9705062374
INFO:root:final train perplexity: 4.567351341247559
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.53s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.53s/it]
INFO:root:eval mean loss: 22333.803548177082
INFO:root:eval perplexity: 10.089067459106445
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/102

 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/200 [8:12:45<7:54:23, 290.45s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15458.353841145834
INFO:root:current train perplexity4.5012946128845215
INFO:root:current mean train loss 15422.091597770024
INFO:root:current train perplexity4.555877685546875
INFO:root:current mean train loss 15388.3460591133
INFO:root:current train perplexity4.557161331176758


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:13<00:00, 253.68s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:13<00:00, 253.68s/it]
INFO:root:final mean train loss: 15391.189378307712
INFO:root:final train perplexity: 4.563397407531738
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.26s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.26s/it]
INFO:root:eval mean loss: 22335.664109002977
INFO:root:eval perplexity: 10.091011047363281
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/103

 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/200 [8:17:41<7:52:38, 292.36s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15351.877059659091
INFO:root:current train perplexity4.530515670776367
INFO:root:current mean train loss 15366.101159274194
INFO:root:current train perplexity4.551370620727539


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:07<00:00, 247.53s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:07<00:00, 247.53s/it]
INFO:root:final mean train loss: 15372.858721333165
INFO:root:final train perplexity: 4.555154800415039
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.04s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.04s/it]
INFO:root:eval mean loss: 22335.892066592263
INFO:root:eval perplexity: 10.091249465942383
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/104

 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/200 [8:22:31<7:46:22, 291.49s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15170.176199776786
INFO:root:current train perplexity4.488934516906738
INFO:root:current mean train loss 15286.191716559579
INFO:root:current train perplexity4.5336384773254395
INFO:root:current mean train loss 15371.077464522947
INFO:root:current train perplexity4.551846504211426


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:00<00:00, 240.48s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:00<00:00, 240.48s/it]
INFO:root:final mean train loss: 15362.65374952747
INFO:root:final train perplexity: 4.550571918487549
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.81s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.81s/it]
INFO:root:eval mean loss: 22355.478678385418
INFO:root:eval perplexity: 10.111727714538574
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/105

 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/200 [8:27:13<7:37:09, 288.73s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15356.897990598516
INFO:root:current train perplexity4.545511245727539
INFO:root:current mean train loss 15382.833879962658
INFO:root:current train perplexity4.545655250549316


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:10<00:00, 250.17s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:10<00:00, 250.17s/it]
INFO:root:final mean train loss: 15342.20576329385
INFO:root:final train perplexity: 4.541403293609619
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.63s/it]
INFO:root:eval mean loss: 22350.846261160714
INFO:root:eval perplexity: 10.106878280639648
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/106

 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/200 [8:32:06<7:34:11, 289.91s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15063.589488636364
INFO:root:current train perplexity4.494102478027344
INFO:root:current mean train loss 15303.461060670044
INFO:root:current train perplexity4.523073196411133
INFO:root:current mean train loss 15342.452787137145
INFO:root:current train perplexity4.535477161407471


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.73s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.73s/it]
INFO:root:final mean train loss: 15331.491273941532
INFO:root:final train perplexity: 4.536606311798096
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.01s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.01s/it]
INFO:root:eval mean loss: 22358.167805989582
INFO:root:eval perplexity: 10.114541053771973
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/107

 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/200 [8:36:48<7:25:31, 287.44s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15274.044828869048
INFO:root:current train perplexity4.516401767730713
INFO:root:current mean train loss 15321.121321414877
INFO:root:current train perplexity4.5279436111450195


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:02<00:00, 242.48s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:02<00:00, 242.48s/it]
INFO:root:final mean train loss: 15316.260155462449
INFO:root:final train perplexity: 4.5297956466674805
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.33s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.33s/it]
INFO:root:eval mean loss: 22348.165550595237
INFO:root:eval perplexity: 10.104076385498047
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/108

 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/200 [8:41:32<7:19:30, 286.63s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15042.959700520832
INFO:root:current train perplexity4.497917175292969
INFO:root:current mean train loss 15280.0408203125
INFO:root:current train perplexity4.513219833374023
INFO:root:current mean train loss 15307.011659702035
INFO:root:current train perplexity4.519918918609619


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:05<00:00, 245.21s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:05<00:00, 245.21s/it]
INFO:root:final mean train loss: 15301.571958480343
INFO:root:final train perplexity: 4.523238658905029
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.85s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.85s/it]
INFO:root:eval mean loss: 22362.04943266369
INFO:root:eval perplexity: 10.118603706359863
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/109

 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 109/200 [8:46:19<7:14:54, 286.76s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15220.914820429105
INFO:root:current train perplexity4.50040340423584
INFO:root:current mean train loss 15284.980661723428
INFO:root:current train perplexity4.517956256866455


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.28s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.28s/it]
INFO:root:final mean train loss: 15291.984701833417
INFO:root:final train perplexity: 4.51896333694458
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.52s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.52s/it]
INFO:root:eval mean loss: 22364.90294828869
INFO:root:eval perplexity: 10.12159252166748
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/110

 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/200 [8:51:39<7:24:45, 296.51s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15276.343133223685
INFO:root:current train perplexity4.496068477630615
INFO:root:current mean train loss 15325.227818080357
INFO:root:current train perplexity4.513160228729248
INFO:root:current mean train loss 15281.07980165525
INFO:root:current train perplexity4.509191989898682


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:00<00:00, 240.14s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:00<00:00, 240.14s/it]
INFO:root:final mean train loss: 15274.97888577369
INFO:root:final train perplexity: 4.51138973236084
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.07s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.07s/it]
INFO:root:eval mean loss: 22369.542178199405
INFO:root:eval perplexity: 10.12645435333252
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/111

 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/200 [8:57:03<7:32:04, 304.76s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15292.15069322183
INFO:root:current train perplexity4.510108470916748
INFO:root:current mean train loss 15279.69190880848
INFO:root:current train perplexity4.509419918060303


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.40s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.40s/it]
INFO:root:final mean train loss: 15263.659943611392
INFO:root:final train perplexity: 4.5063557624816895
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.78s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.78s/it]
INFO:root:eval mean loss: 22360.747233072918
INFO:root:eval perplexity: 10.117242813110352
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/112

 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/200 [9:02:23<7:33:44, 309.37s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15177.123896059782
INFO:root:current train perplexity4.477271556854248
INFO:root:current mean train loss 15233.761599657013
INFO:root:current train perplexity4.497996807098389
INFO:root:current mean train loss 15254.769995445628
INFO:root:current train perplexity4.501307487487793


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:56<00:00, 236.82s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:56<00:00, 236.82s/it]
INFO:root:final mean train loss: 15249.909293882309
INFO:root:final train perplexity: 4.500248908996582
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.21s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.21s/it]
INFO:root:eval mean loss: 22369.77908761161
INFO:root:eval perplexity: 10.126702308654785
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/113

 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/200 [9:07:23<7:24:47, 306.76s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15225.202903645833
INFO:root:current train perplexity4.4909491539001465
INFO:root:current mean train loss 15249.649481026785
INFO:root:current train perplexity4.497360706329346


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:14<00:00, 254.73s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:14<00:00, 254.73s/it]
INFO:root:final mean train loss: 15238.677687121975
INFO:root:final train perplexity: 4.495265483856201
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.82s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.82s/it]
INFO:root:eval mean loss: 22363.562034970237
INFO:root:eval perplexity: 10.12018871307373
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/114

 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/200 [9:12:50<7:28:07, 312.64s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15195.247504340277
INFO:root:current train perplexity4.5070343017578125
INFO:root:current mean train loss 15218.93734621063
INFO:root:current train perplexity4.4906721115112305
INFO:root:current mean train loss 15230.720281525331
INFO:root:current train perplexity4.4861650466918945


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:01<00:00, 241.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:01<00:00, 241.62s/it]
INFO:root:final mean train loss: 15224.42175686744
INFO:root:final train perplexity: 4.488949775695801
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.24s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.24s/it]
INFO:root:eval mean loss: 22353.141252790178
INFO:root:eval perplexity: 10.109278678894043
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/115

 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/200 [9:17:35<7:11:04, 304.29s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15195.020668512658
INFO:root:current train perplexity4.477460861206055
INFO:root:current mean train loss 15233.862277409218
INFO:root:current train perplexity4.48262357711792


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:04<00:00, 244.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:04<00:00, 244.91s/it]
INFO:root:final mean train loss: 15218.015002835182
INFO:root:final train perplexity: 4.486114025115967
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.91s/it]
INFO:root:eval mean loss: 22362.613932291668
INFO:root:eval perplexity: 10.119194030761719
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/116

 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/200 [9:22:21<6:58:38, 299.02s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15261.003370715725
INFO:root:current train perplexity4.488544940948486
INFO:root:current mean train loss 15236.849236641221
INFO:root:current train perplexity4.481784343719482
INFO:root:current mean train loss 15220.531440239449
INFO:root:current train perplexity4.481628894805908


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:15<00:00, 255.55s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:15<00:00, 255.55s/it]
INFO:root:final mean train loss: 15202.556282289566
INFO:root:final train perplexity: 4.479279041290283
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.59s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.60s/it]
INFO:root:eval mean loss: 22371.69847470238
INFO:root:eval perplexity: 10.128714561462402
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/117

 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 117/200 [9:27:19<6:53:17, 298.76s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15202.50249435241
INFO:root:current train perplexity4.468183994293213
INFO:root:current mean train loss 15216.658998249659
INFO:root:current train perplexity4.474259376525879


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:07<00:00, 247.58s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:07<00:00, 247.58s/it]
INFO:root:final mean train loss: 15187.944438319053
INFO:root:final train perplexity: 4.472828388214111
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.74s/it]
INFO:root:eval mean loss: 22376.088890438987
INFO:root:eval perplexity: 10.133317947387695
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/118

 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/200 [9:32:11<6:45:13, 296.51s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15145.280998883929
INFO:root:current train perplexity4.447673320770264
INFO:root:current mean train loss 15192.490964988427
INFO:root:current train perplexity4.459954738616943
INFO:root:current mean train loss 15191.426525099734
INFO:root:current train perplexity4.469854354858398


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:08<00:00, 248.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:08<00:00, 248.65s/it]
INFO:root:final mean train loss: 15179.776694020917
INFO:root:final train perplexity: 4.469226360321045
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.56s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.56s/it]
INFO:root:eval mean loss: 22378.607096354168
INFO:root:eval perplexity: 10.135958671569824
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/119

 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 119/200 [9:37:31<6:49:49, 303.58s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15122.897618085488
INFO:root:current train perplexity4.459848880767822
INFO:root:current mean train loss 15176.673645345922
INFO:root:current train perplexity4.464545726776123


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:01<00:00, 241.50s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:01<00:00, 241.50s/it]
INFO:root:final mean train loss: 15169.521791519657
INFO:root:final train perplexity: 4.46470832824707
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.38s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.38s/it]
INFO:root:eval mean loss: 22381.931617373513
INFO:root:eval perplexity: 10.139446258544922
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/120

 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/200 [9:43:04<6:56:36, 312.45s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15117.570663060897
INFO:root:current train perplexity4.440435886383057
INFO:root:current mean train loss 15133.634449471672
INFO:root:current train perplexity4.453242778778076
INFO:root:current mean train loss 15165.578427366632
INFO:root:current train perplexity4.4584479331970215


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:03<00:00, 243.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:03<00:00, 243.94s/it]
INFO:root:final mean train loss: 15156.255308089718
INFO:root:final train perplexity: 4.458869934082031
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.85s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.85s/it]
INFO:root:eval mean loss: 22383.150227864582
INFO:root:eval perplexity: 10.140725135803223
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/121

 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/200 [9:48:21<6:53:18, 313.91s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15123.214940333104
INFO:root:current train perplexity4.445774078369141
INFO:root:current mean train loss 15141.094798142998
INFO:root:current train perplexity4.4449920654296875


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.84s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.84s/it]
INFO:root:final mean train loss: 15147.07513624622
INFO:root:final train perplexity: 4.454833984375
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.53s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.53s/it]
INFO:root:eval mean loss: 22369.384765625
INFO:root:eval perplexity: 10.126287460327148
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/122

 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/200 [9:53:40<6:49:57, 315.36s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15117.902434593023
INFO:root:current train perplexity4.4628400802612305
INFO:root:current mean train loss 15158.211675043707
INFO:root:current train perplexity4.453125
INFO:root:current mean train loss 15145.109981835134
INFO:root:current train perplexity4.448910713195801


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:03<00:00, 243.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:03<00:00, 243.61s/it]
INFO:root:final mean train loss: 15132.238379693801
INFO:root:final train perplexity: 4.448320388793945
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.70s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.70s/it]
INFO:root:eval mean loss: 22388.12169828869
INFO:root:eval perplexity: 10.145944595336914
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/123

 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 123/200 [9:58:42<6:39:24, 311.23s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15091.78383018092
INFO:root:current train perplexity4.435700416564941
INFO:root:current mean train loss 15136.974273838141
INFO:root:current train perplexity4.440688610076904


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:17<00:00, 257.55s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:17<00:00, 257.55s/it]
INFO:root:final mean train loss: 15119.14967001638
INFO:root:final train perplexity: 4.442580699920654
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.75s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.75s/it]
INFO:root:eval mean loss: 22389.87425595238
INFO:root:eval perplexity: 10.147785186767578
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/124

 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/200 [10:03:48<6:32:20, 309.74s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15119.632791722075
INFO:root:current train perplexity4.432250499725342
INFO:root:current mean train loss 15127.984614158164
INFO:root:current train perplexity4.437053203582764
INFO:root:current mean train loss 15126.473628858806
INFO:root:current train perplexity4.440094947814941


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:02<00:00, 242.70s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:02<00:00, 242.70s/it]
INFO:root:final mean train loss: 15112.582590410786
INFO:root:final train perplexity: 4.439704418182373
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.47s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.47s/it]
INFO:root:eval mean loss: 22378.866024925595
INFO:root:eval perplexity: 10.136229515075684
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/125

 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 125/200 [10:08:35<6:18:41, 302.96s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15093.97788431187
INFO:root:current train perplexity4.431070327758789
INFO:root:current mean train loss 15109.148668145415
INFO:root:current train perplexity4.4317307472229


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:03<00:00, 243.70s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:03<00:00, 243.70s/it]
INFO:root:final mean train loss: 15099.213134765625
INFO:root:final train perplexity: 4.4338531494140625
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.28s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.28s/it]
INFO:root:eval mean loss: 22370.26536923363
INFO:root:eval perplexity: 10.127214431762695
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/126

 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/200 [10:13:21<6:07:20, 297.85s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15089.96120557598
INFO:root:current train perplexity4.422826290130615
INFO:root:current mean train loss 15076.03730339404
INFO:root:current train perplexity4.422917366027832


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.05s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.05s/it]
INFO:root:final mean train loss: 15090.11677797379
INFO:root:final train perplexity: 4.429877758026123
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.84s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.84s/it]
INFO:root:eval mean loss: 22386.157040550595
INFO:root:eval perplexity: 10.14388370513916
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/127

 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 127/200 [10:18:02<5:56:12, 292.78s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14914.571940104166
INFO:root:current train perplexity4.4340901374816895
INFO:root:current mean train loss 15081.163275561286
INFO:root:current train perplexity4.4283599853515625
INFO:root:current mean train loss 15077.036349291871
INFO:root:current train perplexity4.420719146728516


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:58<00:00, 238.35s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:58<00:00, 238.35s/it]
INFO:root:final mean train loss: 15079.617396200856
INFO:root:final train perplexity: 4.42529296875
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.06s/it]
INFO:root:eval mean loss: 22378.029343377977
INFO:root:eval perplexity: 10.13535213470459
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/128

 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/200 [10:22:47<5:48:37, 290.52s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15100.434818892045
INFO:root:current train perplexity4.427130222320557
INFO:root:current mean train loss 15080.410414566531
INFO:root:current train perplexity4.424349784851074


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.33s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.33s/it]
INFO:root:final mean train loss: 15071.593021515877
INFO:root:final train perplexity: 4.421790599822998
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.67s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.67s/it]
INFO:root:eval mean loss: 22387.925432477678
INFO:root:eval perplexity: 10.14574146270752
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/129

 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 129/200 [10:27:48<5:47:18, 293.50s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15107.576171875
INFO:root:current train perplexity4.381041526794434
INFO:root:current mean train loss 15087.149341048482
INFO:root:current train perplexity4.426760673522949
INFO:root:current mean train loss 15072.685259095713
INFO:root:current train perplexity4.418363571166992


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:02<00:00, 242.46s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:02<00:00, 242.46s/it]
INFO:root:final mean train loss: 15062.741588961693
INFO:root:final train perplexity: 4.417933464050293
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.91s/it]
INFO:root:eval mean loss: 22408.228422619046
INFO:root:eval perplexity: 10.167081832885742
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/130

 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/200 [10:32:35<5:40:19, 291.70s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15100.666247351695
INFO:root:current train perplexity4.426970958709717
INFO:root:current mean train loss 15048.445269506683
INFO:root:current train perplexity4.415123462677002


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:04<00:00, 244.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:04<00:00, 244.06s/it]
INFO:root:final mean train loss: 15054.474955897178
INFO:root:final train perplexity: 4.414332389831543
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.24s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.24s/it]
INFO:root:eval mean loss: 22385.595656622023
INFO:root:eval perplexity: 10.143291473388672
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/131

 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/200 [10:37:31<5:36:57, 293.00s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14957.639115767046
INFO:root:current train perplexity4.428703784942627
INFO:root:current mean train loss 15045.621894355292
INFO:root:current train perplexity4.40413761138916
INFO:root:current mean train loss 15041.659096378851
INFO:root:current train perplexity4.406042575836182


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:58<00:00, 238.81s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:58<00:00, 238.81s/it]
INFO:root:final mean train loss: 15040.0759749874
INFO:root:final train perplexity: 4.408066272735596
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.49s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.49s/it]
INFO:root:eval mean loss: 22387.422293526786
INFO:root:eval perplexity: 10.145211219787598
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/132

 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/200 [10:42:13<5:28:26, 289.81s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14946.738513764882
INFO:root:current train perplexity4.397921562194824
INFO:root:current mean train loss 15002.034191669862
INFO:root:current train perplexity4.396707534790039


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:01<00:00, 241.99s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:01<00:00, 242.00s/it]
INFO:root:final mean train loss: 15033.093131772934
INFO:root:final train perplexity: 4.405032157897949
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.97s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.97s/it]
INFO:root:eval mean loss: 22398.090750558036
INFO:root:eval perplexity: 10.156418800354004
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/133

 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/200 [10:46:57<5:21:37, 288.02s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15003.548111979168
INFO:root:current train perplexity4.415830612182617
INFO:root:current mean train loss 15038.496076766305
INFO:root:current train perplexity4.407315254211426
INFO:root:current mean train loss 15029.131227289245
INFO:root:current train perplexity4.397757530212402


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:56<00:00, 236.77s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:56<00:00, 236.77s/it]
INFO:root:final mean train loss: 15021.1343718498
INFO:root:final train perplexity: 4.399838447570801
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.50s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.50s/it]
INFO:root:eval mean loss: 22408.75362723214
INFO:root:eval perplexity: 10.167634010314941
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/134

 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 134/200 [10:51:40<5:14:59, 286.36s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14983.130451259329
INFO:root:current train perplexity4.384446620941162
INFO:root:current mean train loss 15051.462639174775
INFO:root:current train perplexity4.404232501983643


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:02<00:00, 242.69s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:02<00:00, 242.69s/it]
INFO:root:final mean train loss: 15016.180250598538
INFO:root:final train perplexity: 4.397690296173096
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.84s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.85s/it]
INFO:root:eval mean loss: 22400.452008928572
INFO:root:eval perplexity: 10.15890121459961
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/135

 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/200 [10:56:24<5:09:37, 285.81s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15062.198601973685
INFO:root:current train perplexity4.3979644775390625
INFO:root:current mean train loss 15044.908958114496
INFO:root:current train perplexity4.405294418334961
INFO:root:current mean train loss 15039.981396261415
INFO:root:current train perplexity4.396650791168213


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:08<00:00, 248.57s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:08<00:00, 248.57s/it]
INFO:root:final mean train loss: 15008.803427419354
INFO:root:final train perplexity: 4.394491195678711
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.14s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.14s/it]
INFO:root:eval mean loss: 22410.318033854168
INFO:root:eval perplexity: 10.169280052185059
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/136

 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/200 [11:01:15<5:06:24, 287.25s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14989.288814920774
INFO:root:current train perplexity4.382376670837402
INFO:root:current mean train loss 15024.936420641447
INFO:root:current train perplexity4.390195846557617


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:00<00:00, 240.38s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:00<00:00, 240.38s/it]
INFO:root:final mean train loss: 14994.270759828629
INFO:root:final train perplexity: 4.3881964683532715
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.34s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.34s/it]
INFO:root:eval mean loss: 22405.301432291668
INFO:root:eval perplexity: 10.16400146484375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/137

 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 137/200 [11:06:05<5:02:37, 288.22s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14877.211744225544
INFO:root:current train perplexity4.349967956542969
INFO:root:current mean train loss 14984.02447757876
INFO:root:current train perplexity4.385744094848633
INFO:root:current mean train loss 14996.012007777466
INFO:root:current train perplexity4.386850833892822


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:03<00:00, 243.26s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:03<00:00, 243.26s/it]
INFO:root:final mean train loss: 14990.794228830646
INFO:root:final train perplexity: 4.386693000793457
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.85s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.85s/it]
INFO:root:eval mean loss: 22411.72693452381
INFO:root:eval perplexity: 10.170762062072754
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/138

 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/200 [11:11:06<5:01:38, 291.91s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14934.045755208334
INFO:root:current train perplexity4.36562967300415
INFO:root:current mean train loss 14979.740206473214
INFO:root:current train perplexity4.372967720031738


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:00<00:00, 240.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:00<00:00, 240.30s/it]
INFO:root:final mean train loss: 14981.692926222278
INFO:root:final train perplexity: 4.38275671005249
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.04s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.04s/it]
INFO:root:eval mean loss: 22400.767717633928
INFO:root:eval perplexity: 10.159233093261719
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/139

 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/200 [11:15:52<4:55:03, 290.23s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14993.40552662037
INFO:root:current train perplexity4.38515043258667
INFO:root:current mean train loss 14940.521092212106
INFO:root:current train perplexity4.367842197418213
INFO:root:current mean train loss 14979.77604883673
INFO:root:current train perplexity4.375563621520996


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:10<00:00, 250.03s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:10<00:00, 250.03s/it]
INFO:root:final mean train loss: 14969.697765719506
INFO:root:final train perplexity: 4.3775739669799805
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.34s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.34s/it]
INFO:root:eval mean loss: 22405.41873604911
INFO:root:eval perplexity: 10.164122581481934
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/140

 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/200 [11:20:47<4:51:32, 291.54s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14931.757231507121
INFO:root:current train perplexity4.374907493591309
INFO:root:current mean train loss 14953.255641148742
INFO:root:current train perplexity4.3710222244262695


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:17<00:00, 257.79s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:17<00:00, 257.79s/it]
INFO:root:final mean train loss: 14963.726302608367
INFO:root:final train perplexity: 4.374996185302734
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.38s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.38s/it]
INFO:root:eval mean loss: 22412.45824032738
INFO:root:eval perplexity: 10.171533584594727
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/141

 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/200 [11:25:47<4:49:11, 294.10s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14951.060767389114
INFO:root:current train perplexity4.3789873123168945
INFO:root:current mean train loss 14979.367232228053
INFO:root:current train perplexity4.380836009979248
INFO:root:current mean train loss 14967.390667275433
INFO:root:current train perplexity4.373971462249756


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:10<00:00, 250.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:10<00:00, 250.61s/it]
INFO:root:final mean train loss: 14956.030131678428
INFO:root:final train perplexity: 4.371676921844482
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.78s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.78s/it]
INFO:root:eval mean loss: 22421.97595796131
INFO:root:eval perplexity: 10.18155574798584
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/142

 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 142/200 [11:30:39<4:43:45, 293.54s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15023.075030591115
INFO:root:current train perplexity4.3654255867004395
INFO:root:current mean train loss 14966.229993809768
INFO:root:current train perplexity4.362545967102051


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:06<00:00, 246.13s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:06<00:00, 246.13s/it]
INFO:root:final mean train loss: 14948.938606508316
INFO:root:final train perplexity: 4.3686203956604
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.36s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.36s/it]
INFO:root:eval mean loss: 22407.806826636905
INFO:root:eval perplexity: 10.166638374328613
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/143

 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 143/200 [11:35:28<4:37:24, 292.00s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14894.286830357143
INFO:root:current train perplexity4.337067604064941
INFO:root:current mean train loss 14930.419386574074
INFO:root:current train perplexity4.353198051452637
INFO:root:current mean train loss 14940.423312832447
INFO:root:current train perplexity4.3640360832214355


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:12<00:00, 252.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:12<00:00, 252.64s/it]
INFO:root:final mean train loss: 14939.10015672253
INFO:root:final train perplexity: 4.364382743835449
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.26s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.26s/it]
INFO:root:eval mean loss: 22414.318661644345
INFO:root:eval perplexity: 10.173490524291992
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/144

 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/200 [11:40:21<4:33:02, 292.54s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14913.679193606322
INFO:root:current train perplexity4.350917816162109
INFO:root:current mean train loss 14953.622221758022
INFO:root:current train perplexity4.363746166229248


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:09<00:00, 249.42s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:09<00:00, 249.42s/it]
INFO:root:final mean train loss: 14934.808971774193
INFO:root:final train perplexity: 4.36253547668457
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.61s/it]
INFO:root:eval mean loss: 22427.959077380954
INFO:root:eval perplexity: 10.187862396240234
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/145

 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 145/200 [11:45:14<4:28:16, 292.66s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14951.105018028846
INFO:root:current train perplexity4.357279300689697
INFO:root:current mean train loss 14933.970105946493
INFO:root:current train perplexity4.35939884185791
INFO:root:current mean train loss 14938.81672904681
INFO:root:current train perplexity4.359569549560547


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:07<00:00, 247.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:07<00:00, 247.74s/it]
INFO:root:final mean train loss: 14927.767645066784
INFO:root:final train perplexity: 4.3595075607299805
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.78s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.78s/it]
INFO:root:eval mean loss: 22417.5478515625
INFO:root:eval perplexity: 10.176891326904297
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/146

 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/200 [11:50:07<4:23:29, 292.77s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14961.675759787087
INFO:root:current train perplexity4.347103118896484
INFO:root:current mean train loss 14920.475647292213
INFO:root:current train perplexity4.345366477966309


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:06<00:00, 246.15s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:06<00:00, 246.15s/it]
INFO:root:final mean train loss: 14915.617026052167
INFO:root:final train perplexity: 4.3542866706848145
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.59s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.59s/it]
INFO:root:eval mean loss: 22427.972749255954
INFO:root:eval perplexity: 10.187874794006348
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/147

 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 147/200 [11:54:55<4:17:15, 291.24s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14908.453352107557
INFO:root:current train perplexity4.351239204406738
INFO:root:current mean train loss 14904.550726617133
INFO:root:current train perplexity4.344014644622803
INFO:root:current mean train loss 14926.919134194959
INFO:root:current train perplexity4.352500915527344


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:06<00:00, 246.98s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:06<00:00, 246.98s/it]
INFO:root:final mean train loss: 14912.315921906502
INFO:root:final train perplexity: 4.35286808013916
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.72s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.72s/it]
INFO:root:eval mean loss: 22427.330519903273
INFO:root:eval perplexity: 10.187199592590332
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/148

 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/200 [11:59:44<4:11:43, 290.45s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14926.990614720395
INFO:root:current train perplexity4.350816249847412
INFO:root:current mean train loss 14908.908819110577
INFO:root:current train perplexity4.3481597900390625


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:09<00:00, 249.97s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:09<00:00, 249.97s/it]
INFO:root:final mean train loss: 14907.095518050655
INFO:root:final train perplexity: 4.350627422332764
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.85s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.85s/it]
INFO:root:eval mean loss: 22425.210007440477
INFO:root:eval perplexity: 10.184967041015625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/149

 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 149/200 [12:04:42<4:08:52, 292.79s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14892.612429355053
INFO:root:current train perplexity4.338436603546143
INFO:root:current mean train loss 14879.858816964286
INFO:root:current train perplexity4.337695598602295
INFO:root:current mean train loss 14909.150011070344
INFO:root:current train perplexity4.346944332122803


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:08<00:00, 248.47s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:08<00:00, 248.47s/it]
INFO:root:final mean train loss: 14898.04851704259
INFO:root:final train perplexity: 4.346747398376465
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.17s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.17s/it]
INFO:root:eval mean loss: 22418.253115699405
INFO:root:eval perplexity: 10.177634239196777
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/150

 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/200 [12:10:20<4:15:19, 306.39s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14880.754655934343
INFO:root:current train perplexity4.3351287841796875
INFO:root:current mean train loss 14872.438496191897
INFO:root:current train perplexity4.340085506439209


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:09<00:00, 249.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:09<00:00, 249.45s/it]
INFO:root:final mean train loss: 14891.761742376511
INFO:root:final train perplexity: 4.344052314758301
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.12s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.12s/it]
INFO:root:eval mean loss: 22428.10425967262
INFO:root:eval perplexity: 10.18801498413086
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/151

 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 151/200 [12:15:11<4:06:33, 301.90s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14885.58092064951
INFO:root:current train perplexity4.3447651863098145
INFO:root:current mean train loss 14867.526134364653
INFO:root:current train perplexity4.334132671356201


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:03<00:00, 243.42s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:03<00:00, 243.42s/it]
INFO:root:final mean train loss: 14882.358969411542
INFO:root:final train perplexity: 4.340025901794434
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.72s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.72s/it]
INFO:root:eval mean loss: 22430.390183221727
INFO:root:eval perplexity: 10.190424919128418
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/152

 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 152/200 [12:19:58<3:57:43, 297.16s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14997.099934895834
INFO:root:current train perplexity4.368411540985107
INFO:root:current mean train loss 14873.199844508496
INFO:root:current train perplexity4.3327860832214355
INFO:root:current mean train loss 14893.561634082513
INFO:root:current train perplexity4.339787483215332


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:04<00:00, 244.22s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:04<00:00, 244.23s/it]
INFO:root:final mean train loss: 14880.979984406502
INFO:root:final train perplexity: 4.33943510055542
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.49s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.49s/it]
INFO:root:eval mean loss: 22435.349702380954
INFO:root:eval perplexity: 10.195657730102539
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/153

 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 153/200 [12:24:44<3:50:18, 294.01s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14834.388192471592
INFO:root:current train perplexity4.314537525177002
INFO:root:current mean train loss 14857.535855594759
INFO:root:current train perplexity4.326715469360352


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:01<00:00, 241.72s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:01<00:00, 241.73s/it]
INFO:root:final mean train loss: 14873.486202116936
INFO:root:final train perplexity: 4.33622932434082
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.10s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.10s/it]
INFO:root:eval mean loss: 22428.816429501487
INFO:root:eval perplexity: 10.18876838684082
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/154

 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/200 [12:30:09<3:52:29, 303.26s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14955.198521205357
INFO:root:current train perplexity4.378027439117432
INFO:root:current mean train loss 14840.646530008762
INFO:root:current train perplexity4.330038547515869
INFO:root:current mean train loss 14881.785745961655
INFO:root:current train perplexity4.334275245666504


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:10<00:00, 250.18s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:10<00:00, 250.18s/it]
INFO:root:final mean train loss: 14865.072269562752
INFO:root:final train perplexity: 4.332632541656494
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.07s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.07s/it]
INFO:root:eval mean loss: 22430.53478422619
INFO:root:eval perplexity: 10.190580368041992
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/155

 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 155/200 [12:35:08<3:46:30, 302.01s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14894.30417439089
INFO:root:current train perplexity4.330651760101318
INFO:root:current mean train loss 14853.613225972877
INFO:root:current train perplexity4.327125072479248


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:10<00:00, 250.17s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:10<00:00, 250.17s/it]
INFO:root:final mean train loss: 14859.951451455394
INFO:root:final train perplexity: 4.330444812774658
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.67s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.67s/it]
INFO:root:eval mean loss: 22430.929780505954
INFO:root:eval perplexity: 10.190998077392578
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/156

 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 156/200 [12:40:29<3:45:32, 307.55s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14830.520241477272
INFO:root:current train perplexity4.279917240142822
INFO:root:current mean train loss 14858.584397874436
INFO:root:current train perplexity4.320191383361816
INFO:root:current mean train loss 14859.96207605154
INFO:root:current train perplexity4.323570251464844


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:09<00:00, 249.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:09<00:00, 249.06s/it]
INFO:root:final mean train loss: 14851.634458480343
INFO:root:final train perplexity: 4.3268938064575195
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.80s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.80s/it]
INFO:root:eval mean loss: 22442.596726190477
INFO:root:eval perplexity: 10.203310012817383
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/157

 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 157/200 [12:45:32<3:39:36, 306.42s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14776.795309399802
INFO:root:current train perplexity4.31153678894043
INFO:root:current mean train loss 14856.632285276073
INFO:root:current train perplexity4.326214790344238


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:07<00:00, 247.27s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:07<00:00, 247.28s/it]
INFO:root:final mean train loss: 14846.956031060989
INFO:root:final train perplexity: 4.324897289276123
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.21s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.21s/it]
INFO:root:eval mean loss: 22437.343610491072
INFO:root:eval perplexity: 10.197762489318848
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/158

 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/200 [12:50:57<3:38:20, 311.91s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14930.01953125
INFO:root:current train perplexity4.328251838684082
INFO:root:current mean train loss 14856.70289572011
INFO:root:current train perplexity4.328488826751709
INFO:root:current mean train loss 14854.071825036337
INFO:root:current train perplexity4.323398113250732


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:04<00:00, 244.60s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:04<00:00, 244.60s/it]
INFO:root:final mean train loss: 14839.620392830142
INFO:root:final train perplexity: 4.321769714355469
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.85s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.85s/it]
INFO:root:eval mean loss: 22447.37000093006
INFO:root:eval perplexity: 10.208349227905273
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/159

 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 159/200 [12:55:49<3:29:03, 305.93s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14819.807325676305
INFO:root:current train perplexity4.306726932525635
INFO:root:current mean train loss 14827.275016373504
INFO:root:current train perplexity4.313520908355713


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:05<00:00, 245.18s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:05<00:00, 245.18s/it]
INFO:root:final mean train loss: 14838.679514238911
INFO:root:final train perplexity: 4.32136869430542
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.47s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.47s/it]
INFO:root:eval mean loss: 22436.228864397322
INFO:root:eval perplexity: 10.196588516235352
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/160

 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 160/200 [13:00:41<3:21:11, 301.79s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14824.117393092105
INFO:root:current train perplexity4.302722930908203
INFO:root:current mean train loss 14848.655650932247
INFO:root:current train perplexity4.317978858947754
INFO:root:current mean train loss 14832.374335580766
INFO:root:current train perplexity4.318572521209717


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.38s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.38s/it]
INFO:root:final mean train loss: 14830.393743699597
INFO:root:final train perplexity: 4.317838191986084
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.25s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.25s/it]
INFO:root:eval mean loss: 22444.20091610863
INFO:root:eval perplexity: 10.20500373840332
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/161

 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/200 [13:05:22<3:12:01, 295.42s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14836.3955078125
INFO:root:current train perplexity4.304818153381348
INFO:root:current mean train loss 14835.322482638889
INFO:root:current train perplexity4.310817241668701


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.55s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.55s/it]
INFO:root:final mean train loss: 14829.298532793598
INFO:root:final train perplexity: 4.317371368408203
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.82s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.82s/it]
INFO:root:eval mean loss: 22434.869187127977
INFO:root:eval perplexity: 10.195151329040527
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/162

 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 162/200 [13:10:23<3:08:10, 297.13s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14807.2333984375
INFO:root:current train perplexity4.320561408996582
INFO:root:current mean train loss 14866.211223323171
INFO:root:current train perplexity4.317588806152344
INFO:root:current mean train loss 14829.994171279428
INFO:root:current train perplexity4.310558795928955


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:57<00:00, 237.76s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:57<00:00, 237.76s/it]
INFO:root:final mean train loss: 14818.898327242943
INFO:root:final train perplexity: 4.312945365905762
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.30s/it]
INFO:root:eval mean loss: 22439.547340029763
INFO:root:eval perplexity: 10.200087547302246
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/163

 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/200 [13:15:40<3:06:58, 303.21s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14801.082278645834
INFO:root:current train perplexity4.320099830627441
INFO:root:current mean train loss 14806.077566964286
INFO:root:current train perplexity4.30898380279541


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:15<00:00, 255.89s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:15<00:00, 255.89s/it]
INFO:root:final mean train loss: 14814.281029485886
INFO:root:final train perplexity: 4.3109822273254395
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.76s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.76s/it]
INFO:root:eval mean loss: 22445.185570126487
INFO:root:eval perplexity: 10.206042289733887
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/164

 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/200 [13:21:12<3:07:01, 311.70s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14857.39503761574
INFO:root:current train perplexity4.304381847381592
INFO:root:current mean train loss 14851.729038508858
INFO:root:current train perplexity4.324580192565918
INFO:root:current mean train loss 14828.544409932543
INFO:root:current train perplexity4.312524318695068


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:11<00:00, 251.75s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:11<00:00, 251.75s/it]
INFO:root:final mean train loss: 14811.67423765121
INFO:root:final train perplexity: 4.309873104095459
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.10s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.10s/it]
INFO:root:eval mean loss: 22441.422340029763
INFO:root:eval perplexity: 10.202069282531738
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/165

 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 165/200 [13:26:13<3:00:01, 308.62s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14834.748355913765
INFO:root:current train perplexity4.3126606941223145
INFO:root:current mean train loss 14831.980261435056
INFO:root:current train perplexity4.3110127449035645


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:14<00:00, 254.60s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:14<00:00, 254.60s/it]
INFO:root:final mean train loss: 14804.633308656754
INFO:root:final train perplexity: 4.306880474090576
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.95s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.95s/it]
INFO:root:eval mean loss: 22451.697567894345
INFO:root:eval perplexity: 10.212921142578125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/166

 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/200 [13:31:21<2:54:44, 308.37s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14820.397681451614
INFO:root:current train perplexity4.301044940948486
INFO:root:current mean train loss 14828.865987297233
INFO:root:current train perplexity4.303595066070557
INFO:root:current mean train loss 14816.143804112555
INFO:root:current train perplexity4.306657314300537


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:12<00:00, 252.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:12<00:00, 252.61s/it]
INFO:root:final mean train loss: 14801.937858335434
INFO:root:final train perplexity: 4.305736541748047
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.34s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.34s/it]
INFO:root:eval mean loss: 22449.25069754464
INFO:root:eval perplexity: 10.21034049987793
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/167

 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 167/200 [13:36:15<2:47:12, 304.02s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14794.416780402862
INFO:root:current train perplexity4.2982282638549805
INFO:root:current mean train loss 14818.971610314207
INFO:root:current train perplexity4.3030524253845215


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:01<00:00, 241.54s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:01<00:00, 241.54s/it]
INFO:root:final mean train loss: 14798.011360414566
INFO:root:final train perplexity: 4.3040690422058105
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.45s/it]
INFO:root:eval mean loss: 22446.869140625
INFO:root:eval perplexity: 10.207820892333984
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/168

 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 168/200 [13:41:04<2:39:43, 299.47s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14936.701116071428
INFO:root:current train perplexity4.335011959075928
INFO:root:current mean train loss 14791.42820457176
INFO:root:current train perplexity4.2966413497924805
INFO:root:current mean train loss 14815.679276097075
INFO:root:current train perplexity4.30556058883667


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:06<00:00, 246.17s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:06<00:00, 246.17s/it]
INFO:root:final mean train loss: 14797.323903729839
INFO:root:final train perplexity: 4.30377721786499
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.57s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.57s/it]
INFO:root:eval mean loss: 22447.335867745536
INFO:root:eval perplexity: 10.20831298828125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/169

 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 169/200 [13:46:15<2:36:34, 303.06s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14787.65671021911
INFO:root:current train perplexity4.300804138183594
INFO:root:current mean train loss 14784.84652301972
INFO:root:current train perplexity4.29757833480835


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:14<00:00, 254.18s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:14<00:00, 254.19s/it]
INFO:root:final mean train loss: 14784.576187626008
INFO:root:final train perplexity: 4.298369884490967
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.98s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.98s/it]
INFO:root:eval mean loss: 22457.35535249256
INFO:root:eval perplexity: 10.218905448913574
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/170

 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/200 [13:51:24<2:32:22, 304.75s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14710.37805488782
INFO:root:current train perplexity4.26784610748291
INFO:root:current mean train loss 14765.465237185252
INFO:root:current train perplexity4.297340393066406
INFO:root:current mean train loss 14794.896733623169
INFO:root:current train perplexity4.3006134033203125


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:04<00:00, 244.57s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:04<00:00, 244.57s/it]
INFO:root:final mean train loss: 14785.869664346019
INFO:root:final train perplexity: 4.298917770385742
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.60s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.60s/it]
INFO:root:eval mean loss: 22449.67322358631
INFO:root:eval perplexity: 10.210785865783691
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/171

 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/200 [13:56:31<2:27:40, 305.55s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14825.244988410028
INFO:root:current train perplexity4.3148064613342285
INFO:root:current mean train loss 14779.097610233966
INFO:root:current train perplexity4.2966156005859375


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:08<00:00, 248.37s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:08<00:00, 248.37s/it]
INFO:root:final mean train loss: 14780.141605500252
INFO:root:final train perplexity: 4.296489715576172
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.58s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.58s/it]
INFO:root:eval mean loss: 22451.009788876487
INFO:root:eval perplexity: 10.21219539642334
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/172

 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 172/200 [14:01:22<2:20:32, 301.15s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14736.042173873546
INFO:root:current train perplexity4.284771919250488
INFO:root:current mean train loss 14796.174176409528
INFO:root:current train perplexity4.297094345092773
INFO:root:current mean train loss 14787.479106385032
INFO:root:current train perplexity4.293465614318848


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:03<00:00, 243.69s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:03<00:00, 243.69s/it]
INFO:root:final mean train loss: 14773.084031628025
INFO:root:final train perplexity: 4.29349946975708
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.93s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.93s/it]
INFO:root:eval mean loss: 22456.627464657737
INFO:root:eval perplexity: 10.218135833740234
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/173

 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 173/200 [14:06:46<2:18:34, 307.94s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14799.085413240131
INFO:root:current train perplexity4.291920185089111
INFO:root:current mean train loss 14792.604832732372
INFO:root:current train perplexity4.293360710144043


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:00<00:00, 240.89s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:00<00:00, 240.89s/it]
INFO:root:final mean train loss: 14773.80146641885
INFO:root:final train perplexity: 4.293803691864014
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.99s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.99s/it]
INFO:root:eval mean loss: 22457.233491443454
INFO:root:eval perplexity: 10.21877670288086
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/174

 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 174/200 [14:11:33<2:10:46, 301.81s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14697.311606549201
INFO:root:current train perplexity4.2696614265441895
INFO:root:current mean train loss 14761.423117293793
INFO:root:current train perplexity4.28266716003418
INFO:root:current mean train loss 14780.849680541498
INFO:root:current train perplexity4.292106628417969


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:05<00:00, 245.52s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:05<00:00, 245.52s/it]
INFO:root:final mean train loss: 14768.825211063508
INFO:root:final train perplexity: 4.291697025299072
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.00s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.00s/it]
INFO:root:eval mean loss: 22451.245210193454
INFO:root:eval perplexity: 10.212444305419922
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/175

 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/200 [14:16:38<2:06:07, 302.70s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14752.39482717803
INFO:root:current train perplexity4.280774116516113
INFO:root:current mean train loss 14763.703021945666
INFO:root:current train perplexity4.284866809844971


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.16s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.16s/it]
INFO:root:final mean train loss: 14762.379780430947
INFO:root:final train perplexity: 4.28896951675415
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.78s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.78s/it]
INFO:root:eval mean loss: 22453.908714657737
INFO:root:eval perplexity: 10.215258598327637
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/176

 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 176/200 [14:21:55<2:02:44, 306.86s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14778.608685661764
INFO:root:current train perplexity4.276774883270264
INFO:root:current mean train loss 14759.652990480132
INFO:root:current train perplexity4.280677318572998


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:06<00:00, 246.89s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:06<00:00, 246.89s/it]
INFO:root:final mean train loss: 14758.509222215222
INFO:root:final train perplexity: 4.287332534790039
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.65s/it]
INFO:root:eval mean loss: 22457.692731584822
INFO:root:eval perplexity: 10.219259262084961
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/177

 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 177/200 [14:26:43<1:55:30, 301.34s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14730.0615234375
INFO:root:current train perplexity4.2469587326049805
INFO:root:current mean train loss 14759.867860664443
INFO:root:current train perplexity4.2871856689453125
INFO:root:current mean train loss 14784.679538369766
INFO:root:current train perplexity4.28927755355835


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:02<00:00, 242.35s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:02<00:00, 242.36s/it]
INFO:root:final mean train loss: 14762.829050371723
INFO:root:final train perplexity: 4.289159774780273
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.26s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.26s/it]
INFO:root:eval mean loss: 22460.540062313987
INFO:root:eval perplexity: 10.222275733947754
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/178

 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/200 [14:31:27<1:48:31, 295.99s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14808.923064630682
INFO:root:current train perplexity4.2756123542785645
INFO:root:current mean train loss 14792.835099546372
INFO:root:current train perplexity4.288312911987305


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.95s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.95s/it]
INFO:root:final mean train loss: 14759.07408092868
INFO:root:final train perplexity: 4.287571907043457
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.61s/it]
INFO:root:eval mean loss: 22461.165876116072
INFO:root:eval perplexity: 10.222935676574707
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/179

 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/200 [14:36:08<1:42:04, 291.65s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14671.052315848214
INFO:root:current train perplexity4.264316558837891
INFO:root:current mean train loss 14730.445677570093
INFO:root:current train perplexity4.2817230224609375
INFO:root:current mean train loss 14759.445609714674
INFO:root:current train perplexity4.282968044281006


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:05<00:00, 245.27s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:05<00:00, 245.27s/it]
INFO:root:final mean train loss: 14750.162786668347
INFO:root:final train perplexity: 4.283804416656494
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.45s/it]
INFO:root:eval mean loss: 22461.34937686012
INFO:root:eval perplexity: 10.223128318786621
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/180

 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/200 [14:40:55<1:36:42, 290.15s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14720.524860963984
INFO:root:current train perplexity4.279454231262207
INFO:root:current mean train loss 14733.047433913129
INFO:root:current train perplexity4.274406433105469


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:00<00:00, 240.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:00<00:00, 240.62s/it]
INFO:root:final mean train loss: 14749.300123645413
INFO:root:final train perplexity: 4.283440113067627
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.79s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.79s/it]
INFO:root:eval mean loss: 22458.102074032737
INFO:root:eval perplexity: 10.219695091247559
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/181

 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 181/200 [14:45:38<1:31:13, 288.09s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14869.564453125
INFO:root:current train perplexity4.278501987457275
INFO:root:current mean train loss 14776.566740568695
INFO:root:current train perplexity4.28633975982666
INFO:root:current mean train loss 14763.045579087679
INFO:root:current train perplexity4.2841033935546875


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 240.00s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:00<00:00, 240.05s/it]
INFO:root:final mean train loss: 14747.122200258316
INFO:root:final train perplexity: 4.282519817352295
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.26s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.27s/it]
INFO:root:eval mean loss: 22466.504441034227
INFO:root:eval perplexity: 10.228585243225098
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/182

 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/200 [14:50:19<1:25:48, 286.02s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14700.435422867064
INFO:root:current train perplexity4.27317476272583
INFO:root:current mean train loss 14737.742846529907
INFO:root:current train perplexity4.273902416229248


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:58<00:00, 238.02s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:58<00:00, 238.02s/it]
INFO:root:final mean train loss: 14745.562586630544
INFO:root:final train perplexity: 4.281860828399658
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.09s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.10s/it]
INFO:root:eval mean loss: 22464.58861142113
INFO:root:eval perplexity: 10.226558685302734
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/183

 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 183/200 [14:54:59<1:20:31, 284.21s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14721.930338541666
INFO:root:current train perplexity4.294884204864502
INFO:root:current mean train loss 14760.79140625
INFO:root:current train perplexity4.286807060241699
INFO:root:current mean train loss 14755.712372819768
INFO:root:current train perplexity4.281510353088379


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.74s/it]
INFO:root:final mean train loss: 14744.561964465725
INFO:root:final train perplexity: 4.281438827514648
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.73s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.73s/it]
INFO:root:eval mean loss: 22463.913225446428
INFO:root:eval perplexity: 10.225844383239746
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/184

 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 184/200 [14:59:41<1:15:33, 283.35s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14765.526338036381
INFO:root:current train perplexity4.2699785232543945
INFO:root:current mean train loss 14746.586381923653
INFO:root:current train perplexity4.274982452392578


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:55<00:00, 235.85s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:55<00:00, 235.86s/it]
INFO:root:final mean train loss: 14740.969612367691
INFO:root:final train perplexity: 4.279922008514404
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.69s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.69s/it]
INFO:root:eval mean loss: 22460.834774925595
INFO:root:eval perplexity: 10.22258472442627
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/185

 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 185/200 [15:04:18<1:10:23, 281.56s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14664.721217105263
INFO:root:current train perplexity4.251041412353516
INFO:root:current mean train loss 14727.02855829832
INFO:root:current train perplexity4.276126384735107
INFO:root:current mean train loss 14758.235485338186
INFO:root:current train perplexity4.280849456787109


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:55<00:00, 235.88s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:55<00:00, 235.90s/it]
INFO:root:final mean train loss: 14735.719112273186
INFO:root:final train perplexity: 4.277705192565918
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.24s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.24s/it]
INFO:root:eval mean loss: 22459.87109375
INFO:root:eval perplexity: 10.221566200256348
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/186

 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 186/200 [15:08:53<1:05:14, 279.59s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14756.384545554578
INFO:root:current train perplexity4.274538040161133
INFO:root:current mean train loss 14762.101065652412
INFO:root:current train perplexity4.2772016525268555


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:56<00:00, 236.85s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:56<00:00, 236.85s/it]
INFO:root:final mean train loss: 14729.373483965473
INFO:root:final train perplexity: 4.275029182434082
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.14s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.14s/it]
INFO:root:eval mean loss: 22467.757905505954
INFO:root:eval perplexity: 10.229911804199219
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/187

 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 187/200 [15:13:31<1:00:27, 279.07s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14787.190302309782
INFO:root:current train perplexity4.264259338378906
INFO:root:current mean train loss 14721.663514672256
INFO:root:current train perplexity4.268558979034424
INFO:root:current mean train loss 14745.113688515976
INFO:root:current train perplexity4.2768940925598145


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:12<00:00, 252.41s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:12<00:00, 252.41s/it]
INFO:root:final mean train loss: 14730.848766696068
INFO:root:final train perplexity: 4.275650978088379
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.00s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.00s/it]
INFO:root:eval mean loss: 22465.97293526786
INFO:root:eval perplexity: 10.22802448272705
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/188

 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/200 [15:18:24<56:40, 283.35s/it]  

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14744.353151041667
INFO:root:current train perplexity4.275452613830566
INFO:root:current mean train loss 14719.943052455357
INFO:root:current train perplexity4.270040988922119


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.11s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.11s/it]
INFO:root:final mean train loss: 14730.035022366432
INFO:root:final train perplexity: 4.275308609008789
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.53s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.53s/it]
INFO:root:eval mean loss: 22464.89574032738
INFO:root:eval perplexity: 10.226885795593262
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/189

 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 189/200 [15:23:05<51:47, 282.50s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14676.111183449075
INFO:root:current train perplexity4.259212493896484
INFO:root:current mean train loss 14711.911317359743
INFO:root:current train perplexity4.271628379821777
INFO:root:current mean train loss 14730.672946207324
INFO:root:current train perplexity4.272523880004883


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:07<00:00, 247.28s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:07<00:00, 247.28s/it]
INFO:root:final mean train loss: 14728.868947675152
INFO:root:final train perplexity: 4.27481746673584
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.40s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.40s/it]
INFO:root:eval mean loss: 22464.72570219494
INFO:root:eval perplexity: 10.226704597473145
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/190

 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/200 [15:27:53<47:23, 284.30s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14790.728762856013
INFO:root:current train perplexity4.278372764587402
INFO:root:current mean train loss 14738.186223376397
INFO:root:current train perplexity4.273738861083984


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:05<00:00, 245.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:05<00:00, 245.62s/it]
INFO:root:final mean train loss: 14732.029285061744
INFO:root:final train perplexity: 4.276149749755859
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.63s/it]
INFO:root:eval mean loss: 22464.263764880954
INFO:root:eval perplexity: 10.226215362548828
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/191

 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/200 [15:32:40<42:46, 285.15s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14831.711756552419
INFO:root:current train perplexity4.276395797729492
INFO:root:current mean train loss 14772.767227755248
INFO:root:current train perplexity4.275906085968018
INFO:root:current mean train loss 14729.231386126894
INFO:root:current train perplexity4.2713236808776855


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.77s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:59<00:00, 239.77s/it]
INFO:root:final mean train loss: 14723.76671969506
INFO:root:final train perplexity: 4.272665977478027
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.36s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.36s/it]
INFO:root:eval mean loss: 22467.207449776786
INFO:root:eval perplexity: 10.229331016540527
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/192

 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/200 [15:37:19<37:46, 283.32s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14720.569324171687
INFO:root:current train perplexity4.272497177124023
INFO:root:current mean train loss 14722.22661586407
INFO:root:current train perplexity4.270590782165527


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:03<00:00, 243.73s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:03<00:00, 243.73s/it]
INFO:root:final mean train loss: 14723.63099719632
INFO:root:final train perplexity: 4.272608757019043
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.83s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.83s/it]
INFO:root:eval mean loss: 22466.812034970237
INFO:root:eval perplexity: 10.228911399841309
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/193

 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193/200 [15:42:06<33:09, 284.27s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14729.381612723215
INFO:root:current train perplexity4.292401313781738
INFO:root:current mean train loss 14735.310337094907
INFO:root:current train perplexity4.271964073181152
INFO:root:current mean train loss 14740.346293218085
INFO:root:current train perplexity4.272242069244385


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:16<00:00, 256.38s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:16<00:00, 256.39s/it]
INFO:root:final mean train loss: 14724.724440051663
INFO:root:final train perplexity: 4.273069381713867
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.60s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.60s/it]
INFO:root:eval mean loss: 22463.27062406994
INFO:root:eval perplexity: 10.225164413452148
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/194

 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/200 [15:47:06<28:53, 288.98s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14689.163456357759
INFO:root:current train perplexity4.2646002769470215
INFO:root:current mean train loss 14720.301486255014
INFO:root:current train perplexity4.2703166007995605


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:09<00:00, 249.41s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:09<00:00, 249.41s/it]
INFO:root:final mean train loss: 14719.655954668598
INFO:root:final train perplexity: 4.270934581756592
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.94s/it]
INFO:root:eval mean loss: 22464.489839099704
INFO:root:eval perplexity: 10.22645378112793
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/195

 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 195/200 [15:52:21<24:43, 296.69s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14792.28986378205
INFO:root:current train perplexity4.287353038787842
INFO:root:current mean train loss 14724.673722740557
INFO:root:current train perplexity4.270058631896973
INFO:root:current mean train loss 14726.528667625524
INFO:root:current train perplexity4.269096851348877


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:10<00:00, 250.55s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:10<00:00, 250.55s/it]
INFO:root:final mean train loss: 14715.292823053176
INFO:root:final train perplexity: 4.269096374511719
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.13s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.13s/it]
INFO:root:eval mean loss: 22469.030133928572
INFO:root:eval perplexity: 10.231260299682617
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/196

 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 196/200 [15:57:13<19:41, 295.47s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14757.538976648351
INFO:root:current train perplexity4.275832653045654
INFO:root:current mean train loss 14744.45913776178
INFO:root:current train perplexity4.275894641876221


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:05<00:00, 245.78s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:05<00:00, 245.78s/it]
INFO:root:final mean train loss: 14719.093856319305
INFO:root:final train perplexity: 4.270696640014648
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.17s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.17s/it]
INFO:root:eval mean loss: 22465.964657738095
INFO:root:eval perplexity: 10.22801399230957
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/197

 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 197/200 [16:02:00<14:38, 292.89s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14722.009720203489
INFO:root:current train perplexity4.261992931365967
INFO:root:current mean train loss 14733.393131282779
INFO:root:current train perplexity4.266876697540283
INFO:root:current mean train loss 14734.02541071888
INFO:root:current train perplexity4.272119998931885


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:08<00:00, 248.48s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:08<00:00, 248.48s/it]
INFO:root:final mean train loss: 14722.010533486644
INFO:root:final train perplexity: 4.271925926208496
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.59s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.59s/it]
INFO:root:eval mean loss: 22466.45691499256
INFO:root:eval perplexity: 10.228533744812012
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/198

 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 198/200 [16:06:51<09:44, 292.33s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14762.045816200658
INFO:root:current train perplexity4.285097599029541
INFO:root:current mean train loss 14741.443314302885
INFO:root:current train perplexity4.2749738693237305


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:07<00:00, 247.16s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:07<00:00, 247.16s/it]
INFO:root:final mean train loss: 14722.601282919606
INFO:root:final train perplexity: 4.272174835205078
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.72s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.72s/it]
INFO:root:eval mean loss: 22466.241024925595
INFO:root:eval perplexity: 10.228306770324707
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/199

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/200 [16:11:41<04:51, 291.55s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14716.013796542553
INFO:root:current train perplexity4.264784336090088
INFO:root:current mean train loss 14719.548715189201
INFO:root:current train perplexity4.270339488983154
INFO:root:current mean train loss 14730.76076986336
INFO:root:current train perplexity4.270843982696533


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:05<00:00, 245.60s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:05<00:00, 245.60s/it]
INFO:root:final mean train loss: 14718.968647618447
INFO:root:final train perplexity: 4.270644187927246
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.78s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.78s/it]
INFO:root:eval mean loss: 22466.448056175595
INFO:root:eval perplexity: 10.22852611541748
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_2/200

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [16:16:57<00:00, 298.99s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [16:16:57<00:00, 293.09s/it]
INFO:root:evaluating final model
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.72s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.72s/it]
INFO:root:eval mean loss: 22466.448056175595
INFO:root:eval perplexity: 10.22852611541748
INFO:root:evalaution complete
INFO:root:save model final: small_topk_2/final
