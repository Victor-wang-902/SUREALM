Downloading:   0%|          | 0.00/385 [00:00<?, ?B/s]Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 385/385 [00:00<00:00, 791kB/s]
INFO:root:Output: minilm_l12_baseline
INFO:root:Steps per epochs:1983
INFO:root:Total steps:396600
Downloading:   0%|          | 0.00/2.00 [00:00<?, ?B/s]Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.00/2.00 [00:00<00:00, 4.32kB/s]
Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232k/232k [00:00<00:00, 2.66MB/s]
Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:00<00:00, 243kB/s]
Downloading:   0%|          | 0.00/133M [00:00<?, ?B/s]Downloading:   5%|â–Œ         | 7.02M/133M [00:00<00:02, 55.1MB/s]Downloading:   9%|â–‰         | 12.5M/133M [00:00<00:03, 39.9MB/s]Downloading:  13%|â–ˆâ–Ž        | 16.8M/133M [00:00<00:03, 33.4MB/s]Downloading:  18%|â–ˆâ–Š        | 23.4M/133M [00:00<00:03, 33.8MB/s]Downloading:  20%|â–ˆâ–ˆ        | 26.8M/133M [00:00<00:03, 28.5MB/s]Downloading:  24%|â–ˆâ–ˆâ–       | 31.8M/133M [00:00<00:03, 31.9MB/s]Downloading:  26%|â–ˆâ–ˆâ–‹       | 35.1M/133M [00:01<00:03, 31.3MB/s]Downloading:  30%|â–ˆâ–ˆâ–ˆ       | 40.2M/133M [00:01<00:02, 31.4MB/s]Downloading:  32%|â–ˆâ–ˆâ–ˆâ–      | 43.4M/133M [00:01<00:02, 30.6MB/s]Downloading:  35%|â–ˆâ–ˆâ–ˆâ–      | 46.5M/133M [00:01<00:02, 30.0MB/s]Downloading:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 50.3M/133M [00:01<00:02, 29.6MB/s]Downloading:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 57.3M/133M [00:01<00:02, 36.6MB/s]Downloading:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 63.4M/133M [00:01<00:01, 42.5MB/s]Downloading:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 67.7M/133M [00:02<00:01, 34.7MB/s]Downloading:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 75.5M/133M [00:02<00:01, 34.5MB/s]Downloading:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 82.5M/133M [00:02<00:01, 39.7MB/s]Downloading:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 86.7M/133M [00:02<00:01, 40.2MB/s]Downloading:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 92.3M/133M [00:02<00:01, 34.1MB/s]Downloading:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 99.3M/133M [00:02<00:00, 36.7MB/s]Downloading:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 103M/133M [00:03<00:00, 32.7MB/s] Downloading:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 107M/133M [00:03<00:00, 33.6MB/s]Downloading:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 111M/133M [00:03<00:00, 33.5MB/s]Downloading:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 117M/133M [00:03<00:00, 36.1MB/s]Downloading:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 126M/133M [00:03<00:00, 44.5MB/s]Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133M/133M [00:03<00:00, 37.4MB/s]
Some weights of RetrievalGenerationModel were not initialized from the model checkpoint at microsoft/MiniLM-L12-H384-uncased and are newly initialized: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:root:started training
  0%|          | 0/200 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][AAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
INFO:root:current mean train loss 12049.046401515152
INFO:root:current train perplexity12383.2841796875
INFO:root:current mean train loss 9847.171151165987
INFO:root:current train perplexity2233.9111328125
INFO:root:current mean train loss 8576.499103456836
INFO:root:current train perplexity831.100830078125
INFO:root:current mean train loss 7724.514869938518
INFO:root:current train perplexity428.41278076171875
INFO:root:current mean train loss 7105.828206706382
INFO:root:current train perplexity265.4502868652344
INFO:root:current mean train loss 6638.6400743589575
INFO:root:current train perplexity183.91702270507812
INFO:root:current mean train loss 6259.392519098154
INFO:root:current train perplexity137.7099151611328
INFO:root:current mean train loss 5957.316371721977
INFO:root:current train perplexity108.96551513671875
INFO:root:current mean train loss 5709.395145810884
INFO:root:current train perplexity89.6087417602539
INFO:root:current mean train loss 5500.873250936484
INFO:root:current train perplexity75.9212875366211
INFO:root:current mean train loss 5320.438983282032
INFO:root:current train perplexity65.86453247070312
INFO:root:current mean train loss 5162.358372577734
INFO:root:current train perplexity58.24443817138672
INFO:root:current mean train loss 5022.177660136704
INFO:root:current train perplexity52.249691009521484
INFO:root:current mean train loss 4900.2467738186315
INFO:root:current train perplexity47.4742317199707
INFO:root:current mean train loss 4790.338036392752
INFO:root:current train perplexity43.59341812133789
INFO:root:current mean train loss 4689.687471295536
INFO:root:current train perplexity40.32822036743164
INFO:root:current mean train loss 4599.911951164978
INFO:root:current train perplexity37.591773986816406
INFO:root:current mean train loss 4518.7998041446635
INFO:root:current train perplexity35.21403503417969
INFO:root:current mean train loss 4444.393371357047
INFO:root:current train perplexity33.225704193115234

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.50s/it]
INFO:root:final mean train loss: 4384.292615466327
INFO:root:final train perplexity: 31.744426727294922
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.34s/it]
INFO:root:eval mean loss: 2861.3035386538677
INFO:root:eval perplexity: 10.115351676940918
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/1
  0%|          | 1/200 [03:20<11:03:54, 200.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3010.971725463867
INFO:root:current train perplexity10.86597728729248
INFO:root:current mean train loss 3003.3440257105335
INFO:root:current train perplexity10.833869934082031
INFO:root:current mean train loss 2991.8599887424043
INFO:root:current train perplexity10.729636192321777
INFO:root:current mean train loss 2982.4729220233385
INFO:root:current train perplexity10.624914169311523
INFO:root:current mean train loss 2972.1745634812573
INFO:root:current train perplexity10.487311363220215
INFO:root:current mean train loss 2966.7250129640565
INFO:root:current train perplexity10.381084442138672
INFO:root:current mean train loss 2953.210200322139
INFO:root:current train perplexity10.286746978759766
INFO:root:current mean train loss 2937.322391787055
INFO:root:current train perplexity10.187080383300781
INFO:root:current mean train loss 2930.894935757506
INFO:root:current train perplexity10.112344741821289
INFO:root:current mean train loss 2920.1639932024427
INFO:root:current train perplexity10.02518081665039
INFO:root:current mean train loss 2911.979464072881
INFO:root:current train perplexity9.940073013305664
INFO:root:current mean train loss 2902.0907734970037
INFO:root:current train perplexity9.859946250915527
INFO:root:current mean train loss 2895.7151945013748
INFO:root:current train perplexity9.792807579040527
INFO:root:current mean train loss 2886.371275927883
INFO:root:current train perplexity9.724205017089844
INFO:root:current mean train loss 2878.6306307517875
INFO:root:current train perplexity9.675141334533691
INFO:root:current mean train loss 2871.153030274726
INFO:root:current train perplexity9.615877151489258
INFO:root:current mean train loss 2862.1181914074587
INFO:root:current train perplexity9.546133041381836
INFO:root:current mean train loss 2854.0561755342637
INFO:root:current train perplexity9.487605094909668
INFO:root:current mean train loss 2846.5583212428155
INFO:root:current train perplexity9.425808906555176
INFO:root:current mean train loss 2839.437570591809
INFO:root:current train perplexity9.381217002868652

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.12s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.12s/it]
INFO:root:final mean train loss: 2833.6803054848046
INFO:root:final train perplexity: 9.344714164733887
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2541.3163564695533
INFO:root:eval perplexity: 7.808927059173584
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/2
  1%|          | 2/200 [06:38<10:57:45, 199.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2670.2524857954545
INFO:root:current train perplexity8.289801597595215
INFO:root:current mean train loss 2693.990478515625
INFO:root:current train perplexity8.264031410217285
INFO:root:current mean train loss 2678.237415755767
INFO:root:current train perplexity8.18982982635498
INFO:root:current mean train loss 2665.6258160015486
INFO:root:current train perplexity8.105957984924316
INFO:root:current mean train loss 2657.8936234753896
INFO:root:current train perplexity8.083702087402344
INFO:root:current mean train loss 2652.406463909328
INFO:root:current train perplexity8.063117980957031
INFO:root:current mean train loss 2649.2572493952407
INFO:root:current train perplexity8.042919158935547
INFO:root:current mean train loss 2642.5994135295873
INFO:root:current train perplexity8.013081550598145
INFO:root:current mean train loss 2639.2319209910524
INFO:root:current train perplexity7.999269485473633
INFO:root:current mean train loss 2639.2319307153502
INFO:root:current train perplexity7.97977352142334
INFO:root:current mean train loss 2635.968874551897
INFO:root:current train perplexity7.9674811363220215
INFO:root:current mean train loss 2632.166943488664
INFO:root:current train perplexity7.951982498168945
INFO:root:current mean train loss 2627.3274650480917
INFO:root:current train perplexity7.925268650054932
INFO:root:current mean train loss 2622.855250250551
INFO:root:current train perplexity7.906455993652344
INFO:root:current mean train loss 2619.2501921776866
INFO:root:current train perplexity7.889277458190918
INFO:root:current mean train loss 2616.089580498563
INFO:root:current train perplexity7.867563724517822
INFO:root:current mean train loss 2613.251507153485
INFO:root:current train perplexity7.843268871307373
INFO:root:current mean train loss 2609.5262320860234
INFO:root:current train perplexity7.816433906555176
INFO:root:current mean train loss 2604.4253814081253
INFO:root:current train perplexity7.793123245239258
INFO:root:current mean train loss 2601.2484146647053
INFO:root:current train perplexity7.774813652038574

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.22s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.22s/it]
INFO:root:final mean train loss: 2598.844935122368
INFO:root:final train perplexity: 7.764836311340332
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2412.833587862921
INFO:root:eval perplexity: 7.038239479064941
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/3
  2%|â–         | 3/200 [09:57<10:53:39, 199.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2493.163972167969
INFO:root:current train perplexity7.166497230529785
INFO:root:current mean train loss 2515.174260253906
INFO:root:current train perplexity7.267324447631836
INFO:root:current mean train loss 2508.6414301757814
INFO:root:current train perplexity7.242365837097168
INFO:root:current mean train loss 2517.909051688058
INFO:root:current train perplexity7.2454142570495605
INFO:root:current mean train loss 2509.8012898763022
INFO:root:current train perplexity7.226974010467529
INFO:root:current mean train loss 2504.0634110884234
INFO:root:current train perplexity7.202277660369873
INFO:root:current mean train loss 2503.9891956505408
INFO:root:current train perplexity7.193202018737793
INFO:root:current mean train loss 2501.4425275065105
INFO:root:current train perplexity7.1900529861450195
INFO:root:current mean train loss 2500.361048224954
INFO:root:current train perplexity7.17730712890625
INFO:root:current mean train loss 2499.582796695107
INFO:root:current train perplexity7.177075386047363
INFO:root:current mean train loss 2496.951492629278
INFO:root:current train perplexity7.169205665588379
INFO:root:current mean train loss 2495.1003511379076
INFO:root:current train perplexity7.154445171356201
INFO:root:current mean train loss 2494.631474316406
INFO:root:current train perplexity7.148250102996826
INFO:root:current mean train loss 2494.3001098632812
INFO:root:current train perplexity7.135692119598389
INFO:root:current mean train loss 2492.176917261584
INFO:root:current train perplexity7.125737190246582
INFO:root:current mean train loss 2487.7990011498237
INFO:root:current train perplexity7.108165740966797
INFO:root:current mean train loss 2484.517907862808
INFO:root:current train perplexity7.084214210510254
INFO:root:current mean train loss 2482.853084751674
INFO:root:current train perplexity7.076754093170166
INFO:root:current mean train loss 2480.9359977433487
INFO:root:current train perplexity7.071220397949219
INFO:root:current mean train loss 2479.1565912334736
INFO:root:current train perplexity7.06003999710083

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.34s/it]
INFO:root:final mean train loss: 2477.709250799767
INFO:root:final train perplexity: 7.057356834411621
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2336.8328385589816
INFO:root:eval perplexity: 6.618660926818848
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/4
  2%|â–         | 4/200 [13:16<10:50:07, 199.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2437.3094591738572
INFO:root:current train perplexity6.835146427154541
INFO:root:current mean train loss 2426.558479720247
INFO:root:current train perplexity6.769119739532471
INFO:root:current mean train loss 2421.9435449950256
INFO:root:current train perplexity6.73708438873291
INFO:root:current mean train loss 2424.998287356842
INFO:root:current train perplexity6.745074272155762
INFO:root:current mean train loss 2416.9904061098937
INFO:root:current train perplexity6.728027820587158
INFO:root:current mean train loss 2415.6199714351164
INFO:root:current train perplexity6.709183216094971
INFO:root:current mean train loss 2412.558977530278
INFO:root:current train perplexity6.7007060050964355
INFO:root:current mean train loss 2410.304364101206
INFO:root:current train perplexity6.69522762298584
INFO:root:current mean train loss 2409.409821999802
INFO:root:current train perplexity6.696597099304199
INFO:root:current mean train loss 2409.205583448124
INFO:root:current train perplexity6.695136547088623
INFO:root:current mean train loss 2409.2996620242575
INFO:root:current train perplexity6.692209720611572
INFO:root:current mean train loss 2407.9430879333026
INFO:root:current train perplexity6.683163642883301
INFO:root:current mean train loss 2406.1610301077226
INFO:root:current train perplexity6.671844005584717
INFO:root:current mean train loss 2405.854120975877
INFO:root:current train perplexity6.666590690612793
INFO:root:current mean train loss 2405.134539374707
INFO:root:current train perplexity6.661542892456055
INFO:root:current mean train loss 2401.9563134734467
INFO:root:current train perplexity6.651975631713867
INFO:root:current mean train loss 2399.034636187997
INFO:root:current train perplexity6.640515327453613
INFO:root:current mean train loss 2398.977138724378
INFO:root:current train perplexity6.637448310852051
INFO:root:current mean train loss 2398.582169862246
INFO:root:current train perplexity6.63330602645874
INFO:root:current mean train loss 2398.5050218249553
INFO:root:current train perplexity6.630080699920654

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.25s/it]
INFO:root:final mean train loss: 2397.9177916464755
INFO:root:final train perplexity: 6.626934051513672
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it]
INFO:root:eval mean loss: 2285.9545556467474
INFO:root:eval perplexity: 6.351849555969238
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/5
  2%|â–Ž         | 5/200 [16:35<10:46:33, 198.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2349.335518973214
INFO:root:current train perplexity6.275898456573486
INFO:root:current mean train loss 2346.7043669327445
INFO:root:current train perplexity6.368297100067139
INFO:root:current mean train loss 2344.690885302047
INFO:root:current train perplexity6.352921009063721
INFO:root:current mean train loss 2342.6955591837564
INFO:root:current train perplexity6.354613304138184
INFO:root:current mean train loss 2347.38529022469
INFO:root:current train perplexity6.369734287261963
INFO:root:current mean train loss 2348.5027804440015
INFO:root:current train perplexity6.380476951599121
INFO:root:current mean train loss 2346.7370680424206
INFO:root:current train perplexity6.365281581878662
INFO:root:current mean train loss 2345.6618470172493
INFO:root:current train perplexity6.362903118133545
INFO:root:current mean train loss 2349.6677012724035
INFO:root:current train perplexity6.378552436828613
INFO:root:current mean train loss 2345.1958049991267
INFO:root:current train perplexity6.362438201904297
INFO:root:current mean train loss 2344.9652291048055
INFO:root:current train perplexity6.356172561645508
INFO:root:current mean train loss 2344.8479959642564
INFO:root:current train perplexity6.354216575622559
INFO:root:current mean train loss 2342.7939439815154
INFO:root:current train perplexity6.346848964691162
INFO:root:current mean train loss 2341.914960298924
INFO:root:current train perplexity6.343208312988281
INFO:root:current mean train loss 2342.1178421884215
INFO:root:current train perplexity6.344629287719727
INFO:root:current mean train loss 2342.1774094706834
INFO:root:current train perplexity6.3447442054748535
INFO:root:current mean train loss 2343.4034524586873
INFO:root:current train perplexity6.343534469604492
INFO:root:current mean train loss 2341.8072138901784
INFO:root:current train perplexity6.338038921356201
INFO:root:current mean train loss 2341.1466299652293
INFO:root:current train perplexity6.332837104797363

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.29s/it]
INFO:root:final mean train loss: 2340.3837954030155
INFO:root:final train perplexity: 6.332957744598389
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2249.3268056017287
INFO:root:eval perplexity: 6.166450500488281
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/6
  3%|â–Ž         | 6/200 [19:54<10:43:10, 198.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2254.347412109375
INFO:root:current train perplexity5.779723644256592
INFO:root:current mean train loss 2312.1645991259284
INFO:root:current train perplexity6.2030510902404785
INFO:root:current mean train loss 2298.0973878167756
INFO:root:current train perplexity6.145812034606934
INFO:root:current mean train loss 2292.4701430615396
INFO:root:current train perplexity6.1317315101623535
INFO:root:current mean train loss 2294.0367863909564
INFO:root:current train perplexity6.119511604309082
INFO:root:current mean train loss 2293.2930457444486
INFO:root:current train perplexity6.116715431213379
INFO:root:current mean train loss 2297.7103241017576
INFO:root:current train perplexity6.115863800048828
INFO:root:current mean train loss 2299.1119100921674
INFO:root:current train perplexity6.122040748596191
INFO:root:current mean train loss 2297.9964931835693
INFO:root:current train perplexity6.1230645179748535
INFO:root:current mean train loss 2300.0869551138926
INFO:root:current train perplexity6.134683132171631
INFO:root:current mean train loss 2299.8761653385677
INFO:root:current train perplexity6.138955593109131
INFO:root:current mean train loss 2297.0634556076507
INFO:root:current train perplexity6.1363372802734375
INFO:root:current mean train loss 2296.253287970474
INFO:root:current train perplexity6.130409240722656
INFO:root:current mean train loss 2296.0158680146883
INFO:root:current train perplexity6.126014232635498
INFO:root:current mean train loss 2295.3578350494627
INFO:root:current train perplexity6.1204938888549805
INFO:root:current mean train loss 2297.458933383604
INFO:root:current train perplexity6.120418548583984
INFO:root:current mean train loss 2297.0541420340314
INFO:root:current train perplexity6.117642879486084
INFO:root:current mean train loss 2294.6650493247307
INFO:root:current train perplexity6.109729766845703
INFO:root:current mean train loss 2295.3709101361874
INFO:root:current train perplexity6.111142158508301
INFO:root:current mean train loss 2295.2031869020416
INFO:root:current train perplexity6.111623764038086

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.08s/it]
INFO:root:final mean train loss: 2294.7027909225485
INFO:root:final train perplexity: 6.108863830566406
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2222.2190850440493
INFO:root:eval perplexity: 6.03273344039917
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/7
  4%|â–Ž         | 7/200 [23:12<10:39:36, 198.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2230.193359375
INFO:root:current train perplexity5.817732810974121
INFO:root:current mean train loss 2259.965206857455
INFO:root:current train perplexity5.926822185516357
INFO:root:current mean train loss 2265.4464139325905
INFO:root:current train perplexity5.979698181152344
INFO:root:current mean train loss 2267.7258158749755
INFO:root:current train perplexity5.974157810211182
INFO:root:current mean train loss 2266.9233962063586
INFO:root:current train perplexity5.967924118041992
INFO:root:current mean train loss 2268.3752973991013
INFO:root:current train perplexity5.973782539367676
INFO:root:current mean train loss 2265.342377795371
INFO:root:current train perplexity5.960738658905029
INFO:root:current mean train loss 2262.4300747927186
INFO:root:current train perplexity5.95906400680542
INFO:root:current mean train loss 2262.699335448025
INFO:root:current train perplexity5.953709602355957
INFO:root:current mean train loss 2259.039973373247
INFO:root:current train perplexity5.941527366638184
INFO:root:current mean train loss 2259.0970094452205
INFO:root:current train perplexity5.933096885681152
INFO:root:current mean train loss 2257.415113418388
INFO:root:current train perplexity5.934569358825684
INFO:root:current mean train loss 2256.460257995305
INFO:root:current train perplexity5.930235385894775
INFO:root:current mean train loss 2258.5069469862897
INFO:root:current train perplexity5.931366443634033
INFO:root:current mean train loss 2259.2592521204765
INFO:root:current train perplexity5.931838512420654
INFO:root:current mean train loss 2259.2382675794115
INFO:root:current train perplexity5.93496561050415
INFO:root:current mean train loss 2258.8244243381346
INFO:root:current train perplexity5.936970233917236
INFO:root:current mean train loss 2258.8420935243334
INFO:root:current train perplexity5.9365081787109375
INFO:root:current mean train loss 2257.564520136096
INFO:root:current train perplexity5.931407451629639
INFO:root:current mean train loss 2257.3043617670182
INFO:root:current train perplexity5.929513454437256

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.14s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.15s/it]
INFO:root:final mean train loss: 2256.552659489203
INFO:root:final train perplexity: 5.927800178527832
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2199.3596550691213
INFO:root:eval perplexity: 5.922229766845703
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/8
  4%|â–         | 8/200 [26:31<10:36:13, 198.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2198.4029087611607
INFO:root:current train perplexity5.693032264709473
INFO:root:current mean train loss 2232.980181206597
INFO:root:current train perplexity5.772933006286621
INFO:root:current mean train loss 2227.852332841589
INFO:root:current train perplexity5.7811150550842285
INFO:root:current mean train loss 2225.2113182864973
INFO:root:current train perplexity5.780807018280029
INFO:root:current mean train loss 2228.205890243355
INFO:root:current train perplexity5.789562702178955
INFO:root:current mean train loss 2228.6106607312354
INFO:root:current train perplexity5.789865970611572
INFO:root:current mean train loss 2224.8338849886195
INFO:root:current train perplexity5.772374153137207
INFO:root:current mean train loss 2226.654710585406
INFO:root:current train perplexity5.779973983764648
INFO:root:current mean train loss 2225.640870456353
INFO:root:current train perplexity5.78327751159668
INFO:root:current mean train loss 2222.583830579462
INFO:root:current train perplexity5.772426605224609
INFO:root:current mean train loss 2221.9502934405195
INFO:root:current train perplexity5.775426864624023
INFO:root:current mean train loss 2223.207288834492
INFO:root:current train perplexity5.784702777862549
INFO:root:current mean train loss 2222.1490586253794
INFO:root:current train perplexity5.778334140777588
INFO:root:current mean train loss 2223.763878525866
INFO:root:current train perplexity5.780917167663574
INFO:root:current mean train loss 2224.7021448647106
INFO:root:current train perplexity5.7822957038879395
INFO:root:current mean train loss 2225.3827057779417
INFO:root:current train perplexity5.777579307556152
INFO:root:current mean train loss 2224.2515633212683
INFO:root:current train perplexity5.77495813369751
INFO:root:current mean train loss 2225.259292822406
INFO:root:current train perplexity5.776256561279297
INFO:root:current mean train loss 2225.3825614409484
INFO:root:current train perplexity5.777027606964111
INFO:root:current mean train loss 2225.240280868964
INFO:root:current train perplexity5.780747890472412

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.06s/it]
INFO:root:final mean train loss: 2225.1033510793895
INFO:root:final train perplexity: 5.782582759857178
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2181.373708738503
INFO:root:eval perplexity: 5.836708068847656
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/9
  4%|â–         | 9/200 [29:50<10:32:47, 198.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2170.3370314378003
INFO:root:current train perplexity5.682961940765381
INFO:root:current mean train loss 2180.7995693809107
INFO:root:current train perplexity5.647937774658203
INFO:root:current mean train loss 2189.3060554625495
INFO:root:current train perplexity5.636364459991455
INFO:root:current mean train loss 2193.414029901678
INFO:root:current train perplexity5.656444072723389
INFO:root:current mean train loss 2193.8127679065265
INFO:root:current train perplexity5.650737285614014
INFO:root:current mean train loss 2191.441629824431
INFO:root:current train perplexity5.645578384399414
INFO:root:current mean train loss 2195.1705136913465
INFO:root:current train perplexity5.661291122436523
INFO:root:current mean train loss 2196.864365760316
INFO:root:current train perplexity5.657853603363037
INFO:root:current mean train loss 2195.880477403811
INFO:root:current train perplexity5.656959533691406
INFO:root:current mean train loss 2195.885043873506
INFO:root:current train perplexity5.656890869140625
INFO:root:current mean train loss 2196.887663402485
INFO:root:current train perplexity5.6573028564453125
INFO:root:current mean train loss 2197.6154397328696
INFO:root:current train perplexity5.657869338989258
INFO:root:current mean train loss 2195.8936550152566
INFO:root:current train perplexity5.653661251068115
INFO:root:current mean train loss 2194.9585257084414
INFO:root:current train perplexity5.653069972991943
INFO:root:current mean train loss 2195.9456222155864
INFO:root:current train perplexity5.655287265777588
INFO:root:current mean train loss 2196.425175696304
INFO:root:current train perplexity5.65754508972168
INFO:root:current mean train loss 2195.8904283722145
INFO:root:current train perplexity5.6570725440979
INFO:root:current mean train loss 2196.7387277960233
INFO:root:current train perplexity5.65899658203125
INFO:root:current mean train loss 2197.1099259642237
INFO:root:current train perplexity5.658966541290283
INFO:root:current mean train loss 2196.965551282539
INFO:root:current train perplexity5.656667709350586

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.60s/it]
INFO:root:final mean train loss: 2197.505139972727
INFO:root:final train perplexity: 5.658080577850342
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2164.8259467808066
INFO:root:eval perplexity: 5.759116172790527
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/10
  5%|â–Œ         | 10/200 [33:08<10:28:54, 198.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2160.5955739781475
INFO:root:current train perplexity5.5465826988220215
INFO:root:current mean train loss 2163.749778250971
INFO:root:current train perplexity5.535259246826172
INFO:root:current mean train loss 2162.9368243022477
INFO:root:current train perplexity5.545365333557129
INFO:root:current mean train loss 2166.3854007876016
INFO:root:current train perplexity5.556395053863525
INFO:root:current mean train loss 2169.9728675956158
INFO:root:current train perplexity5.5589752197265625
INFO:root:current mean train loss 2172.2835109824664
INFO:root:current train perplexity5.562445163726807
INFO:root:current mean train loss 2170.298920453218
INFO:root:current train perplexity5.554035186767578
INFO:root:current mean train loss 2172.261314282957
INFO:root:current train perplexity5.559929847717285
INFO:root:current mean train loss 2173.5886375155083
INFO:root:current train perplexity5.566441059112549
INFO:root:current mean train loss 2173.5089733647364
INFO:root:current train perplexity5.562893390655518
INFO:root:current mean train loss 2174.085053089738
INFO:root:current train perplexity5.561359405517578
INFO:root:current mean train loss 2173.456404399627
INFO:root:current train perplexity5.560897350311279
INFO:root:current mean train loss 2172.4477524633385
INFO:root:current train perplexity5.5552263259887695
INFO:root:current mean train loss 2174.736850557404
INFO:root:current train perplexity5.557264804840088
INFO:root:current mean train loss 2175.170044277703
INFO:root:current train perplexity5.5558180809021
INFO:root:current mean train loss 2174.1319182513244
INFO:root:current train perplexity5.5522356033325195
INFO:root:current mean train loss 2174.3247398710164
INFO:root:current train perplexity5.551387786865234
INFO:root:current mean train loss 2174.5859626179163
INFO:root:current train perplexity5.550289154052734
INFO:root:current mean train loss 2174.1544805356307
INFO:root:current train perplexity5.550739765167236
INFO:root:current mean train loss 2173.4510558183088
INFO:root:current train perplexity5.549706935882568

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.97s/it]
INFO:root:final mean train loss: 2172.878039938599
INFO:root:final train perplexity: 5.5492472648620605
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2151.561122596687
INFO:root:eval perplexity: 5.697664737701416
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/11
  6%|â–Œ         | 11/200 [36:27<10:25:33, 198.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2136.899581554324
INFO:root:current train perplexity5.406261444091797
INFO:root:current mean train loss 2149.1341447727655
INFO:root:current train perplexity5.45319938659668
INFO:root:current mean train loss 2151.6383756624236
INFO:root:current train perplexity5.450878620147705
INFO:root:current mean train loss 2146.152542035197
INFO:root:current train perplexity5.423563480377197
INFO:root:current mean train loss 2148.6657609350887
INFO:root:current train perplexity5.43742561340332
INFO:root:current mean train loss 2143.774648829125
INFO:root:current train perplexity5.4260735511779785
INFO:root:current mean train loss 2148.84228444447
INFO:root:current train perplexity5.453320503234863
INFO:root:current mean train loss 2150.210349046547
INFO:root:current train perplexity5.453401565551758
INFO:root:current mean train loss 2150.2133247599377
INFO:root:current train perplexity5.452709674835205
INFO:root:current mean train loss 2151.793385720398
INFO:root:current train perplexity5.458198547363281
INFO:root:current mean train loss 2152.12316793368
INFO:root:current train perplexity5.45753288269043
INFO:root:current mean train loss 2151.312544875764
INFO:root:current train perplexity5.452866554260254
INFO:root:current mean train loss 2150.9581554169604
INFO:root:current train perplexity5.45390510559082
INFO:root:current mean train loss 2150.882995517395
INFO:root:current train perplexity5.454921722412109
INFO:root:current mean train loss 2151.944477065896
INFO:root:current train perplexity5.458970069885254
INFO:root:current mean train loss 2151.5373535925924
INFO:root:current train perplexity5.458338737487793
INFO:root:current mean train loss 2149.840471116108
INFO:root:current train perplexity5.453291893005371
INFO:root:current mean train loss 2150.194255969826
INFO:root:current train perplexity5.452693939208984
INFO:root:current mean train loss 2151.2129873880526
INFO:root:current train perplexity5.454042911529541

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.94s/it]
INFO:root:final mean train loss: 2150.6058124305623
INFO:root:final train perplexity: 5.452624797821045
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2140.517695866578
INFO:root:eval perplexity: 5.647003650665283
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/12
  6%|â–Œ         | 12/200 [39:45<10:22:12, 198.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2065.3506673177085
INFO:root:current train perplexity5.412752628326416
INFO:root:current mean train loss 2121.360188011984
INFO:root:current train perplexity5.281738758087158
INFO:root:current mean train loss 2130.5064847598524
INFO:root:current train perplexity5.332004547119141
INFO:root:current mean train loss 2131.315623066213
INFO:root:current train perplexity5.339130401611328
INFO:root:current mean train loss 2132.251464540846
INFO:root:current train perplexity5.345993995666504
INFO:root:current mean train loss 2133.7333324273113
INFO:root:current train perplexity5.350167274475098
INFO:root:current mean train loss 2134.8062553848595
INFO:root:current train perplexity5.353503227233887
INFO:root:current mean train loss 2133.7664147237288
INFO:root:current train perplexity5.357500076293945
INFO:root:current mean train loss 2131.5888320713825
INFO:root:current train perplexity5.35644006729126
INFO:root:current mean train loss 2129.2681488679227
INFO:root:current train perplexity5.351362228393555
INFO:root:current mean train loss 2129.135004045481
INFO:root:current train perplexity5.345183849334717
INFO:root:current mean train loss 2130.3247500823395
INFO:root:current train perplexity5.352519512176514
INFO:root:current mean train loss 2132.2987492815814
INFO:root:current train perplexity5.35718297958374
INFO:root:current mean train loss 2132.752666060593
INFO:root:current train perplexity5.3581929206848145
INFO:root:current mean train loss 2132.584602383147
INFO:root:current train perplexity5.360226631164551
INFO:root:current mean train loss 2131.4297437839164
INFO:root:current train perplexity5.358802318572998
INFO:root:current mean train loss 2130.8130935983068
INFO:root:current train perplexity5.359608173370361
INFO:root:current mean train loss 2130.9032371641956
INFO:root:current train perplexity5.360504627227783
INFO:root:current mean train loss 2130.75878770842
INFO:root:current train perplexity5.361372470855713
INFO:root:current mean train loss 2130.6686922555464
INFO:root:current train perplexity5.363255500793457

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.26s/it]
INFO:root:final mean train loss: 2130.1974708607145
INFO:root:final train perplexity: 5.365565776824951
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2132.1926698075963
INFO:root:eval perplexity: 5.6091108322143555
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/13
  6%|â–‹         | 13/200 [43:04<10:19:09, 198.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2074.1102783203123
INFO:root:current train perplexity5.0824666023254395
INFO:root:current mean train loss 2095.8442932128905
INFO:root:current train perplexity5.2593584060668945
INFO:root:current mean train loss 2107.0463217995384
INFO:root:current train perplexity5.2674880027771
INFO:root:current mean train loss 2105.381165313721
INFO:root:current train perplexity5.269089221954346
INFO:root:current mean train loss 2111.292280215309
INFO:root:current train perplexity5.2765069007873535
INFO:root:current mean train loss 2112.8827538123496
INFO:root:current train perplexity5.281679153442383
INFO:root:current mean train loss 2113.4935678789693
INFO:root:current train perplexity5.281713485717773
INFO:root:current mean train loss 2113.772728644477
INFO:root:current train perplexity5.285286903381348
INFO:root:current mean train loss 2116.3879866437214
INFO:root:current train perplexity5.296990871429443
INFO:root:current mean train loss 2118.0994050399117
INFO:root:current train perplexity5.298779487609863
INFO:root:current mean train loss 2115.984785970052
INFO:root:current train perplexity5.290423393249512
INFO:root:current mean train loss 2113.811257171631
INFO:root:current train perplexity5.2886834144592285
INFO:root:current mean train loss 2114.177948898566
INFO:root:current train perplexity5.294559955596924
INFO:root:current mean train loss 2114.521380707712
INFO:root:current train perplexity5.298708915710449
INFO:root:current mean train loss 2116.2961387097
INFO:root:current train perplexity5.302290439605713
INFO:root:current mean train loss 2115.3506514699834
INFO:root:current train perplexity5.296900749206543
INFO:root:current mean train loss 2113.9796342261043
INFO:root:current train perplexity5.291619300842285
INFO:root:current mean train loss 2112.727784409634
INFO:root:current train perplexity5.285828113555908
INFO:root:current mean train loss 2111.4459958925354
INFO:root:current train perplexity5.28447961807251
INFO:root:current mean train loss 2112.588177998861
INFO:root:current train perplexity5.287499904632568

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.43s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.43s/it]
INFO:root:final mean train loss: 2112.03778417821
INFO:root:final train perplexity: 5.289268970489502
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2122.434267301086
INFO:root:eval perplexity: 5.5650177001953125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/14
  7%|â–‹         | 14/200 [46:23<10:16:09, 198.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2086.5896161053633
INFO:root:current train perplexity5.1821746826171875
INFO:root:current mean train loss 2079.550160206147
INFO:root:current train perplexity5.1941118240356445
INFO:root:current mean train loss 2083.4086538065335
INFO:root:current train perplexity5.18049430847168
INFO:root:current mean train loss 2083.891523683814
INFO:root:current train perplexity5.182722091674805
INFO:root:current mean train loss 2078.4699799212494
INFO:root:current train perplexity5.169428825378418
INFO:root:current mean train loss 2080.363637458901
INFO:root:current train perplexity5.181382656097412
INFO:root:current mean train loss 2085.5813241237
INFO:root:current train perplexity5.191640853881836
INFO:root:current mean train loss 2088.1966968469087
INFO:root:current train perplexity5.195657730102539
INFO:root:current mean train loss 2089.526882157958
INFO:root:current train perplexity5.202591419219971
INFO:root:current mean train loss 2090.3699037924393
INFO:root:current train perplexity5.2033257484436035
INFO:root:current mean train loss 2092.0404593045596
INFO:root:current train perplexity5.209377288818359
INFO:root:current mean train loss 2091.1104289703235
INFO:root:current train perplexity5.20367956161499
INFO:root:current mean train loss 2092.30436184759
INFO:root:current train perplexity5.2074689865112305
INFO:root:current mean train loss 2093.285254125374
INFO:root:current train perplexity5.21189546585083
INFO:root:current mean train loss 2093.5463068676063
INFO:root:current train perplexity5.211287975311279
INFO:root:current mean train loss 2093.9307732665857
INFO:root:current train perplexity5.21133279800415
INFO:root:current mean train loss 2093.330493253546
INFO:root:current train perplexity5.2107415199279785
INFO:root:current mean train loss 2095.2012037805393
INFO:root:current train perplexity5.217557430267334
INFO:root:current mean train loss 2095.3698614179666
INFO:root:current train perplexity5.220001697540283
INFO:root:current mean train loss 2095.5910457991176
INFO:root:current train perplexity5.218192100524902

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.38s/it]
INFO:root:final mean train loss: 2095.171260893375
INFO:root:final train perplexity: 5.219377040863037
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.36s/it]
INFO:root:eval mean loss: 2114.554879262938
INFO:root:eval perplexity: 5.529668807983398
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/15
  8%|â–Š         | 15/200 [49:42<10:13:07, 198.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2059.573183412905
INFO:root:current train perplexity5.088473796844482
INFO:root:current mean train loss 2078.0465135450486
INFO:root:current train perplexity5.147972583770752
INFO:root:current mean train loss 2078.061118298628
INFO:root:current train perplexity5.13633394241333
INFO:root:current mean train loss 2080.6693908346574
INFO:root:current train perplexity5.149661540985107
INFO:root:current mean train loss 2081.582474359857
INFO:root:current train perplexity5.158017635345459
INFO:root:current mean train loss 2081.518314733402
INFO:root:current train perplexity5.154364585876465
INFO:root:current mean train loss 2078.755645845279
INFO:root:current train perplexity5.144659996032715
INFO:root:current mean train loss 2079.740852011926
INFO:root:current train perplexity5.150132179260254
INFO:root:current mean train loss 2079.233279797735
INFO:root:current train perplexity5.151889324188232
INFO:root:current mean train loss 2080.44020026185
INFO:root:current train perplexity5.155925273895264
INFO:root:current mean train loss 2078.3956713450248
INFO:root:current train perplexity5.150835037231445
INFO:root:current mean train loss 2078.263752162147
INFO:root:current train perplexity5.149569034576416
INFO:root:current mean train loss 2076.731427109032
INFO:root:current train perplexity5.150275230407715
INFO:root:current mean train loss 2076.5712271257903
INFO:root:current train perplexity5.150826454162598
INFO:root:current mean train loss 2077.0182064428946
INFO:root:current train perplexity5.150885581970215
INFO:root:current mean train loss 2079.6238693963915
INFO:root:current train perplexity5.154147148132324
INFO:root:current mean train loss 2079.8502131580876
INFO:root:current train perplexity5.157076835632324
INFO:root:current mean train loss 2079.6194402331503
INFO:root:current train perplexity5.156265735626221
INFO:root:current mean train loss 2078.972329346493
INFO:root:current train perplexity5.151822566986084
INFO:root:current mean train loss 2079.8732550942177
INFO:root:current train perplexity5.154446125030518

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.91s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.91s/it]
INFO:root:final mean train loss: 2078.7382850974
INFO:root:final train perplexity: 5.152170181274414
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2108.3752328859155
INFO:root:eval perplexity: 5.502101898193359
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/16
  8%|â–Š         | 16/200 [53:01<10:09:28, 198.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2061.287532322843
INFO:root:current train perplexity5.08148193359375
INFO:root:current mean train loss 2060.4240208675988
INFO:root:current train perplexity5.073400497436523
INFO:root:current mean train loss 2053.1474537303966
INFO:root:current train perplexity5.065461158752441
INFO:root:current mean train loss 2054.95111166242
INFO:root:current train perplexity5.063309192657471
INFO:root:current mean train loss 2052.797182897094
INFO:root:current train perplexity5.063080787658691
INFO:root:current mean train loss 2058.4267340825445
INFO:root:current train perplexity5.078618049621582
INFO:root:current mean train loss 2058.9416636710016
INFO:root:current train perplexity5.084216117858887
INFO:root:current mean train loss 2059.9653253815054
INFO:root:current train perplexity5.082579612731934
INFO:root:current mean train loss 2059.5323875944046
INFO:root:current train perplexity5.077674388885498
INFO:root:current mean train loss 2060.2112514130486
INFO:root:current train perplexity5.081553936004639
INFO:root:current mean train loss 2060.0911339796335
INFO:root:current train perplexity5.07934045791626
INFO:root:current mean train loss 2060.6526130135503
INFO:root:current train perplexity5.0829691886901855
INFO:root:current mean train loss 2060.7783003356117
INFO:root:current train perplexity5.084163665771484
INFO:root:current mean train loss 2060.8661514604987
INFO:root:current train perplexity5.084502220153809
INFO:root:current mean train loss 2062.5829487274814
INFO:root:current train perplexity5.087314128875732
INFO:root:current mean train loss 2063.5750285633653
INFO:root:current train perplexity5.090271949768066
INFO:root:current mean train loss 2062.353353595106
INFO:root:current train perplexity5.08797550201416
INFO:root:current mean train loss 2062.599038656656
INFO:root:current train perplexity5.08780574798584
INFO:root:current mean train loss 2063.762446931057
INFO:root:current train perplexity5.090740203857422
INFO:root:current mean train loss 2064.134015118634
INFO:root:current train perplexity5.090630054473877

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.11s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.11s/it]
INFO:root:final mean train loss: 2063.473660821573
INFO:root:final train perplexity: 5.090517044067383
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2101.1414613115026
INFO:root:eval perplexity: 5.470006465911865
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/17
  8%|â–Š         | 17/200 [56:19<10:06:06, 198.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2020.0917830033736
INFO:root:current train perplexity4.938107490539551
INFO:root:current mean train loss 2017.2987859198388
INFO:root:current train perplexity4.910636901855469
INFO:root:current mean train loss 2034.1301015218098
INFO:root:current train perplexity4.972564697265625
INFO:root:current mean train loss 2036.9207398719395
INFO:root:current train perplexity4.988831043243408
INFO:root:current mean train loss 2038.7070049848712
INFO:root:current train perplexity4.994309425354004
INFO:root:current mean train loss 2037.2196015857514
INFO:root:current train perplexity4.9993391036987305
INFO:root:current mean train loss 2045.337623951047
INFO:root:current train perplexity5.015161514282227
INFO:root:current mean train loss 2046.7611869386005
INFO:root:current train perplexity5.012624740600586
INFO:root:current mean train loss 2046.9020509462098
INFO:root:current train perplexity5.015880584716797
INFO:root:current mean train loss 2046.0390978361431
INFO:root:current train perplexity5.015495777130127
INFO:root:current mean train loss 2045.3586258607752
INFO:root:current train perplexity5.023059844970703
INFO:root:current mean train loss 2044.3231802275686
INFO:root:current train perplexity5.020124435424805
INFO:root:current mean train loss 2047.6954857488597
INFO:root:current train perplexity5.024686336517334
INFO:root:current mean train loss 2049.809161095523
INFO:root:current train perplexity5.028581619262695
INFO:root:current mean train loss 2050.221611187022
INFO:root:current train perplexity5.029069423675537
INFO:root:current mean train loss 2048.410697879359
INFO:root:current train perplexity5.027193546295166
INFO:root:current mean train loss 2048.125492114026
INFO:root:current train perplexity5.0298919677734375
INFO:root:current mean train loss 2048.374630512007
INFO:root:current train perplexity5.031761169433594
INFO:root:current mean train loss 2049.183161977994
INFO:root:current train perplexity5.030963897705078

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.96s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.96s/it]
INFO:root:final mean train loss: 2049.43373594991
INFO:root:final train perplexity: 5.034461975097656
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.35s/it]
INFO:root:eval mean loss: 2096.968167785212
INFO:root:eval perplexity: 5.4515767097473145
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/18
  9%|â–‰         | 18/200 [59:38<10:02:42, 198.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2127.69609375
INFO:root:current train perplexity4.8827223777771
INFO:root:current mean train loss 2039.1337739490327
INFO:root:current train perplexity4.948426723480225
INFO:root:current mean train loss 2027.7372052448552
INFO:root:current train perplexity4.929211616516113
INFO:root:current mean train loss 2032.568340564165
INFO:root:current train perplexity4.952119827270508
INFO:root:current mean train loss 2031.475141360436
INFO:root:current train perplexity4.948785305023193
INFO:root:current mean train loss 2032.4367506574877
INFO:root:current train perplexity4.946690082550049
INFO:root:current mean train loss 2033.310221421423
INFO:root:current train perplexity4.950451850891113
INFO:root:current mean train loss 2034.1882092198582
INFO:root:current train perplexity4.958771705627441
INFO:root:current mean train loss 2035.2669773267662
INFO:root:current train perplexity4.965175151824951
INFO:root:current mean train loss 2034.7678025725138
INFO:root:current train perplexity4.967958927154541
INFO:root:current mean train loss 2034.0712439997278
INFO:root:current train perplexity4.965940475463867
INFO:root:current mean train loss 2034.1525729770574
INFO:root:current train perplexity4.967575550079346
INFO:root:current mean train loss 2036.8818862851724
INFO:root:current train perplexity4.96917200088501
INFO:root:current mean train loss 2037.3036919487847
INFO:root:current train perplexity4.971402168273926
INFO:root:current mean train loss 2037.5026164750611
INFO:root:current train perplexity4.9763875007629395
INFO:root:current mean train loss 2037.194488667333
INFO:root:current train perplexity4.978179454803467
INFO:root:current mean train loss 2037.456653630622
INFO:root:current train perplexity4.9801025390625
INFO:root:current mean train loss 2037.410225912413
INFO:root:current train perplexity4.98106050491333
INFO:root:current mean train loss 2037.5189145413133
INFO:root:current train perplexity4.981088161468506
INFO:root:current mean train loss 2036.6581500676673
INFO:root:current train perplexity4.979869365692139

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.03s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.03s/it]
INFO:root:final mean train loss: 2036.0457743665395
INFO:root:final train perplexity: 4.981585502624512
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it]
INFO:root:eval mean loss: 2091.9108168148823
INFO:root:eval perplexity: 5.429323673248291
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/19
 10%|â–‰         | 19/200 [1:02:57<9:59:21, 198.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2008.070595481179
INFO:root:current train perplexity4.804656505584717
INFO:root:current mean train loss 2024.9766805680072
INFO:root:current train perplexity4.900717735290527
INFO:root:current mean train loss 2016.96957754874
INFO:root:current train perplexity4.897048473358154
INFO:root:current mean train loss 2017.4647774074388
INFO:root:current train perplexity4.914487361907959
INFO:root:current mean train loss 2021.8913814309649
INFO:root:current train perplexity4.911999702453613
INFO:root:current mean train loss 2018.7475939052772
INFO:root:current train perplexity4.9090094566345215
INFO:root:current mean train loss 2015.4816203715334
INFO:root:current train perplexity4.908553600311279
INFO:root:current mean train loss 2015.7245486779887
INFO:root:current train perplexity4.905661582946777
INFO:root:current mean train loss 2018.0470184548929
INFO:root:current train perplexity4.913600444793701
INFO:root:current mean train loss 2016.7668696670366
INFO:root:current train perplexity4.91221284866333
INFO:root:current mean train loss 2017.4423589239848
INFO:root:current train perplexity4.914815902709961
INFO:root:current mean train loss 2021.1506387911165
INFO:root:current train perplexity4.918255805969238
INFO:root:current mean train loss 2022.9586010822102
INFO:root:current train perplexity4.920051574707031
INFO:root:current mean train loss 2023.0170781168742
INFO:root:current train perplexity4.919447898864746
INFO:root:current mean train loss 2023.5668445699828
INFO:root:current train perplexity4.92488431930542
INFO:root:current mean train loss 2023.9582270899207
INFO:root:current train perplexity4.928730487823486
INFO:root:current mean train loss 2024.09049075276
INFO:root:current train perplexity4.93320369720459
INFO:root:current mean train loss 2023.5631496947817
INFO:root:current train perplexity4.93116569519043
INFO:root:current mean train loss 2025.0710242864984
INFO:root:current train perplexity4.933717250823975
INFO:root:current mean train loss 2024.3907936247033
INFO:root:current train perplexity4.932255744934082

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.18s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.18s/it]
INFO:root:final mean train loss: 2023.4194660965866
INFO:root:final train perplexity: 4.932224750518799
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.33s/it]
INFO:root:eval mean loss: 2087.73147041916
INFO:root:eval perplexity: 5.411003589630127
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/20
 10%|â–ˆ         | 20/200 [1:06:15<9:56:09, 198.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2029.2775065104167
INFO:root:current train perplexity4.948493003845215
INFO:root:current mean train loss 1999.091968124719
INFO:root:current train perplexity4.846836090087891
INFO:root:current mean train loss 1995.6198046057793
INFO:root:current train perplexity4.816702842712402
INFO:root:current mean train loss 1996.6685164460039
INFO:root:current train perplexity4.83352518081665
INFO:root:current mean train loss 2001.3714029577163
INFO:root:current train perplexity4.829781532287598
INFO:root:current mean train loss 2002.5527567960778
INFO:root:current train perplexity4.838917255401611
INFO:root:current mean train loss 2007.0605604383682
INFO:root:current train perplexity4.849823951721191
INFO:root:current mean train loss 2006.487594088295
INFO:root:current train perplexity4.850117206573486
INFO:root:current mean train loss 2008.3830367078087
INFO:root:current train perplexity4.849918842315674
INFO:root:current mean train loss 2008.0625538201377
INFO:root:current train perplexity4.856790065765381
INFO:root:current mean train loss 2008.1793296307296
INFO:root:current train perplexity4.860846042633057
INFO:root:current mean train loss 2009.6318333653423
INFO:root:current train perplexity4.8675665855407715
INFO:root:current mean train loss 2010.2853143798236
INFO:root:current train perplexity4.868990898132324
INFO:root:current mean train loss 2010.8553877040645
INFO:root:current train perplexity4.875854969024658
INFO:root:current mean train loss 2011.5311789973234
INFO:root:current train perplexity4.879185199737549
INFO:root:current mean train loss 2010.9499864683592
INFO:root:current train perplexity4.879251956939697
INFO:root:current mean train loss 2010.803738569035
INFO:root:current train perplexity4.880938529968262
INFO:root:current mean train loss 2011.9279395570147
INFO:root:current train perplexity4.882196426391602
INFO:root:current mean train loss 2010.9183230791616
INFO:root:current train perplexity4.882046699523926
INFO:root:current mean train loss 2011.5756922815804
INFO:root:current train perplexity4.88439416885376

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.81s/it]
INFO:root:final mean train loss: 2010.7907752702167
INFO:root:final train perplexity: 4.883345603942871
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it]
INFO:root:eval mean loss: 2084.1513039879765
INFO:root:eval perplexity: 5.395359516143799
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/21
 10%|â–ˆ         | 21/200 [1:09:34<9:52:33, 198.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1996.8920266287666
INFO:root:current train perplexity4.847805976867676
INFO:root:current mean train loss 2006.686276166867
INFO:root:current train perplexity4.85010290145874
INFO:root:current mean train loss 2005.3264737129211
INFO:root:current train perplexity4.824777126312256
INFO:root:current mean train loss 2001.9444559504477
INFO:root:current train perplexity4.835420608520508
INFO:root:current mean train loss 2000.2609210097999
INFO:root:current train perplexity4.831098556518555
INFO:root:current mean train loss 2001.95274868972
INFO:root:current train perplexity4.832024574279785
INFO:root:current mean train loss 2001.7392678609708
INFO:root:current train perplexity4.824838638305664
INFO:root:current mean train loss 1999.2846353520792
INFO:root:current train perplexity4.8242316246032715
INFO:root:current mean train loss 1998.5888577755366
INFO:root:current train perplexity4.828502655029297
INFO:root:current mean train loss 1999.3791581796304
INFO:root:current train perplexity4.830606937408447
INFO:root:current mean train loss 1999.5221720608797
INFO:root:current train perplexity4.833853244781494
INFO:root:current mean train loss 1998.6480187016787
INFO:root:current train perplexity4.835331916809082
INFO:root:current mean train loss 1999.5948750684215
INFO:root:current train perplexity4.835459232330322
INFO:root:current mean train loss 1999.5021778207965
INFO:root:current train perplexity4.832935810089111
INFO:root:current mean train loss 1999.7825631780938
INFO:root:current train perplexity4.8365678787231445
INFO:root:current mean train loss 1998.5703730644482
INFO:root:current train perplexity4.833690643310547
INFO:root:current mean train loss 2000.3050262893455
INFO:root:current train perplexity4.837657928466797
INFO:root:current mean train loss 2000.4438993762458
INFO:root:current train perplexity4.841309547424316
INFO:root:current mean train loss 1999.7023345026478
INFO:root:current train perplexity4.8406267166137695
INFO:root:current mean train loss 1999.4839725923441
INFO:root:current train perplexity4.839746475219727

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.97s/it]
INFO:root:final mean train loss: 1999.4182493639785
INFO:root:final train perplexity: 4.839742183685303
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it]
INFO:root:eval mean loss: 2080.722841519836
INFO:root:eval perplexity: 5.380420684814453
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/22
 11%|â–ˆ         | 22/200 [1:12:52<9:49:09, 198.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1977.4920620852954
INFO:root:current train perplexity4.753332138061523
INFO:root:current mean train loss 1974.0390744953486
INFO:root:current train perplexity4.75969934463501
INFO:root:current mean train loss 1978.6441945505667
INFO:root:current train perplexity4.767667770385742
INFO:root:current mean train loss 1982.3439388326283
INFO:root:current train perplexity4.764098644256592
INFO:root:current mean train loss 1985.1839271851877
INFO:root:current train perplexity4.778686046600342
INFO:root:current mean train loss 1986.5503068161677
INFO:root:current train perplexity4.778140544891357
INFO:root:current mean train loss 1985.569111748932
INFO:root:current train perplexity4.778944969177246
INFO:root:current mean train loss 1988.7679425988438
INFO:root:current train perplexity4.791881084442139
INFO:root:current mean train loss 1986.0207242670747
INFO:root:current train perplexity4.785351753234863
INFO:root:current mean train loss 1985.796108453639
INFO:root:current train perplexity4.787619590759277
INFO:root:current mean train loss 1986.5091576625116
INFO:root:current train perplexity4.786873817443848
INFO:root:current mean train loss 1986.9939056422502
INFO:root:current train perplexity4.792368412017822
INFO:root:current mean train loss 1986.6786621285535
INFO:root:current train perplexity4.7940826416015625
INFO:root:current mean train loss 1985.5654317323779
INFO:root:current train perplexity4.794242858886719
INFO:root:current mean train loss 1984.9746675510755
INFO:root:current train perplexity4.794760704040527
INFO:root:current mean train loss 1985.836834363701
INFO:root:current train perplexity4.795271396636963
INFO:root:current mean train loss 1988.2037014228742
INFO:root:current train perplexity4.79726505279541
INFO:root:current mean train loss 1987.7133096435684
INFO:root:current train perplexity4.795156002044678
INFO:root:current mean train loss 1988.2028644356062
INFO:root:current train perplexity4.796090602874756
INFO:root:current mean train loss 1988.0422493827807
INFO:root:current train perplexity4.795140266418457

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.08s/it]
INFO:root:final mean train loss: 1987.5621947934396
INFO:root:final train perplexity: 4.7947001457214355
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2078.496224044908
INFO:root:eval perplexity: 5.3707404136657715
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/23
 12%|â–ˆâ–        | 23/200 [1:16:11<9:45:55, 198.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1957.5016303168402
INFO:root:current train perplexity4.684242248535156
INFO:root:current mean train loss 1965.8835031609785
INFO:root:current train perplexity4.701803684234619
INFO:root:current mean train loss 1962.9597273201778
INFO:root:current train perplexity4.702927112579346
INFO:root:current mean train loss 1970.878346917568
INFO:root:current train perplexity4.725825786590576
INFO:root:current mean train loss 1967.3637363978794
INFO:root:current train perplexity4.729630470275879
INFO:root:current mean train loss 1967.6819989737817
INFO:root:current train perplexity4.729623317718506
INFO:root:current mean train loss 1970.5077700407608
INFO:root:current train perplexity4.740139484405518
INFO:root:current mean train loss 1972.7031780001482
INFO:root:current train perplexity4.743614196777344
INFO:root:current mean train loss 1972.8304912438552
INFO:root:current train perplexity4.745980739593506
INFO:root:current mean train loss 1972.329697117661
INFO:root:current train perplexity4.74177885055542
INFO:root:current mean train loss 1972.7520644441656
INFO:root:current train perplexity4.74534797668457
INFO:root:current mean train loss 1972.6842450310203
INFO:root:current train perplexity4.745720386505127
INFO:root:current mean train loss 1974.8308380836663
INFO:root:current train perplexity4.750044345855713
INFO:root:current mean train loss 1975.6391855363365
INFO:root:current train perplexity4.751787185668945
INFO:root:current mean train loss 1974.1724420944315
INFO:root:current train perplexity4.748997211456299
INFO:root:current mean train loss 1975.2185691209709
INFO:root:current train perplexity4.750421524047852
INFO:root:current mean train loss 1976.396088910526
INFO:root:current train perplexity4.749970436096191
INFO:root:current mean train loss 1976.5548549204566
INFO:root:current train perplexity4.750258922576904
INFO:root:current mean train loss 1976.6344468858506
INFO:root:current train perplexity4.752350807189941

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.03s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.03s/it]
INFO:root:final mean train loss: 1976.8767148939817
INFO:root:final train perplexity: 4.7544636726379395
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2074.8495327563996
INFO:root:eval perplexity: 5.35492467880249
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/24
 12%|â–ˆâ–        | 24/200 [1:19:30<9:42:36, 198.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1887.1072300502233
INFO:root:current train perplexity4.555998802185059
INFO:root:current mean train loss 1965.1213310455607
INFO:root:current train perplexity4.686151504516602
INFO:root:current mean train loss 1963.5890794836957
INFO:root:current train perplexity4.679340362548828
INFO:root:current mean train loss 1964.4914956356881
INFO:root:current train perplexity4.690594673156738
INFO:root:current mean train loss 1965.3481976183393
INFO:root:current train perplexity4.69776725769043
INFO:root:current mean train loss 1966.5457908133783
INFO:root:current train perplexity4.704858779907227
INFO:root:current mean train loss 1965.9581367203589
INFO:root:current train perplexity4.701502323150635
INFO:root:current mean train loss 1966.2121119303615
INFO:root:current train perplexity4.7017011642456055
INFO:root:current mean train loss 1963.9731797758384
INFO:root:current train perplexity4.697017669677734
INFO:root:current mean train loss 1967.5640665914416
INFO:root:current train perplexity4.707087993621826
INFO:root:current mean train loss 1967.7353383493282
INFO:root:current train perplexity4.704980850219727
INFO:root:current mean train loss 1967.8632085812233
INFO:root:current train perplexity4.705436706542969
INFO:root:current mean train loss 1968.783503395719
INFO:root:current train perplexity4.714757919311523
INFO:root:current mean train loss 1968.1241820261691
INFO:root:current train perplexity4.713050842285156
INFO:root:current mean train loss 1967.69335816037
INFO:root:current train perplexity4.7117838859558105
INFO:root:current mean train loss 1967.6497177397405
INFO:root:current train perplexity4.710856914520264
INFO:root:current mean train loss 1966.4567002092895
INFO:root:current train perplexity4.7101335525512695
INFO:root:current mean train loss 1966.3333267542655
INFO:root:current train perplexity4.710593223571777
INFO:root:current mean train loss 1967.0478165694608
INFO:root:current train perplexity4.714249134063721
INFO:root:current mean train loss 1967.1071507214374
INFO:root:current train perplexity4.713898658752441

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.35s/it]
INFO:root:final mean train loss: 1966.2034849319805
INFO:root:final train perplexity: 4.7146100997924805
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it]
INFO:root:eval mean loss: 2074.672779272634
INFO:root:eval perplexity: 5.354158878326416
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/25
 12%|â–ˆâ–Ž        | 25/200 [1:22:49<9:39:44, 198.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1961.3878072102864
INFO:root:current train perplexity4.680179119110107
INFO:root:current mean train loss 1951.6431904454384
INFO:root:current train perplexity4.646309852600098
INFO:root:current mean train loss 1944.505989074707
INFO:root:current train perplexity4.65640926361084
INFO:root:current mean train loss 1949.2516611358267
INFO:root:current train perplexity4.657060623168945
INFO:root:current mean train loss 1951.004359407245
INFO:root:current train perplexity4.660390853881836
INFO:root:current mean train loss 1952.1111473491173
INFO:root:current train perplexity4.657948017120361
INFO:root:current mean train loss 1955.0494676247622
INFO:root:current train perplexity4.657646179199219
INFO:root:current mean train loss 1954.8872220371309
INFO:root:current train perplexity4.6653313636779785
INFO:root:current mean train loss 1953.2480679113887
INFO:root:current train perplexity4.665350914001465
INFO:root:current mean train loss 1954.0578678015506
INFO:root:current train perplexity4.667844772338867
INFO:root:current mean train loss 1956.5512026548386
INFO:root:current train perplexity4.674612045288086
INFO:root:current mean train loss 1955.590070514068
INFO:root:current train perplexity4.674613952636719
INFO:root:current mean train loss 1955.7698557735268
INFO:root:current train perplexity4.674540996551514
INFO:root:current mean train loss 1956.133471993161
INFO:root:current train perplexity4.673162937164307
INFO:root:current mean train loss 1955.757261126229
INFO:root:current train perplexity4.672189235687256
INFO:root:current mean train loss 1955.8151275554667
INFO:root:current train perplexity4.674165725708008
INFO:root:current mean train loss 1955.9684572266829
INFO:root:current train perplexity4.675505638122559
INFO:root:current mean train loss 1955.9254963956687
INFO:root:current train perplexity4.675367832183838
INFO:root:current mean train loss 1956.359302253054
INFO:root:current train perplexity4.676060676574707
INFO:root:current mean train loss 1956.3630857725402
INFO:root:current train perplexity4.676187038421631

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.32s/it]
INFO:root:final mean train loss: 1955.4895562784154
INFO:root:final train perplexity: 4.6749420166015625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2072.5100573817044
INFO:root:eval perplexity: 5.344802379608154
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/26
 13%|â–ˆâ–Ž        | 26/200 [1:26:08<9:36:35, 198.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1937.0912162966845
INFO:root:current train perplexity4.580554962158203
INFO:root:current mean train loss 1926.0854604734598
INFO:root:current train perplexity4.574240207672119
INFO:root:current mean train loss 1935.2130861401063
INFO:root:current train perplexity4.585978984832764
INFO:root:current mean train loss 1933.9226503791697
INFO:root:current train perplexity4.587893962860107
INFO:root:current mean train loss 1935.0531996815653
INFO:root:current train perplexity4.590532302856445
INFO:root:current mean train loss 1933.6647701016636
INFO:root:current train perplexity4.594423770904541
INFO:root:current mean train loss 1938.0096989719432
INFO:root:current train perplexity4.6090898513793945
INFO:root:current mean train loss 1939.24282750427
INFO:root:current train perplexity4.613900661468506
INFO:root:current mean train loss 1938.4508278718602
INFO:root:current train perplexity4.615026473999023
INFO:root:current mean train loss 1939.9643694789452
INFO:root:current train perplexity4.621665000915527
INFO:root:current mean train loss 1941.9229977888974
INFO:root:current train perplexity4.624352931976318
INFO:root:current mean train loss 1942.2177088183337
INFO:root:current train perplexity4.628551006317139
INFO:root:current mean train loss 1942.119330861893
INFO:root:current train perplexity4.632128715515137
INFO:root:current mean train loss 1942.5316173032893
INFO:root:current train perplexity4.632336139678955
INFO:root:current mean train loss 1942.8365451407715
INFO:root:current train perplexity4.633697986602783
INFO:root:current mean train loss 1944.0218096317833
INFO:root:current train perplexity4.637491226196289
INFO:root:current mean train loss 1944.9211499425132
INFO:root:current train perplexity4.637547969818115
INFO:root:current mean train loss 1945.4580132113592
INFO:root:current train perplexity4.6360344886779785
INFO:root:current mean train loss 1946.2515584685632
INFO:root:current train perplexity4.6392998695373535
INFO:root:current mean train loss 1947.0437874575364
INFO:root:current train perplexity4.640496253967285

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.11s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.11s/it]
INFO:root:final mean train loss: 1946.302498021513
INFO:root:final train perplexity: 4.6411919593811035
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.33s/it]
INFO:root:eval mean loss: 2070.542146290448
INFO:root:eval perplexity: 5.336301803588867
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/27
 14%|â–ˆâ–Ž        | 27/200 [1:29:26<9:33:11, 198.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1914.8007959826239
INFO:root:current train perplexity4.547439098358154
INFO:root:current mean train loss 1926.5465172876286
INFO:root:current train perplexity4.567802906036377
INFO:root:current mean train loss 1923.949127433836
INFO:root:current train perplexity4.575974941253662
INFO:root:current mean train loss 1925.6964227260823
INFO:root:current train perplexity4.578274250030518
INFO:root:current mean train loss 1923.850718135917
INFO:root:current train perplexity4.573155403137207
INFO:root:current mean train loss 1927.9452455582157
INFO:root:current train perplexity4.579710006713867
INFO:root:current mean train loss 1928.8523301701416
INFO:root:current train perplexity4.586560249328613
INFO:root:current mean train loss 1926.8946895549038
INFO:root:current train perplexity4.584085941314697
INFO:root:current mean train loss 1928.2379124781469
INFO:root:current train perplexity4.58657693862915
INFO:root:current mean train loss 1929.514604349475
INFO:root:current train perplexity4.584567546844482
INFO:root:current mean train loss 1929.9095456676807
INFO:root:current train perplexity4.588249206542969
INFO:root:current mean train loss 1931.0571569465808
INFO:root:current train perplexity4.587793350219727
INFO:root:current mean train loss 1933.155491184544
INFO:root:current train perplexity4.596452236175537
INFO:root:current mean train loss 1933.3408861118141
INFO:root:current train perplexity4.598195552825928
INFO:root:current mean train loss 1934.2000452782065
INFO:root:current train perplexity4.598223686218262
INFO:root:current mean train loss 1934.630248866736
INFO:root:current train perplexity4.600918769836426
INFO:root:current mean train loss 1935.0453018538196
INFO:root:current train perplexity4.600395679473877
INFO:root:current mean train loss 1935.931433216568
INFO:root:current train perplexity4.601459503173828
INFO:root:current mean train loss 1936.5072158140053
INFO:root:current train perplexity4.604739665985107
INFO:root:current mean train loss 1937.0815525697858
INFO:root:current train perplexity4.605915069580078

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.89s/it]
INFO:root:final mean train loss: 1936.728053475292
INFO:root:final train perplexity: 4.606279373168945
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it]
INFO:root:eval mean loss: 2068.789117474928
INFO:root:eval perplexity: 5.328741550445557
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/28
 14%|â–ˆâ–        | 28/200 [1:32:45<9:29:37, 198.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1942.7464713541667
INFO:root:current train perplexity4.580667495727539
INFO:root:current mean train loss 1922.8716517857142
INFO:root:current train perplexity4.538217067718506
INFO:root:current mean train loss 1917.9370028409091
INFO:root:current train perplexity4.540460109710693
INFO:root:current mean train loss 1922.410001627604
INFO:root:current train perplexity4.547844409942627
INFO:root:current mean train loss 1922.0713802939968
INFO:root:current train perplexity4.549554347991943
INFO:root:current mean train loss 1922.4861992612093
INFO:root:current train perplexity4.550710201263428
INFO:root:current mean train loss 1923.9999828197338
INFO:root:current train perplexity4.55081844329834
INFO:root:current mean train loss 1927.0824998424898
INFO:root:current train perplexity4.554043769836426
INFO:root:current mean train loss 1924.0277292131696
INFO:root:current train perplexity4.556611061096191
INFO:root:current mean train loss 1921.509557792468
INFO:root:current train perplexity4.550938129425049
INFO:root:current mean train loss 1920.5505614098838
INFO:root:current train perplexity4.551712989807129
INFO:root:current mean train loss 1921.9375030127992
INFO:root:current train perplexity4.55618143081665
INFO:root:current mean train loss 1922.982802064185
INFO:root:current train perplexity4.55976676940918
INFO:root:current mean train loss 1923.1432283380682
INFO:root:current train perplexity4.559858322143555
INFO:root:current mean train loss 1924.4501853813558
INFO:root:current train perplexity4.563418388366699
INFO:root:current mean train loss 1925.3851869419643
INFO:root:current train perplexity4.563891887664795
INFO:root:current mean train loss 1926.235455704874
INFO:root:current train perplexity4.566030025482178
INFO:root:current mean train loss 1925.814179549956
INFO:root:current train perplexity4.565210342407227
INFO:root:current mean train loss 1926.6101022135417
INFO:root:current train perplexity4.568756580352783
INFO:root:current mean train loss 1926.8140612020372
INFO:root:current train perplexity4.568817615509033

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.27s/it]
INFO:root:final mean train loss: 1926.3908185703972
INFO:root:final train perplexity: 4.568878173828125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2067.2820339338155
INFO:root:eval perplexity: 5.322251796722412
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/29
 14%|â–ˆâ–        | 29/200 [1:36:04<9:26:27, 198.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1894.6169261103091
INFO:root:current train perplexity4.497499942779541
INFO:root:current mean train loss 1897.6956634521484
INFO:root:current train perplexity4.503791332244873
INFO:root:current mean train loss 1896.1335645701788
INFO:root:current train perplexity4.500907897949219
INFO:root:current mean train loss 1901.8739437181123
INFO:root:current train perplexity4.50834321975708
INFO:root:current mean train loss 1904.20437423582
INFO:root:current train perplexity4.515742301940918
INFO:root:current mean train loss 1909.328528120711
INFO:root:current train perplexity4.523783206939697
INFO:root:current mean train loss 1909.8998331940932
INFO:root:current train perplexity4.522653102874756
INFO:root:current mean train loss 1911.593451914161
INFO:root:current train perplexity4.5283684730529785
INFO:root:current mean train loss 1910.2817098164237
INFO:root:current train perplexity4.5231709480285645
INFO:root:current mean train loss 1915.2862242421795
INFO:root:current train perplexity4.527721405029297
INFO:root:current mean train loss 1915.5899269187844
INFO:root:current train perplexity4.527835845947266
INFO:root:current mean train loss 1916.2062860271274
INFO:root:current train perplexity4.529398441314697
INFO:root:current mean train loss 1916.3878626395306
INFO:root:current train perplexity4.528859615325928
INFO:root:current mean train loss 1915.6583049379547
INFO:root:current train perplexity4.528217792510986
INFO:root:current mean train loss 1915.5851162253373
INFO:root:current train perplexity4.528140544891357
INFO:root:current mean train loss 1916.5747528076172
INFO:root:current train perplexity4.530301094055176
INFO:root:current mean train loss 1917.117303221502
INFO:root:current train perplexity4.534222602844238
INFO:root:current mean train loss 1917.1707337924413
INFO:root:current train perplexity4.535643577575684
INFO:root:current mean train loss 1918.0907696004147
INFO:root:current train perplexity4.540089130401611

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.10s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.10s/it]
INFO:root:final mean train loss: 1918.2473646540025
INFO:root:final train perplexity: 4.539628982543945
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.28s/it]
INFO:root:eval mean loss: 2067.8456052956008
INFO:root:eval perplexity: 5.324676990509033
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/30
 15%|â–ˆâ–Œ        | 30/200 [1:39:22<9:23:04, 198.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1824.690890842014
INFO:root:current train perplexity4.4183669090271
INFO:root:current mean train loss 1887.554985396359
INFO:root:current train perplexity4.439579963684082
INFO:root:current mean train loss 1887.9307738673745
INFO:root:current train perplexity4.458338260650635
INFO:root:current mean train loss 1889.5105616498533
INFO:root:current train perplexity4.447865009307861
INFO:root:current mean train loss 1892.4367559381685
INFO:root:current train perplexity4.457447052001953
INFO:root:current mean train loss 1897.2943990591232
INFO:root:current train perplexity4.4611897468566895
INFO:root:current mean train loss 1897.7031027507312
INFO:root:current train perplexity4.464987754821777
INFO:root:current mean train loss 1898.8575804458856
INFO:root:current train perplexity4.468132495880127
INFO:root:current mean train loss 1897.6968248742178
INFO:root:current train perplexity4.470994472503662
INFO:root:current mean train loss 1898.9066735530976
INFO:root:current train perplexity4.475765228271484
INFO:root:current mean train loss 1900.7170578320506
INFO:root:current train perplexity4.480217456817627
INFO:root:current mean train loss 1903.2921361622239
INFO:root:current train perplexity4.489876747131348
INFO:root:current mean train loss 1903.0772234567244
INFO:root:current train perplexity4.488191604614258
INFO:root:current mean train loss 1903.913875524464
INFO:root:current train perplexity4.490026473999023
INFO:root:current mean train loss 1904.3977963059738
INFO:root:current train perplexity4.492812156677246
INFO:root:current mean train loss 1906.2441055166398
INFO:root:current train perplexity4.49705696105957
INFO:root:current mean train loss 1906.8279411737929
INFO:root:current train perplexity4.499631881713867
INFO:root:current mean train loss 1908.9664692351384
INFO:root:current train perplexity4.500568389892578
INFO:root:current mean train loss 1909.0273897035008
INFO:root:current train perplexity4.503335952758789
INFO:root:current mean train loss 1909.1075728482886
INFO:root:current train perplexity4.505242347717285

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.14s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.14s/it]
INFO:root:final mean train loss: 1908.689235577605
INFO:root:final train perplexity: 4.505537509918213
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it]
INFO:root:eval mean loss: 2068.1036502486427
INFO:root:eval perplexity: 5.325789451599121
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/31
 16%|â–ˆâ–Œ        | 31/200 [1:42:42<9:20:26, 198.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1890.62204683744
INFO:root:current train perplexity4.3903303146362305
INFO:root:current mean train loss 1895.6133820064483
INFO:root:current train perplexity4.448645114898682
INFO:root:current mean train loss 1883.08018021035
INFO:root:current train perplexity4.428540229797363
INFO:root:current mean train loss 1889.1916799720811
INFO:root:current train perplexity4.431249141693115
INFO:root:current mean train loss 1889.2759250408046
INFO:root:current train perplexity4.443426132202148
INFO:root:current mean train loss 1891.211414641754
INFO:root:current train perplexity4.446195125579834
INFO:root:current mean train loss 1891.342984038039
INFO:root:current train perplexity4.452030181884766
INFO:root:current mean train loss 1895.0633521382144
INFO:root:current train perplexity4.454136848449707
INFO:root:current mean train loss 1897.173596398305
INFO:root:current train perplexity4.457775115966797
INFO:root:current mean train loss 1898.2811246340527
INFO:root:current train perplexity4.458421230316162
INFO:root:current mean train loss 1896.3622797023484
INFO:root:current train perplexity4.454766750335693
INFO:root:current mean train loss 1896.6098374795322
INFO:root:current train perplexity4.454421520233154
INFO:root:current mean train loss 1895.4038301004282
INFO:root:current train perplexity4.454256057739258
INFO:root:current mean train loss 1896.4899720987403
INFO:root:current train perplexity4.458156585693359
INFO:root:current mean train loss 1897.7205436460433
INFO:root:current train perplexity4.463017463684082
INFO:root:current mean train loss 1898.951106440193
INFO:root:current train perplexity4.469810962677002
INFO:root:current mean train loss 1900.5736392538486
INFO:root:current train perplexity4.472882270812988
INFO:root:current mean train loss 1900.9436496279332
INFO:root:current train perplexity4.474475383758545
INFO:root:current mean train loss 1901.5259365988456
INFO:root:current train perplexity4.475987911224365
INFO:root:current mean train loss 1901.3435670212916
INFO:root:current train perplexity4.4764509201049805

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.20s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.20s/it]
INFO:root:final mean train loss: 1900.5490678026408
INFO:root:final train perplexity: 4.476705074310303
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it]
INFO:root:eval mean loss: 2064.202490840398
INFO:root:eval perplexity: 5.309012413024902
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/32
 16%|â–ˆâ–Œ        | 32/200 [1:46:01<9:17:02, 198.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1872.6309104742006
INFO:root:current train perplexity4.445724010467529
INFO:root:current mean train loss 1892.3357761623142
INFO:root:current train perplexity4.448063850402832
INFO:root:current mean train loss 1881.5928101088284
INFO:root:current train perplexity4.419741153717041
INFO:root:current mean train loss 1880.5038436133382
INFO:root:current train perplexity4.422255992889404
INFO:root:current mean train loss 1882.261918250917
INFO:root:current train perplexity4.422239780426025
INFO:root:current mean train loss 1883.1062360169917
INFO:root:current train perplexity4.42383337020874
INFO:root:current mean train loss 1883.5132445339839
INFO:root:current train perplexity4.4274582862854
INFO:root:current mean train loss 1881.3702938033625
INFO:root:current train perplexity4.425725936889648
INFO:root:current mean train loss 1883.2785566336743
INFO:root:current train perplexity4.429929733276367
INFO:root:current mean train loss 1884.0902916431933
INFO:root:current train perplexity4.429962635040283
INFO:root:current mean train loss 1885.447620249206
INFO:root:current train perplexity4.432328701019287
INFO:root:current mean train loss 1885.7764725972975
INFO:root:current train perplexity4.430993556976318
INFO:root:current mean train loss 1886.992498322638
INFO:root:current train perplexity4.435451030731201
INFO:root:current mean train loss 1888.4630049693433
INFO:root:current train perplexity4.4356689453125
INFO:root:current mean train loss 1888.4902785334914
INFO:root:current train perplexity4.437266826629639
INFO:root:current mean train loss 1888.673176318581
INFO:root:current train perplexity4.439950466156006
INFO:root:current mean train loss 1890.7193688214443
INFO:root:current train perplexity4.443706512451172
INFO:root:current mean train loss 1891.1201976572586
INFO:root:current train perplexity4.44425106048584
INFO:root:current mean train loss 1892.4181049945105
INFO:root:current train perplexity4.446434020996094
INFO:root:current mean train loss 1893.0892065718685
INFO:root:current train perplexity4.4487385749816895

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.30s/it]
INFO:root:final mean train loss: 1892.5092625697334
INFO:root:final train perplexity: 4.448409557342529
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it]
INFO:root:eval mean loss: 2065.110097898659
INFO:root:eval perplexity: 5.3129096031188965
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/33
 16%|â–ˆâ–‹        | 33/200 [1:49:20<9:13:41, 198.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1861.1115498860677
INFO:root:current train perplexity4.33972692489624
INFO:root:current mean train loss 1869.2898078918456
INFO:root:current train perplexity4.368824005126953
INFO:root:current mean train loss 1860.769184758113
INFO:root:current train perplexity4.365461349487305
INFO:root:current mean train loss 1868.0109924316407
INFO:root:current train perplexity4.362772464752197
INFO:root:current mean train loss 1867.5235837190048
INFO:root:current train perplexity4.366836071014404
INFO:root:current mean train loss 1871.3311710902624
INFO:root:current train perplexity4.3780131340026855
INFO:root:current mean train loss 1872.2587578051018
INFO:root:current train perplexity4.379809379577637
INFO:root:current mean train loss 1875.8904150711862
INFO:root:current train perplexity4.384139060974121
INFO:root:current mean train loss 1874.9710477607196
INFO:root:current train perplexity4.384795665740967
INFO:root:current mean train loss 1875.597084681193
INFO:root:current train perplexity4.390379905700684
INFO:root:current mean train loss 1877.3888439250443
INFO:root:current train perplexity4.398462295532227
INFO:root:current mean train loss 1876.7166944832638
INFO:root:current train perplexity4.403254985809326
INFO:root:current mean train loss 1877.1338591076078
INFO:root:current train perplexity4.405888557434082
INFO:root:current mean train loss 1877.1044274722829
INFO:root:current train perplexity4.406506538391113
INFO:root:current mean train loss 1879.7206103181186
INFO:root:current train perplexity4.40897798538208
INFO:root:current mean train loss 1881.4040606376452
INFO:root:current train perplexity4.411149024963379
INFO:root:current mean train loss 1882.330090920322
INFO:root:current train perplexity4.413867950439453
INFO:root:current mean train loss 1882.4684479453347
INFO:root:current train perplexity4.416996002197266
INFO:root:current mean train loss 1883.2629114294564
INFO:root:current train perplexity4.4188923835754395
INFO:root:current mean train loss 1884.641626163405
INFO:root:current train perplexity4.419380187988281

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.32s/it]
INFO:root:final mean train loss: 1883.922771290343
INFO:root:final train perplexity: 4.4183878898620605
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it]
INFO:root:eval mean loss: 2065.5957208728114
INFO:root:eval perplexity: 5.314996242523193
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/34
 17%|â–ˆâ–‹        | 34/200 [1:52:39<9:10:24, 198.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1854.125077681108
INFO:root:current train perplexity4.325352668762207
INFO:root:current mean train loss 1862.6797412936971
INFO:root:current train perplexity4.343283176422119
INFO:root:current mean train loss 1864.413193905827
INFO:root:current train perplexity4.348715305328369
INFO:root:current mean train loss 1866.5699611835835
INFO:root:current train perplexity4.358653545379639
INFO:root:current mean train loss 1861.6107042100693
INFO:root:current train perplexity4.349210262298584
INFO:root:current mean train loss 1864.715580614642
INFO:root:current train perplexity4.356521129608154
INFO:root:current mean train loss 1862.8134096672475
INFO:root:current train perplexity4.354846954345703
INFO:root:current mean train loss 1865.4520004449203
INFO:root:current train perplexity4.3603010177612305
INFO:root:current mean train loss 1867.7517055046055
INFO:root:current train perplexity4.360988616943359
INFO:root:current mean train loss 1870.3209866979594
INFO:root:current train perplexity4.369396686553955
INFO:root:current mean train loss 1870.263064583696
INFO:root:current train perplexity4.372320652008057
INFO:root:current mean train loss 1872.0603111351356
INFO:root:current train perplexity4.377138614654541
INFO:root:current mean train loss 1874.3438730262271
INFO:root:current train perplexity4.381542205810547
INFO:root:current mean train loss 1875.5856926501792
INFO:root:current train perplexity4.381402492523193
INFO:root:current mean train loss 1875.4907791871192
INFO:root:current train perplexity4.383321762084961
INFO:root:current mean train loss 1876.3419749877387
INFO:root:current train perplexity4.385188579559326
INFO:root:current mean train loss 1876.2443473511107
INFO:root:current train perplexity4.387826442718506
INFO:root:current mean train loss 1876.556154954145
INFO:root:current train perplexity4.38950777053833
INFO:root:current mean train loss 1876.0384674316147
INFO:root:current train perplexity4.389322757720947
INFO:root:current mean train loss 1876.2050540443618
INFO:root:current train perplexity4.389822483062744

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.82s/it]
INFO:root:final mean train loss: 1875.7936347811378
INFO:root:final train perplexity: 4.390151023864746
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it]
INFO:root:eval mean loss: 2064.4577892114085
INFO:root:eval perplexity: 5.310108661651611
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/35
 18%|â–ˆâ–Š        | 35/200 [1:55:57<9:06:39, 198.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1854.38450330369
INFO:root:current train perplexity4.320756912231445
INFO:root:current mean train loss 1861.610199918452
INFO:root:current train perplexity4.331956386566162
INFO:root:current mean train loss 1863.7803299053996
INFO:root:current train perplexity4.337047100067139
INFO:root:current mean train loss 1870.6380952941586
INFO:root:current train perplexity4.339872360229492
INFO:root:current mean train loss 1866.3555433759805
INFO:root:current train perplexity4.3349690437316895
INFO:root:current mean train loss 1863.984193949587
INFO:root:current train perplexity4.338375091552734
INFO:root:current mean train loss 1863.8331042023144
INFO:root:current train perplexity4.338130950927734
INFO:root:current mean train loss 1866.3152954716527
INFO:root:current train perplexity4.3455810546875
INFO:root:current mean train loss 1867.6538049070627
INFO:root:current train perplexity4.350335597991943
INFO:root:current mean train loss 1865.9564511089977
INFO:root:current train perplexity4.351603031158447
INFO:root:current mean train loss 1864.902712638897
INFO:root:current train perplexity4.355267524719238
INFO:root:current mean train loss 1865.321673164815
INFO:root:current train perplexity4.35551643371582
INFO:root:current mean train loss 1866.9126483144864
INFO:root:current train perplexity4.358502388000488
INFO:root:current mean train loss 1867.0215891067746
INFO:root:current train perplexity4.359857559204102
INFO:root:current mean train loss 1868.288948191856
INFO:root:current train perplexity4.360856533050537
INFO:root:current mean train loss 1868.2862479905114
INFO:root:current train perplexity4.358112335205078
INFO:root:current mean train loss 1868.0951121576843
INFO:root:current train perplexity4.360547065734863
INFO:root:current mean train loss 1868.7998061844603
INFO:root:current train perplexity4.362213611602783
INFO:root:current mean train loss 1869.194893245838
INFO:root:current train perplexity4.364053726196289

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 189.00s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 189.00s/it]
INFO:root:final mean train loss: 1868.26590016606
INFO:root:final train perplexity: 4.364165306091309
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it]
INFO:root:eval mean loss: 2067.6605397758753
INFO:root:eval perplexity: 5.323879718780518
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/36
 18%|â–ˆâ–Š        | 36/200 [1:59:16<9:03:13, 198.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1823.204423384233
INFO:root:current train perplexity4.158640384674072
INFO:root:current mean train loss 1843.3729292036178
INFO:root:current train perplexity4.253998756408691
INFO:root:current mean train loss 1841.345882470009
INFO:root:current train perplexity4.257415294647217
INFO:root:current mean train loss 1845.5562390882485
INFO:root:current train perplexity4.277617931365967
INFO:root:current mean train loss 1846.779237770396
INFO:root:current train perplexity4.286415100097656
INFO:root:current mean train loss 1849.5497010113443
INFO:root:current train perplexity4.300978660583496
INFO:root:current mean train loss 1851.936495267428
INFO:root:current train perplexity4.30573844909668
INFO:root:current mean train loss 1852.5258583036657
INFO:root:current train perplexity4.315752983093262
INFO:root:current mean train loss 1853.1386213008632
INFO:root:current train perplexity4.320549964904785
INFO:root:current mean train loss 1854.3158488268386
INFO:root:current train perplexity4.3211894035339355
INFO:root:current mean train loss 1856.0472150098913
INFO:root:current train perplexity4.320368766784668
INFO:root:current mean train loss 1856.2837920510801
INFO:root:current train perplexity4.3226752281188965
INFO:root:current mean train loss 1855.7719639873426
INFO:root:current train perplexity4.322488784790039
INFO:root:current mean train loss 1856.1529167635035
INFO:root:current train perplexity4.322105407714844
INFO:root:current mean train loss 1857.5307605940766
INFO:root:current train perplexity4.326725482940674
INFO:root:current mean train loss 1859.0295919119167
INFO:root:current train perplexity4.332140922546387
INFO:root:current mean train loss 1859.1681426490486
INFO:root:current train perplexity4.331732749938965
INFO:root:current mean train loss 1859.2407069604762
INFO:root:current train perplexity4.330897808074951
INFO:root:current mean train loss 1859.9014334465376
INFO:root:current train perplexity4.332622051239014
INFO:root:current mean train loss 1860.277640461984
INFO:root:current train perplexity4.3335747718811035

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.28s/it]
INFO:root:final mean train loss: 1860.1565959274437
INFO:root:final train perplexity: 4.336343288421631
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.33s/it]
INFO:root:eval mean loss: 2066.425884706754
INFO:root:eval perplexity: 5.318566799163818
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/37
 18%|â–ˆâ–Š        | 37/200 [2:02:35<9:00:01, 198.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1848.4105093819755
INFO:root:current train perplexity4.271175861358643
INFO:root:current mean train loss 1848.15065574646
INFO:root:current train perplexity4.304433345794678
INFO:root:current mean train loss 1834.1174439547356
INFO:root:current train perplexity4.2814507484436035
INFO:root:current mean train loss 1840.1849659245188
INFO:root:current train perplexity4.27653694152832
INFO:root:current mean train loss 1839.8861372047495
INFO:root:current train perplexity4.2766032218933105
INFO:root:current mean train loss 1842.876189029578
INFO:root:current train perplexity4.280098915100098
INFO:root:current mean train loss 1843.5625480117312
INFO:root:current train perplexity4.280659198760986
INFO:root:current mean train loss 1845.4630256065955
INFO:root:current train perplexity4.280786037445068
INFO:root:current mean train loss 1846.3785727680593
INFO:root:current train perplexity4.285757541656494
INFO:root:current mean train loss 1848.576828660636
INFO:root:current train perplexity4.292163848876953
INFO:root:current mean train loss 1847.390666560904
INFO:root:current train perplexity4.291558742523193
INFO:root:current mean train loss 1848.84795027929
INFO:root:current train perplexity4.295436382293701
INFO:root:current mean train loss 1850.3484985152752
INFO:root:current train perplexity4.3000054359436035
INFO:root:current mean train loss 1851.4779107886625
INFO:root:current train perplexity4.302036762237549
INFO:root:current mean train loss 1850.6368194494594
INFO:root:current train perplexity4.30283260345459
INFO:root:current mean train loss 1851.5624655678635
INFO:root:current train perplexity4.305144309997559
INFO:root:current mean train loss 1851.3072270573796
INFO:root:current train perplexity4.305047035217285
INFO:root:current mean train loss 1851.5942308637832
INFO:root:current train perplexity4.304656505584717
INFO:root:current mean train loss 1852.1798426922353
INFO:root:current train perplexity4.307328224182129
INFO:root:current mean train loss 1853.3838671292506
INFO:root:current train perplexity4.30990743637085

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.98s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.98s/it]
INFO:root:final mean train loss: 1852.7934005206364
INFO:root:final train perplexity: 4.311234951019287
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2065.3235222566213
INFO:root:eval perplexity: 5.313827037811279
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/38
 19%|â–ˆâ–‰        | 38/200 [2:05:53<8:56:32, 198.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1815.748668077257
INFO:root:current train perplexity4.221889495849609
INFO:root:current mean train loss 1817.6193157327587
INFO:root:current train perplexity4.223626613616943
INFO:root:current mean train loss 1835.6508166254782
INFO:root:current train perplexity4.252893447875977
INFO:root:current mean train loss 1840.529356671762
INFO:root:current train perplexity4.267991542816162
INFO:root:current mean train loss 1843.1329087846734
INFO:root:current train perplexity4.275870323181152
INFO:root:current mean train loss 1844.061765562285
INFO:root:current train perplexity4.275230407714844
INFO:root:current mean train loss 1843.8958329548207
INFO:root:current train perplexity4.280666828155518
INFO:root:current mean train loss 1844.6354870687396
INFO:root:current train perplexity4.2829670906066895
INFO:root:current mean train loss 1843.1209374422153
INFO:root:current train perplexity4.277076721191406
INFO:root:current mean train loss 1844.4463935650215
INFO:root:current train perplexity4.27693510055542
INFO:root:current mean train loss 1843.918106356534
INFO:root:current train perplexity4.275660991668701
INFO:root:current mean train loss 1843.1972031505868
INFO:root:current train perplexity4.274799346923828
INFO:root:current mean train loss 1843.8929762016817
INFO:root:current train perplexity4.276551723480225
INFO:root:current mean train loss 1843.3689768964916
INFO:root:current train perplexity4.275116443634033
INFO:root:current mean train loss 1844.264561256488
INFO:root:current train perplexity4.278216361999512
INFO:root:current mean train loss 1844.2250410851536
INFO:root:current train perplexity4.2807841300964355
INFO:root:current mean train loss 1843.9946476805899
INFO:root:current train perplexity4.2791361808776855
INFO:root:current mean train loss 1844.6571186929173
INFO:root:current train perplexity4.27933931350708
INFO:root:current mean train loss 1845.4263685107553
INFO:root:current train perplexity4.282102584838867
INFO:root:current mean train loss 1846.210694112508
INFO:root:current train perplexity4.28516960144043

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.86s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.86s/it]
INFO:root:final mean train loss: 1845.2065558960146
INFO:root:final train perplexity: 4.285515308380127
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.33s/it]
INFO:root:eval mean loss: 2066.8518784976177
INFO:root:eval perplexity: 5.320399761199951
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/39
 20%|â–ˆâ–‰        | 39/200 [2:09:12<8:53:01, 198.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1818.505617203251
INFO:root:current train perplexity4.24066162109375
INFO:root:current mean train loss 1831.2542792426216
INFO:root:current train perplexity4.234351634979248
INFO:root:current mean train loss 1825.7527302749284
INFO:root:current train perplexity4.217196464538574
INFO:root:current mean train loss 1827.2847374341763
INFO:root:current train perplexity4.220193862915039
INFO:root:current mean train loss 1829.7929735059863
INFO:root:current train perplexity4.224588871002197
INFO:root:current mean train loss 1831.0256997105066
INFO:root:current train perplexity4.2298078536987305
INFO:root:current mean train loss 1831.816082265802
INFO:root:current train perplexity4.236802101135254
INFO:root:current mean train loss 1832.0886739896039
INFO:root:current train perplexity4.2375407218933105
INFO:root:current mean train loss 1831.4605204500344
INFO:root:current train perplexity4.23728609085083
INFO:root:current mean train loss 1831.4497490325737
INFO:root:current train perplexity4.238314151763916
INFO:root:current mean train loss 1832.850458005054
INFO:root:current train perplexity4.243809700012207
INFO:root:current mean train loss 1831.682812058782
INFO:root:current train perplexity4.241178035736084
INFO:root:current mean train loss 1832.4047413386181
INFO:root:current train perplexity4.24611759185791
INFO:root:current mean train loss 1833.00891373196
INFO:root:current train perplexity4.249471187591553
INFO:root:current mean train loss 1834.288582234311
INFO:root:current train perplexity4.251132011413574
INFO:root:current mean train loss 1835.2108959241957
INFO:root:current train perplexity4.255137920379639
INFO:root:current mean train loss 1836.0953931751092
INFO:root:current train perplexity4.258520603179932
INFO:root:current mean train loss 1836.8994606875399
INFO:root:current train perplexity4.259080410003662
INFO:root:current mean train loss 1837.6396525021398
INFO:root:current train perplexity4.258645534515381
INFO:root:current mean train loss 1838.5446282716337
INFO:root:current train perplexity4.26026725769043

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.59s/it]
INFO:root:final mean train loss: 1837.7482884301721
INFO:root:final train perplexity: 4.260382175445557
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it]
INFO:root:eval mean loss: 2067.067062486148
INFO:root:eval perplexity: 5.32132625579834
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/40
 20%|â–ˆâ–ˆ        | 40/200 [2:12:31<8:50:09, 198.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1817.5250599535207
INFO:root:current train perplexity4.19296407699585
INFO:root:current mean train loss 1822.3555812729137
INFO:root:current train perplexity4.198114395141602
INFO:root:current mean train loss 1811.2235308684756
INFO:root:current train perplexity4.191233158111572
INFO:root:current mean train loss 1816.9977215688903
INFO:root:current train perplexity4.2018818855285645
INFO:root:current mean train loss 1819.6313186040254
INFO:root:current train perplexity4.205350399017334
INFO:root:current mean train loss 1818.8852416781358
INFO:root:current train perplexity4.205202102661133
INFO:root:current mean train loss 1819.8873335960511
INFO:root:current train perplexity4.2115583419799805
INFO:root:current mean train loss 1821.0606455968189
INFO:root:current train perplexity4.20948600769043
INFO:root:current mean train loss 1824.2069402874804
INFO:root:current train perplexity4.215470790863037
INFO:root:current mean train loss 1824.5575068379326
INFO:root:current train perplexity4.2181782722473145
INFO:root:current mean train loss 1825.4619302404933
INFO:root:current train perplexity4.218172550201416
INFO:root:current mean train loss 1825.294297649458
INFO:root:current train perplexity4.219186305999756
INFO:root:current mean train loss 1825.2630425304803
INFO:root:current train perplexity4.220221519470215
INFO:root:current mean train loss 1827.1109689072132
INFO:root:current train perplexity4.223471641540527
INFO:root:current mean train loss 1827.5680941052337
INFO:root:current train perplexity4.22599983215332
INFO:root:current mean train loss 1829.055615512686
INFO:root:current train perplexity4.229372501373291
INFO:root:current mean train loss 1829.5671800405514
INFO:root:current train perplexity4.233745574951172
INFO:root:current mean train loss 1829.5144276954004
INFO:root:current train perplexity4.233745098114014
INFO:root:current mean train loss 1830.2193117573136
INFO:root:current train perplexity4.2353129386901855
INFO:root:current mean train loss 1831.2053270152026
INFO:root:current train perplexity4.236136436462402

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.44s/it]
INFO:root:final mean train loss: 1830.56632031447
INFO:root:final train perplexity: 4.236319065093994
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it]
INFO:root:eval mean loss: 2068.416233793218
INFO:root:eval perplexity: 5.3271355628967285
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/41
 20%|â–ˆâ–ˆ        | 41/200 [2:15:50<8:47:01, 198.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1801.4648043314617
INFO:root:current train perplexity4.154777526855469
INFO:root:current mean train loss 1808.7414600605866
INFO:root:current train perplexity4.1498332023620605
INFO:root:current mean train loss 1811.10338984309
INFO:root:current train perplexity4.153502941131592
INFO:root:current mean train loss 1816.238802514895
INFO:root:current train perplexity4.1643171310424805
INFO:root:current mean train loss 1816.0434009182839
INFO:root:current train perplexity4.166762351989746
INFO:root:current mean train loss 1817.1479350864488
INFO:root:current train perplexity4.175111293792725
INFO:root:current mean train loss 1818.8230892488325
INFO:root:current train perplexity4.179565906524658
INFO:root:current mean train loss 1819.991138860808
INFO:root:current train perplexity4.186288356781006
INFO:root:current mean train loss 1819.390228816441
INFO:root:current train perplexity4.190235137939453
INFO:root:current mean train loss 1820.7529587343515
INFO:root:current train perplexity4.1958112716674805
INFO:root:current mean train loss 1819.3449514347271
INFO:root:current train perplexity4.197206974029541
INFO:root:current mean train loss 1819.792556405466
INFO:root:current train perplexity4.199359893798828
INFO:root:current mean train loss 1821.7247706283758
INFO:root:current train perplexity4.205071449279785
INFO:root:current mean train loss 1821.8392917228632
INFO:root:current train perplexity4.205687522888184
INFO:root:current mean train loss 1821.5785729719355
INFO:root:current train perplexity4.206231594085693
INFO:root:current mean train loss 1821.9405860996485
INFO:root:current train perplexity4.21019172668457
INFO:root:current mean train loss 1822.84026948461
INFO:root:current train perplexity4.211256980895996
INFO:root:current mean train loss 1822.9361670139372
INFO:root:current train perplexity4.210244178771973
INFO:root:current mean train loss 1824.488348980996
INFO:root:current train perplexity4.212841987609863

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.22s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.22s/it]
INFO:root:final mean train loss: 1823.9372551822325
INFO:root:final train perplexity: 4.214229106903076
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it]
INFO:root:eval mean loss: 2068.06858403632
INFO:root:eval perplexity: 5.325638294219971
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/42
 21%|â–ˆâ–ˆ        | 42/200 [2:19:09<8:43:40, 198.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1838.7163837139424
INFO:root:current train perplexity4.260882377624512
INFO:root:current mean train loss 1810.2251289840294
INFO:root:current train perplexity4.177770137786865
INFO:root:current mean train loss 1807.203204087808
INFO:root:current train perplexity4.18049430847168
INFO:root:current mean train loss 1811.310041823707
INFO:root:current train perplexity4.185684680938721
INFO:root:current mean train loss 1814.3215766518804
INFO:root:current train perplexity4.181591033935547
INFO:root:current mean train loss 1815.8901493303028
INFO:root:current train perplexity4.175388336181641
INFO:root:current mean train loss 1816.453336084064
INFO:root:current train perplexity4.179409027099609
INFO:root:current mean train loss 1815.8550326867658
INFO:root:current train perplexity4.179121494293213
INFO:root:current mean train loss 1817.3287952606088
INFO:root:current train perplexity4.1832194328308105
INFO:root:current mean train loss 1816.5904923404555
INFO:root:current train perplexity4.185074806213379
INFO:root:current mean train loss 1816.1075835910508
INFO:root:current train perplexity4.186661720275879
INFO:root:current mean train loss 1816.8675123627725
INFO:root:current train perplexity4.186990261077881
INFO:root:current mean train loss 1816.2860472727098
INFO:root:current train perplexity4.1886091232299805
INFO:root:current mean train loss 1816.8068541783189
INFO:root:current train perplexity4.186061859130859
INFO:root:current mean train loss 1816.840190954944
INFO:root:current train perplexity4.186596870422363
INFO:root:current mean train loss 1816.416232334094
INFO:root:current train perplexity4.1872687339782715
INFO:root:current mean train loss 1816.9024566631472
INFO:root:current train perplexity4.189321994781494
INFO:root:current mean train loss 1816.4558110457028
INFO:root:current train perplexity4.19089412689209
INFO:root:current mean train loss 1818.3149206011058
INFO:root:current train perplexity4.195216655731201
INFO:root:current mean train loss 1817.6943430843244
INFO:root:current train perplexity4.192673683166504

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.44s/it]
INFO:root:final mean train loss: 1817.2054286077657
INFO:root:final train perplexity: 4.1919145584106445
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2069.7336239645665
INFO:root:eval perplexity: 5.332813739776611
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/43
 22%|â–ˆâ–ˆâ–       | 43/200 [2:22:28<8:40:30, 198.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1783.3525024414062
INFO:root:current train perplexity4.0974273681640625
INFO:root:current mean train loss 1790.2516216571514
INFO:root:current train perplexity4.1102447509765625
INFO:root:current mean train loss 1787.889144764776
INFO:root:current train perplexity4.112862586975098
INFO:root:current mean train loss 1794.2480313387784
INFO:root:current train perplexity4.12960147857666
INFO:root:current mean train loss 1796.641381552053
INFO:root:current train perplexity4.139177322387695
INFO:root:current mean train loss 1795.3578263192805
INFO:root:current train perplexity4.131084442138672
INFO:root:current mean train loss 1797.4016498868427
INFO:root:current train perplexity4.136640548706055
INFO:root:current mean train loss 1797.671622832834
INFO:root:current train perplexity4.13667631149292
INFO:root:current mean train loss 1798.9330682593657
INFO:root:current train perplexity4.142581939697266
INFO:root:current mean train loss 1800.7573055800572
INFO:root:current train perplexity4.144099235534668
INFO:root:current mean train loss 1803.2773746823802
INFO:root:current train perplexity4.148883819580078
INFO:root:current mean train loss 1803.045376667934
INFO:root:current train perplexity4.149768352508545
INFO:root:current mean train loss 1803.8752744100927
INFO:root:current train perplexity4.153195858001709
INFO:root:current mean train loss 1806.542932496035
INFO:root:current train perplexity4.1549248695373535
INFO:root:current mean train loss 1807.255659965035
INFO:root:current train perplexity4.158416748046875
INFO:root:current mean train loss 1808.7237451491012
INFO:root:current train perplexity4.160505294799805
INFO:root:current mean train loss 1808.4076680376486
INFO:root:current train perplexity4.161684989929199
INFO:root:current mean train loss 1809.671741428265
INFO:root:current train perplexity4.165561676025391
INFO:root:current mean train loss 1809.8753415967597
INFO:root:current train perplexity4.1648335456848145
INFO:root:current mean train loss 1810.4166724644795
INFO:root:current train perplexity4.166889190673828

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.50s/it]
INFO:root:final mean train loss: 1810.045800128731
INFO:root:final train perplexity: 4.16831111907959
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2070.4210174776986
INFO:root:eval perplexity: 5.335779666900635
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/44
 22%|â–ˆâ–ˆâ–       | 44/200 [2:25:47<8:37:18, 198.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1813.0846518658577
INFO:root:current train perplexity4.09883975982666
INFO:root:current mean train loss 1785.0814748751063
INFO:root:current train perplexity4.09348201751709
INFO:root:current mean train loss 1789.7621274631515
INFO:root:current train perplexity4.099342346191406
INFO:root:current mean train loss 1791.7593374994372
INFO:root:current train perplexity4.127987384796143
INFO:root:current mean train loss 1790.6969720554566
INFO:root:current train perplexity4.125696659088135
INFO:root:current mean train loss 1795.4033209819897
INFO:root:current train perplexity4.12180233001709
INFO:root:current mean train loss 1795.6984229345778
INFO:root:current train perplexity4.123574256896973
INFO:root:current mean train loss 1795.2526647932877
INFO:root:current train perplexity4.121528625488281
INFO:root:current mean train loss 1796.4031088872953
INFO:root:current train perplexity4.123939514160156
INFO:root:current mean train loss 1798.38744601579
INFO:root:current train perplexity4.1301445960998535
INFO:root:current mean train loss 1799.8466545039398
INFO:root:current train perplexity4.131799221038818
INFO:root:current mean train loss 1800.2964010862236
INFO:root:current train perplexity4.13243293762207
INFO:root:current mean train loss 1800.1381512896576
INFO:root:current train perplexity4.1323089599609375
INFO:root:current mean train loss 1799.8771603817222
INFO:root:current train perplexity4.132798194885254
INFO:root:current mean train loss 1800.7065996593167
INFO:root:current train perplexity4.136589527130127
INFO:root:current mean train loss 1801.137476027821
INFO:root:current train perplexity4.138552188873291
INFO:root:current mean train loss 1801.004341760113
INFO:root:current train perplexity4.138082981109619
INFO:root:current mean train loss 1802.0870189437474
INFO:root:current train perplexity4.139524936676025
INFO:root:current mean train loss 1803.074699761226
INFO:root:current train perplexity4.142752647399902
INFO:root:current mean train loss 1803.5088007240706
INFO:root:current train perplexity4.145081043243408

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.08s/it]
INFO:root:final mean train loss: 1803.517987519157
INFO:root:final train perplexity: 4.146906852722168
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.34s/it]
INFO:root:eval mean loss: 2071.909750214705
INFO:root:eval perplexity: 5.3422064781188965
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/45
 22%|â–ˆâ–ˆâ–Ž       | 45/200 [2:29:06<8:33:47, 198.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1805.418743133545
INFO:root:current train perplexity4.0973005294799805
INFO:root:current mean train loss 1787.331166337176
INFO:root:current train perplexity4.0868449211120605
INFO:root:current mean train loss 1781.7228740345347
INFO:root:current train perplexity4.074165344238281
INFO:root:current mean train loss 1785.1883306817695
INFO:root:current train perplexity4.075083255767822
INFO:root:current mean train loss 1780.3112861370219
INFO:root:current train perplexity4.071832180023193
INFO:root:current mean train loss 1782.877955659907
INFO:root:current train perplexity4.082944869995117
INFO:root:current mean train loss 1784.5147326366011
INFO:root:current train perplexity4.083249568939209
INFO:root:current mean train loss 1785.5209062985725
INFO:root:current train perplexity4.088080406188965
INFO:root:current mean train loss 1785.6493613631637
INFO:root:current train perplexity4.08826208114624
INFO:root:current mean train loss 1788.9763753424047
INFO:root:current train perplexity4.092642307281494
INFO:root:current mean train loss 1789.7098060550547
INFO:root:current train perplexity4.094770431518555
INFO:root:current mean train loss 1790.470609579709
INFO:root:current train perplexity4.098206043243408
INFO:root:current mean train loss 1790.1489700124234
INFO:root:current train perplexity4.100405693054199
INFO:root:current mean train loss 1792.5732587439572
INFO:root:current train perplexity4.105354309082031
INFO:root:current mean train loss 1792.6541944826888
INFO:root:current train perplexity4.108349323272705
INFO:root:current mean train loss 1793.9017474474504
INFO:root:current train perplexity4.112957000732422
INFO:root:current mean train loss 1794.61783012977
INFO:root:current train perplexity4.116118907928467
INFO:root:current mean train loss 1794.4374102464878
INFO:root:current train perplexity4.115682125091553
INFO:root:current mean train loss 1794.7455553701507
INFO:root:current train perplexity4.117879867553711
INFO:root:current mean train loss 1796.9572072699211
INFO:root:current train perplexity4.1229987144470215

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.15s/it]
INFO:root:final mean train loss: 1796.3974290810265
INFO:root:final train perplexity: 4.123684406280518
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it]
INFO:root:eval mean loss: 2072.9213551189882
INFO:root:eval perplexity: 5.346579551696777
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/46
 23%|â–ˆâ–ˆâ–Ž       | 46/200 [2:32:24<8:30:22, 198.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1756.350915979456
INFO:root:current train perplexity4.069419860839844
INFO:root:current mean train loss 1766.4634746741194
INFO:root:current train perplexity4.053075313568115
INFO:root:current mean train loss 1776.1181966435443
INFO:root:current train perplexity4.061554908752441
INFO:root:current mean train loss 1779.4116422397883
INFO:root:current train perplexity4.071222305297852
INFO:root:current mean train loss 1778.8655881505003
INFO:root:current train perplexity4.0675764083862305
INFO:root:current mean train loss 1781.0084501650576
INFO:root:current train perplexity4.071854591369629
INFO:root:current mean train loss 1782.014988298458
INFO:root:current train perplexity4.073659896850586
INFO:root:current mean train loss 1784.2748935909492
INFO:root:current train perplexity4.08018159866333
INFO:root:current mean train loss 1785.2289282531392
INFO:root:current train perplexity4.085572719573975
INFO:root:current mean train loss 1784.7252879167065
INFO:root:current train perplexity4.08711051940918
INFO:root:current mean train loss 1785.2842361492542
INFO:root:current train perplexity4.088850975036621
INFO:root:current mean train loss 1785.9825462192725
INFO:root:current train perplexity4.092370986938477
INFO:root:current mean train loss 1786.8720728854105
INFO:root:current train perplexity4.094359397888184
INFO:root:current mean train loss 1786.6148133605914
INFO:root:current train perplexity4.094572067260742
INFO:root:current mean train loss 1786.0768634144476
INFO:root:current train perplexity4.093211650848389
INFO:root:current mean train loss 1786.4909258751384
INFO:root:current train perplexity4.09402322769165
INFO:root:current mean train loss 1788.1430971961445
INFO:root:current train perplexity4.097211837768555
INFO:root:current mean train loss 1788.5231957582862
INFO:root:current train perplexity4.099124908447266
INFO:root:current mean train loss 1789.2219270080534
INFO:root:current train perplexity4.101195335388184
INFO:root:current mean train loss 1790.8321634260828
INFO:root:current train perplexity4.1038689613342285

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.12s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.12s/it]
INFO:root:final mean train loss: 1790.2674259218493
INFO:root:final train perplexity: 4.103796482086182
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2074.764960106383
INFO:root:eval perplexity: 5.354557037353516
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/47
 24%|â–ˆâ–ˆâ–Ž       | 47/200 [2:35:43<8:26:58, 198.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1789.4678780691963
INFO:root:current train perplexity4.054553985595703
INFO:root:current mean train loss 1792.4732937282986
INFO:root:current train perplexity4.069818496704102
INFO:root:current mean train loss 1783.1250544810455
INFO:root:current train perplexity4.066168785095215
INFO:root:current mean train loss 1781.2100670589275
INFO:root:current train perplexity4.062295913696289
INFO:root:current mean train loss 1779.461000496125
INFO:root:current train perplexity4.0625505447387695
INFO:root:current mean train loss 1777.7598842250861
INFO:root:current train perplexity4.065658092498779
INFO:root:current mean train loss 1780.3709743029751
INFO:root:current train perplexity4.0690083503723145
INFO:root:current mean train loss 1781.6394968439165
INFO:root:current train perplexity4.070568561553955
INFO:root:current mean train loss 1781.6202816697696
INFO:root:current train perplexity4.072840213775635
INFO:root:current mean train loss 1781.8840719769616
INFO:root:current train perplexity4.072260856628418
INFO:root:current mean train loss 1783.047898144887
INFO:root:current train perplexity4.078654766082764
INFO:root:current mean train loss 1782.5578730460597
INFO:root:current train perplexity4.076054096221924
INFO:root:current mean train loss 1783.6320660654312
INFO:root:current train perplexity4.078098773956299
INFO:root:current mean train loss 1783.699130908273
INFO:root:current train perplexity4.079351902008057
INFO:root:current mean train loss 1784.4367012461928
INFO:root:current train perplexity4.080420017242432
INFO:root:current mean train loss 1785.3018458131257
INFO:root:current train perplexity4.081302165985107
INFO:root:current mean train loss 1785.0398286385869
INFO:root:current train perplexity4.081700325012207
INFO:root:current mean train loss 1784.8636706122036
INFO:root:current train perplexity4.082878589630127
INFO:root:current mean train loss 1785.3315889541418
INFO:root:current train perplexity4.084897994995117

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.39s/it]
INFO:root:final mean train loss: 1784.7524098575686
INFO:root:final train perplexity: 4.085986137390137
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2075.7942903126386
INFO:root:eval perplexity: 5.3590168952941895
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/48
 24%|â–ˆâ–ˆâ–       | 48/200 [2:39:02<8:23:49, 198.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1803.094376627604
INFO:root:current train perplexity4.17288875579834
INFO:root:current mean train loss 1756.6620923913044
INFO:root:current train perplexity4.028822422027588
INFO:root:current mean train loss 1765.8955901389897
INFO:root:current train perplexity4.028907775878906
INFO:root:current mean train loss 1769.0143252418154
INFO:root:current train perplexity4.03882360458374
INFO:root:current mean train loss 1770.8999161685806
INFO:root:current train perplexity4.032699108123779
INFO:root:current mean train loss 1767.364894474363
INFO:root:current train perplexity4.029884338378906
INFO:root:current mean train loss 1769.1167482453634
INFO:root:current train perplexity4.031119346618652
INFO:root:current mean train loss 1770.5231377021416
INFO:root:current train perplexity4.0320281982421875
INFO:root:current mean train loss 1769.774346961273
INFO:root:current train perplexity4.034034252166748
INFO:root:current mean train loss 1771.9382021377646
INFO:root:current train perplexity4.039902687072754
INFO:root:current mean train loss 1771.5105678013392
INFO:root:current train perplexity4.041426181793213
INFO:root:current mean train loss 1773.0634740444577
INFO:root:current train perplexity4.045448303222656
INFO:root:current mean train loss 1774.6737269523212
INFO:root:current train perplexity4.048924446105957
INFO:root:current mean train loss 1776.5891817854385
INFO:root:current train perplexity4.051053524017334
INFO:root:current mean train loss 1776.8622072900564
INFO:root:current train perplexity4.055835723876953
INFO:root:current mean train loss 1777.020052889078
INFO:root:current train perplexity4.057651042938232
INFO:root:current mean train loss 1777.9592007758079
INFO:root:current train perplexity4.0595622062683105
INFO:root:current mean train loss 1777.1476617307078
INFO:root:current train perplexity4.05928897857666
INFO:root:current mean train loss 1777.3364197954331
INFO:root:current train perplexity4.060065269470215
INFO:root:current mean train loss 1777.6723484288307
INFO:root:current train perplexity4.060980796813965

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.08s/it]
INFO:root:final mean train loss: 1777.5826976812673
INFO:root:final train perplexity: 4.062946796417236
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.35s/it]
INFO:root:eval mean loss: 2076.888460632757
INFO:root:eval perplexity: 5.3637614250183105
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/49
 24%|â–ˆâ–ˆâ–       | 49/200 [2:42:21<8:20:24, 198.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1755.7779960632324
INFO:root:current train perplexity3.95690655708313
INFO:root:current mean train loss 1751.1399203213778
INFO:root:current train perplexity3.97115159034729
INFO:root:current mean train loss 1753.625882905105
INFO:root:current train perplexity3.9873130321502686
INFO:root:current mean train loss 1761.0421447753906
INFO:root:current train perplexity3.993195056915283
INFO:root:current mean train loss 1764.4745887473778
INFO:root:current train perplexity4.004183292388916
INFO:root:current mean train loss 1764.9465072746564
INFO:root:current train perplexity4.006228446960449
INFO:root:current mean train loss 1768.9276070896583
INFO:root:current train perplexity4.0118021965026855
INFO:root:current mean train loss 1768.3350891780333
INFO:root:current train perplexity4.016482830047607
INFO:root:current mean train loss 1769.5203100351187
INFO:root:current train perplexity4.021164417266846
INFO:root:current mean train loss 1770.7290687397315
INFO:root:current train perplexity4.026445388793945
INFO:root:current mean train loss 1771.0899189793786
INFO:root:current train perplexity4.030323028564453
INFO:root:current mean train loss 1772.4743318052258
INFO:root:current train perplexity4.033945083618164
INFO:root:current mean train loss 1771.6410711709555
INFO:root:current train perplexity4.032020092010498
INFO:root:current mean train loss 1771.4380422597892
INFO:root:current train perplexity4.032044410705566
INFO:root:current mean train loss 1771.0393572759363
INFO:root:current train perplexity4.033941268920898
INFO:root:current mean train loss 1771.4162194473624
INFO:root:current train perplexity4.037091255187988
INFO:root:current mean train loss 1772.4161426319795
INFO:root:current train perplexity4.03839111328125
INFO:root:current mean train loss 1771.8841575992576
INFO:root:current train perplexity4.041806221008301
INFO:root:current mean train loss 1771.524619223249
INFO:root:current train perplexity4.041160583496094
INFO:root:current mean train loss 1772.0887281208552
INFO:root:current train perplexity4.043853282928467

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.42s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.42s/it]
INFO:root:final mean train loss: 1771.959013399787
INFO:root:final train perplexity: 4.044967174530029
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it]
INFO:root:eval mean loss: 2079.0651431252772
INFO:root:eval perplexity: 5.373211860656738
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/50
 25%|â–ˆâ–ˆâ–Œ       | 50/200 [2:45:40<8:17:14, 198.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1752.2773885921556
INFO:root:current train perplexity3.958982467651367
INFO:root:current mean train loss 1752.9302732736473
INFO:root:current train perplexity3.9709556102752686
INFO:root:current mean train loss 1751.3956259608747
INFO:root:current train perplexity3.9806344509124756
INFO:root:current mean train loss 1752.0047554956125
INFO:root:current train perplexity3.991419553756714
INFO:root:current mean train loss 1758.174560546875
INFO:root:current train perplexity3.999809741973877
INFO:root:current mean train loss 1759.2385011544427
INFO:root:current train perplexity4.005606651306152
INFO:root:current mean train loss 1761.5556542818279
INFO:root:current train perplexity4.006226539611816
INFO:root:current mean train loss 1761.0396787187603
INFO:root:current train perplexity4.008220195770264
INFO:root:current mean train loss 1761.9184471103413
INFO:root:current train perplexity4.008456230163574
INFO:root:current mean train loss 1762.0365381270992
INFO:root:current train perplexity4.010435104370117
INFO:root:current mean train loss 1761.3556502844972
INFO:root:current train perplexity4.0090765953063965
INFO:root:current mean train loss 1762.3625731571951
INFO:root:current train perplexity4.012185573577881
INFO:root:current mean train loss 1763.1711554790706
INFO:root:current train perplexity4.014093399047852
INFO:root:current mean train loss 1763.9690490677413
INFO:root:current train perplexity4.016514301300049
INFO:root:current mean train loss 1763.516663061001
INFO:root:current train perplexity4.016574859619141
INFO:root:current mean train loss 1763.606647764813
INFO:root:current train perplexity4.019769191741943
INFO:root:current mean train loss 1763.9501926475325
INFO:root:current train perplexity4.021763324737549
INFO:root:current mean train loss 1763.6524668672278
INFO:root:current train perplexity4.022290229797363
INFO:root:current mean train loss 1764.4447506068525
INFO:root:current train perplexity4.022849082946777
INFO:root:current mean train loss 1765.1405375027057
INFO:root:current train perplexity4.0235748291015625

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.37s/it]
INFO:root:final mean train loss: 1765.341401700834
INFO:root:final train perplexity: 4.023911476135254
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2080.968760388963
INFO:root:eval perplexity: 5.381491184234619
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/51
 26%|â–ˆâ–ˆâ–Œ       | 51/200 [2:48:59<8:13:59, 198.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1759.2148566968513
INFO:root:current train perplexity3.961808204650879
INFO:root:current mean train loss 1754.955788485975
INFO:root:current train perplexity3.9597041606903076
INFO:root:current mean train loss 1754.6495668798461
INFO:root:current train perplexity3.9695754051208496
INFO:root:current mean train loss 1752.9728413566213
INFO:root:current train perplexity3.9702346324920654
INFO:root:current mean train loss 1753.2936938371781
INFO:root:current train perplexity3.9770827293395996
INFO:root:current mean train loss 1754.5813072393303
INFO:root:current train perplexity3.97574520111084
INFO:root:current mean train loss 1755.4944855744416
INFO:root:current train perplexity3.9772918224334717
INFO:root:current mean train loss 1755.908209977511
INFO:root:current train perplexity3.9771506786346436
INFO:root:current mean train loss 1754.5318542903344
INFO:root:current train perplexity3.9822630882263184
INFO:root:current mean train loss 1753.6757518065394
INFO:root:current train perplexity3.982282876968384
INFO:root:current mean train loss 1754.1471654952802
INFO:root:current train perplexity3.9841666221618652
INFO:root:current mean train loss 1756.2354357344823
INFO:root:current train perplexity3.989314556121826
INFO:root:current mean train loss 1757.2702288635157
INFO:root:current train perplexity3.9916720390319824
INFO:root:current mean train loss 1759.2334186336132
INFO:root:current train perplexity3.9964325428009033
INFO:root:current mean train loss 1759.2068260786152
INFO:root:current train perplexity3.99863600730896
INFO:root:current mean train loss 1758.081550140186
INFO:root:current train perplexity3.999011754989624
INFO:root:current mean train loss 1757.8829861534457
INFO:root:current train perplexity4.000895977020264
INFO:root:current mean train loss 1758.3836602873062
INFO:root:current train perplexity4.003230094909668
INFO:root:current mean train loss 1759.2814255823787
INFO:root:current train perplexity4.004291534423828
INFO:root:current mean train loss 1760.089046940071
INFO:root:current train perplexity4.006343841552734

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.90s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.90s/it]
INFO:root:final mean train loss: 1759.7949043308552
INFO:root:final train perplexity: 4.006348609924316
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it]
INFO:root:eval mean loss: 2082.773104620318
INFO:root:eval perplexity: 5.38934850692749
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/52
 26%|â–ˆâ–ˆâ–Œ       | 52/200 [2:52:17<8:10:24, 198.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1744.7553843302899
INFO:root:current train perplexity3.9700045585632324
INFO:root:current mean train loss 1737.6009761622695
INFO:root:current train perplexity3.9483158588409424
INFO:root:current mean train loss 1736.384273892999
INFO:root:current train perplexity3.940194606781006
INFO:root:current mean train loss 1739.6462886800343
INFO:root:current train perplexity3.9379255771636963
INFO:root:current mean train loss 1741.8352968204094
INFO:root:current train perplexity3.948364734649658
INFO:root:current mean train loss 1746.0907376061991
INFO:root:current train perplexity3.953296422958374
INFO:root:current mean train loss 1748.781508081305
INFO:root:current train perplexity3.9543168544769287
INFO:root:current mean train loss 1750.6839340814076
INFO:root:current train perplexity3.9572160243988037
INFO:root:current mean train loss 1750.827229172565
INFO:root:current train perplexity3.9608981609344482
INFO:root:current mean train loss 1750.931815224043
INFO:root:current train perplexity3.96527099609375
INFO:root:current mean train loss 1752.151345095366
INFO:root:current train perplexity3.969007968902588
INFO:root:current mean train loss 1752.984511103755
INFO:root:current train perplexity3.972930431365967
INFO:root:current mean train loss 1752.9362840540969
INFO:root:current train perplexity3.973668336868286
INFO:root:current mean train loss 1753.2643796709767
INFO:root:current train perplexity3.979280471801758
INFO:root:current mean train loss 1754.4020455296748
INFO:root:current train perplexity3.9823222160339355
INFO:root:current mean train loss 1754.1889262099999
INFO:root:current train perplexity3.983680248260498
INFO:root:current mean train loss 1753.5917686602932
INFO:root:current train perplexity3.9838948249816895
INFO:root:current mean train loss 1752.8292830590779
INFO:root:current train perplexity3.98441743850708
INFO:root:current mean train loss 1753.504471092609
INFO:root:current train perplexity3.9859156608581543
INFO:root:current mean train loss 1753.6093067009522
INFO:root:current train perplexity3.986851453781128

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.31s/it]
INFO:root:final mean train loss: 1753.6093067009522
INFO:root:final train perplexity: 3.986851453781128
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.33s/it]
INFO:root:eval mean loss: 2084.397844896249
INFO:root:eval perplexity: 5.396435260772705
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/53
 26%|â–ˆâ–ˆâ–‹       | 53/200 [2:55:36<8:07:11, 198.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1741.6812939453125
INFO:root:current train perplexity3.9382383823394775
INFO:root:current mean train loss 1737.8911712646484
INFO:root:current train perplexity3.9308252334594727
INFO:root:current mean train loss 1736.8028568522136
INFO:root:current train perplexity3.9171929359436035
INFO:root:current mean train loss 1735.0166397094727
INFO:root:current train perplexity3.9235827922821045
INFO:root:current mean train loss 1734.2985708007814
INFO:root:current train perplexity3.9302620887756348
INFO:root:current mean train loss 1733.8106217447917
INFO:root:current train perplexity3.928351879119873
INFO:root:current mean train loss 1734.4939990234375
INFO:root:current train perplexity3.934218406677246
INFO:root:current mean train loss 1736.4115196228026
INFO:root:current train perplexity3.9422290325164795
INFO:root:current mean train loss 1738.9779973687066
INFO:root:current train perplexity3.9464190006256104
INFO:root:current mean train loss 1739.164828125
INFO:root:current train perplexity3.9482169151306152
INFO:root:current mean train loss 1740.1762361283736
INFO:root:current train perplexity3.952613592147827
INFO:root:current mean train loss 1740.4072709147135
INFO:root:current train perplexity3.9576127529144287
INFO:root:current mean train loss 1742.2808595628005
INFO:root:current train perplexity3.96008038520813
INFO:root:current mean train loss 1743.551562412807
INFO:root:current train perplexity3.9591360092163086
INFO:root:current mean train loss 1744.1426008300782
INFO:root:current train perplexity3.9607059955596924
INFO:root:current mean train loss 1745.6718085479736
INFO:root:current train perplexity3.9624032974243164
INFO:root:current mean train loss 1746.0813397575828
INFO:root:current train perplexity3.9626657962799072
INFO:root:current mean train loss 1747.0292241753473
INFO:root:current train perplexity3.9645090103149414
INFO:root:current mean train loss 1747.9792147024054
INFO:root:current train perplexity3.9673991203308105

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.99s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 189.00s/it]
INFO:root:final mean train loss: 1747.4809523835906
INFO:root:final train perplexity: 3.9676284790039062
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2086.297458513409
INFO:root:eval perplexity: 5.404731750488281
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/54
 27%|â–ˆâ–ˆâ–‹       | 54/200 [2:58:55<8:03:40, 198.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1753.125768324908
INFO:root:current train perplexity4.011756896972656
INFO:root:current mean train loss 1726.3331277961404
INFO:root:current train perplexity3.910616874694824
INFO:root:current mean train loss 1731.0504324776787
INFO:root:current train perplexity3.9209494590759277
INFO:root:current mean train loss 1733.082293104298
INFO:root:current train perplexity3.9313557147979736
INFO:root:current mean train loss 1733.3460318659136
INFO:root:current train perplexity3.929976224899292
INFO:root:current mean train loss 1736.663572093735
INFO:root:current train perplexity3.9279773235321045
INFO:root:current mean train loss 1735.88814897089
INFO:root:current train perplexity3.9298927783966064
INFO:root:current mean train loss 1735.6788966818667
INFO:root:current train perplexity3.9318220615386963
INFO:root:current mean train loss 1734.9552720629017
INFO:root:current train perplexity3.9333934783935547
INFO:root:current mean train loss 1735.7559890331072
INFO:root:current train perplexity3.933802366256714
INFO:root:current mean train loss 1735.654980564774
INFO:root:current train perplexity3.933734893798828
INFO:root:current mean train loss 1737.0138778754826
INFO:root:current train perplexity3.9347333908081055
INFO:root:current mean train loss 1737.832754443881
INFO:root:current train perplexity3.9372365474700928
INFO:root:current mean train loss 1737.0837399563104
INFO:root:current train perplexity3.9367611408233643
INFO:root:current mean train loss 1737.6869720049785
INFO:root:current train perplexity3.938939332962036
INFO:root:current mean train loss 1738.4680187851486
INFO:root:current train perplexity3.938685417175293
INFO:root:current mean train loss 1739.952470259233
INFO:root:current train perplexity3.941215991973877
INFO:root:current mean train loss 1741.2545388543335
INFO:root:current train perplexity3.944504737854004
INFO:root:current mean train loss 1742.0712613161934
INFO:root:current train perplexity3.947457790374756
INFO:root:current mean train loss 1742.3792801659495
INFO:root:current train perplexity3.949589252471924

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.18s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.18s/it]
INFO:root:final mean train loss: 1742.2032417762898
INFO:root:final train perplexity: 3.951148509979248
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.33s/it]
INFO:root:eval mean loss: 2087.6060215293937
INFO:root:eval perplexity: 5.410454750061035
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/55
 28%|â–ˆâ–ˆâ–Š       | 55/200 [3:02:14<8:00:24, 198.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1765.3304227941176
INFO:root:current train perplexity3.9420254230499268
INFO:root:current mean train loss 1741.263726533349
INFO:root:current train perplexity3.925579071044922
INFO:root:current mean train loss 1728.3023973774707
INFO:root:current train perplexity3.9158384799957275
INFO:root:current mean train loss 1729.306833963908
INFO:root:current train perplexity3.906186580657959
INFO:root:current mean train loss 1730.1492019864272
INFO:root:current train perplexity3.912692070007324
INFO:root:current mean train loss 1731.4055043195517
INFO:root:current train perplexity3.9130823612213135
INFO:root:current mean train loss 1727.3815787041601
INFO:root:current train perplexity3.9098846912384033
INFO:root:current mean train loss 1728.3820514730926
INFO:root:current train perplexity3.9108705520629883
INFO:root:current mean train loss 1728.4224607618592
INFO:root:current train perplexity3.9123966693878174
INFO:root:current mean train loss 1730.5954464375334
INFO:root:current train perplexity3.9154765605926514
INFO:root:current mean train loss 1729.1264140795001
INFO:root:current train perplexity3.917327880859375
INFO:root:current mean train loss 1731.3367623895986
INFO:root:current train perplexity3.9194259643554688
INFO:root:current mean train loss 1731.5928950132002
INFO:root:current train perplexity3.919410228729248
INFO:root:current mean train loss 1732.0101419932123
INFO:root:current train perplexity3.92142391204834
INFO:root:current mean train loss 1731.9450955996122
INFO:root:current train perplexity3.922844886779785
INFO:root:current mean train loss 1733.3413268963393
INFO:root:current train perplexity3.926561117172241
INFO:root:current mean train loss 1733.8971042142796
INFO:root:current train perplexity3.9283018112182617
INFO:root:current mean train loss 1735.103532450147
INFO:root:current train perplexity3.9293041229248047
INFO:root:current mean train loss 1735.97999724177
INFO:root:current train perplexity3.9305222034454346
INFO:root:current mean train loss 1736.1429554447147
INFO:root:current train perplexity3.9317104816436768

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.27s/it]
INFO:root:final mean train loss: 1736.2105157941626
INFO:root:final train perplexity: 3.932518243789673
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it]
INFO:root:eval mean loss: 2089.085345761996
INFO:root:eval perplexity: 5.416931629180908
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/56
 28%|â–ˆâ–ˆâ–Š       | 56/200 [3:05:33<7:57:10, 198.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1713.8848470052083
INFO:root:current train perplexity3.8613529205322266
INFO:root:current mean train loss 1715.7565230817984
INFO:root:current train perplexity3.875096321105957
INFO:root:current mean train loss 1727.437588026799
INFO:root:current train perplexity3.8862497806549072
INFO:root:current mean train loss 1727.4309676732773
INFO:root:current train perplexity3.891204595565796
INFO:root:current mean train loss 1724.5162421182094
INFO:root:current train perplexity3.8845064640045166
INFO:root:current mean train loss 1728.4190802323192
INFO:root:current train perplexity3.8882453441619873
INFO:root:current mean train loss 1725.3940682828702
INFO:root:current train perplexity3.8875908851623535
INFO:root:current mean train loss 1725.2306896468772
INFO:root:current train perplexity3.8881242275238037
INFO:root:current mean train loss 1728.153557137513
INFO:root:current train perplexity3.8925843238830566
INFO:root:current mean train loss 1728.7350926604806
INFO:root:current train perplexity3.898383855819702
INFO:root:current mean train loss 1727.4404853218289
INFO:root:current train perplexity3.8970184326171875
INFO:root:current mean train loss 1727.7024082743947
INFO:root:current train perplexity3.898003339767456
INFO:root:current mean train loss 1729.9093658666816
INFO:root:current train perplexity3.904484510421753
INFO:root:current mean train loss 1729.3614392622942
INFO:root:current train perplexity3.9050090312957764
INFO:root:current mean train loss 1730.1444925509347
INFO:root:current train perplexity3.90803599357605
INFO:root:current mean train loss 1730.196899256654
INFO:root:current train perplexity3.9075818061828613
INFO:root:current mean train loss 1729.6612983578843
INFO:root:current train perplexity3.908332347869873
INFO:root:current mean train loss 1729.4076801258657
INFO:root:current train perplexity3.9092941284179688
INFO:root:current mean train loss 1730.2622580752382
INFO:root:current train perplexity3.9110617637634277
INFO:root:current mean train loss 1731.0684910807959
INFO:root:current train perplexity3.9142463207244873

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.96s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.96s/it]
INFO:root:final mean train loss: 1730.565998979608
INFO:root:final train perplexity: 3.9150514602661133
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.35s/it]
INFO:root:eval mean loss: 2091.481811956311
INFO:root:eval perplexity: 5.4274396896362305
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/57
 28%|â–ˆâ–ˆâ–Š       | 57/200 [3:08:51<7:53:42, 198.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1698.4879545323988
INFO:root:current train perplexity3.8554465770721436
INFO:root:current mean train loss 1706.51338777088
INFO:root:current train perplexity3.8457753658294678
INFO:root:current mean train loss 1714.6579639947236
INFO:root:current train perplexity3.8451340198516846
INFO:root:current mean train loss 1715.6605844912322
INFO:root:current train perplexity3.850341796875
INFO:root:current mean train loss 1714.9377884824053
INFO:root:current train perplexity3.858834743499756
INFO:root:current mean train loss 1715.6368760659661
INFO:root:current train perplexity3.8647241592407227
INFO:root:current mean train loss 1720.139063852276
INFO:root:current train perplexity3.869786262512207
INFO:root:current mean train loss 1719.592661221822
INFO:root:current train perplexity3.874635696411133
INFO:root:current mean train loss 1717.0840883035264
INFO:root:current train perplexity3.8741536140441895
INFO:root:current mean train loss 1717.4968647602175
INFO:root:current train perplexity3.8765451908111572
INFO:root:current mean train loss 1718.4849571199452
INFO:root:current train perplexity3.8794891834259033
INFO:root:current mean train loss 1720.3491910124478
INFO:root:current train perplexity3.8840198516845703
INFO:root:current mean train loss 1721.3550197468946
INFO:root:current train perplexity3.8868846893310547
INFO:root:current mean train loss 1722.9054633246528
INFO:root:current train perplexity3.889420986175537
INFO:root:current mean train loss 1723.2319157156048
INFO:root:current train perplexity3.892030715942383
INFO:root:current mean train loss 1724.0257882877272
INFO:root:current train perplexity3.892296075820923
INFO:root:current mean train loss 1723.5779999291583
INFO:root:current train perplexity3.88995361328125
INFO:root:current mean train loss 1724.1839027232174
INFO:root:current train perplexity3.893378257751465
INFO:root:current mean train loss 1724.7569843431102
INFO:root:current train perplexity3.8937268257141113
INFO:root:current mean train loss 1725.1193976053378
INFO:root:current train perplexity3.895825147628784

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.43s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.43s/it]
INFO:root:final mean train loss: 1724.5669082587738
INFO:root:final train perplexity: 3.8965718746185303
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2093.7981974283853
INFO:root:eval perplexity: 5.437617778778076
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/58
 29%|â–ˆâ–ˆâ–‰       | 58/200 [3:12:10<7:50:37, 198.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1694.8321102366729
INFO:root:current train perplexity3.844820976257324
INFO:root:current mean train loss 1706.1818187816723
INFO:root:current train perplexity3.8422675132751465
INFO:root:current mean train loss 1710.6358993797971
INFO:root:current train perplexity3.8466804027557373
INFO:root:current mean train loss 1711.7405580991274
INFO:root:current train perplexity3.853675603866577
INFO:root:current mean train loss 1710.0236051264496
INFO:root:current train perplexity3.85545015335083
INFO:root:current mean train loss 1712.3856681106438
INFO:root:current train perplexity3.8579366207122803
INFO:root:current mean train loss 1711.223523216526
INFO:root:current train perplexity3.8568108081817627
INFO:root:current mean train loss 1712.8919655963873
INFO:root:current train perplexity3.860644578933716
INFO:root:current mean train loss 1712.3722285211422
INFO:root:current train perplexity3.858549118041992
INFO:root:current mean train loss 1715.7654835967244
INFO:root:current train perplexity3.8636457920074463
INFO:root:current mean train loss 1716.7143031529017
INFO:root:current train perplexity3.8669795989990234
INFO:root:current mean train loss 1716.8208395141087
INFO:root:current train perplexity3.8696131706237793
INFO:root:current mean train loss 1716.7253265024624
INFO:root:current train perplexity3.869368314743042
INFO:root:current mean train loss 1717.3120896322202
INFO:root:current train perplexity3.8722567558288574
INFO:root:current mean train loss 1718.3515687473696
INFO:root:current train perplexity3.8756673336029053
INFO:root:current mean train loss 1719.188633598063
INFO:root:current train perplexity3.8763186931610107
INFO:root:current mean train loss 1720.166410234491
INFO:root:current train perplexity3.878699779510498
INFO:root:current mean train loss 1719.9140445826768
INFO:root:current train perplexity3.8798775672912598
INFO:root:current mean train loss 1720.2252023712076
INFO:root:current train perplexity3.8815531730651855

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.44s/it]
INFO:root:final mean train loss: 1720.299805610876
INFO:root:final train perplexity: 3.883481025695801
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2095.3451334635415
INFO:root:eval perplexity: 5.444425582885742
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/59
 30%|â–ˆâ–ˆâ–‰       | 59/200 [3:15:29<7:47:25, 198.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1640.3251342773438
INFO:root:current train perplexity3.819559097290039
INFO:root:current mean train loss 1696.8901091930913
INFO:root:current train perplexity3.8214330673217773
INFO:root:current mean train loss 1691.5468979637221
INFO:root:current train perplexity3.8347043991088867
INFO:root:current mean train loss 1695.8674094092767
INFO:root:current train perplexity3.828282594680786
INFO:root:current mean train loss 1699.7131927642063
INFO:root:current train perplexity3.837266206741333
INFO:root:current mean train loss 1701.359827049225
INFO:root:current train perplexity3.833881139755249
INFO:root:current mean train loss 1705.1557349525021
INFO:root:current train perplexity3.840317487716675
INFO:root:current mean train loss 1706.7895295667513
INFO:root:current train perplexity3.839704990386963
INFO:root:current mean train loss 1708.0768676148982
INFO:root:current train perplexity3.8456215858459473
INFO:root:current mean train loss 1709.0638606373857
INFO:root:current train perplexity3.8462562561035156
INFO:root:current mean train loss 1711.0401728281718
INFO:root:current train perplexity3.850768804550171
INFO:root:current mean train loss 1711.9286454714795
INFO:root:current train perplexity3.8515822887420654
INFO:root:current mean train loss 1712.9324228093153
INFO:root:current train perplexity3.8526980876922607
INFO:root:current mean train loss 1712.3939898090978
INFO:root:current train perplexity3.855229139328003
INFO:root:current mean train loss 1712.9887725786543
INFO:root:current train perplexity3.8575148582458496
INFO:root:current mean train loss 1712.7269409829862
INFO:root:current train perplexity3.859581470489502
INFO:root:current mean train loss 1714.3779175719071
INFO:root:current train perplexity3.8623766899108887
INFO:root:current mean train loss 1715.2506307205217
INFO:root:current train perplexity3.863663911819458
INFO:root:current mean train loss 1714.8546226577673
INFO:root:current train perplexity3.863192558288574
INFO:root:current mean train loss 1714.195790576634
INFO:root:current train perplexity3.8630852699279785

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.67s/it]
INFO:root:final mean train loss: 1713.771982567153
INFO:root:final train perplexity: 3.863538980484009
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it]
INFO:root:eval mean loss: 2097.2470971506536
INFO:root:eval perplexity: 5.45280647277832
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/60
 30%|â–ˆâ–ˆâ–ˆ       | 60/200 [3:18:49<7:44:23, 199.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1663.2309313322369
INFO:root:current train perplexity3.860863208770752
INFO:root:current mean train loss 1699.8525390625
INFO:root:current train perplexity3.8398990631103516
INFO:root:current mean train loss 1706.730862830872
INFO:root:current train perplexity3.8377020359039307
INFO:root:current mean train loss 1709.7602389822932
INFO:root:current train perplexity3.8360097408294678
INFO:root:current mean train loss 1707.8616287850537
INFO:root:current train perplexity3.8297595977783203
INFO:root:current mean train loss 1709.5280230160163
INFO:root:current train perplexity3.8376994132995605
INFO:root:current mean train loss 1705.55406472448
INFO:root:current train perplexity3.839024066925049
INFO:root:current mean train loss 1704.0202154549504
INFO:root:current train perplexity3.835968255996704
INFO:root:current mean train loss 1704.1733580276061
INFO:root:current train perplexity3.8337180614471436
INFO:root:current mean train loss 1704.1016655756937
INFO:root:current train perplexity3.8349969387054443
INFO:root:current mean train loss 1704.3146362903658
INFO:root:current train perplexity3.8369922637939453
INFO:root:current mean train loss 1705.3664679505976
INFO:root:current train perplexity3.841491222381592
INFO:root:current mean train loss 1706.1023721295953
INFO:root:current train perplexity3.8428916931152344
INFO:root:current mean train loss 1705.3329776419755
INFO:root:current train perplexity3.842589855194092
INFO:root:current mean train loss 1706.5121099083588
INFO:root:current train perplexity3.844858407974243
INFO:root:current mean train loss 1706.1317293771087
INFO:root:current train perplexity3.843919038772583
INFO:root:current mean train loss 1706.3621950579543
INFO:root:current train perplexity3.842857837677002
INFO:root:current mean train loss 1706.4784465299365
INFO:root:current train perplexity3.8416976928710938
INFO:root:current mean train loss 1707.2276291220708
INFO:root:current train perplexity3.8436262607574463
INFO:root:current mean train loss 1707.549627084419
INFO:root:current train perplexity3.8457069396972656

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.78s/it]
INFO:root:final mean train loss: 1708.0838548868999
INFO:root:final train perplexity: 3.8462464809417725
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2098.5182460487313
INFO:root:eval perplexity: 5.4584150314331055
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/61
 30%|â–ˆâ–ˆâ–ˆ       | 61/200 [3:22:07<7:40:39, 198.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1682.6561821831597
INFO:root:current train perplexity3.7495312690734863
INFO:root:current mean train loss 1696.2949308507582
INFO:root:current train perplexity3.77999210357666
INFO:root:current mean train loss 1693.9057058560645
INFO:root:current train perplexity3.7837178707122803
INFO:root:current mean train loss 1696.589734395345
INFO:root:current train perplexity3.798011064529419
INFO:root:current mean train loss 1693.1510141355182
INFO:root:current train perplexity3.796671152114868
INFO:root:current mean train loss 1696.0577886780695
INFO:root:current train perplexity3.801926851272583
INFO:root:current mean train loss 1698.3411264479535
INFO:root:current train perplexity3.8032686710357666
INFO:root:current mean train loss 1699.4254435663638
INFO:root:current train perplexity3.8081483840942383
INFO:root:current mean train loss 1702.2690491014691
INFO:root:current train perplexity3.8121185302734375
INFO:root:current mean train loss 1704.0840484097473
INFO:root:current train perplexity3.8191051483154297
INFO:root:current mean train loss 1704.4185357406793
INFO:root:current train perplexity3.822634220123291
INFO:root:current mean train loss 1703.641306809976
INFO:root:current train perplexity3.82220721244812
INFO:root:current mean train loss 1703.2580918000353
INFO:root:current train perplexity3.8247087001800537
INFO:root:current mean train loss 1702.810448926366
INFO:root:current train perplexity3.823815107345581
INFO:root:current mean train loss 1702.488497678284
INFO:root:current train perplexity3.8244123458862305
INFO:root:current mean train loss 1702.4226366678874
INFO:root:current train perplexity3.8228766918182373
INFO:root:current mean train loss 1702.9856653190184
INFO:root:current train perplexity3.826676845550537
INFO:root:current mean train loss 1702.7394640522618
INFO:root:current train perplexity3.82700777053833
INFO:root:current mean train loss 1702.9425957041888
INFO:root:current train perplexity3.829619884490967
INFO:root:current mean train loss 1704.0455717606978
INFO:root:current train perplexity3.8329572677612305

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.86s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.86s/it]
INFO:root:final mean train loss: 1703.517458116894
INFO:root:final train perplexity: 3.8324198722839355
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2100.518480233267
INFO:root:eval perplexity: 5.467251300811768
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/62
 31%|â–ˆâ–ˆâ–ˆ       | 62/200 [3:25:26<7:37:05, 198.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1699.3811265477593
INFO:root:current train perplexity3.7845892906188965
INFO:root:current mean train loss 1696.4195524726817
INFO:root:current train perplexity3.7845780849456787
INFO:root:current mean train loss 1688.5830415868948
INFO:root:current train perplexity3.78108549118042
INFO:root:current mean train loss 1691.1740096743317
INFO:root:current train perplexity3.7837064266204834
INFO:root:current mean train loss 1692.9409255139349
INFO:root:current train perplexity3.785639762878418
INFO:root:current mean train loss 1688.8092111653057
INFO:root:current train perplexity3.7864391803741455
INFO:root:current mean train loss 1688.819541195085
INFO:root:current train perplexity3.7897374629974365
INFO:root:current mean train loss 1687.6298118074576
INFO:root:current train perplexity3.7906880378723145
INFO:root:current mean train loss 1688.836346499945
INFO:root:current train perplexity3.7907116413116455
INFO:root:current mean train loss 1691.0017166698092
INFO:root:current train perplexity3.7957680225372314
INFO:root:current mean train loss 1692.0060681581272
INFO:root:current train perplexity3.797407627105713
INFO:root:current mean train loss 1692.7529722480078
INFO:root:current train perplexity3.8012337684631348
INFO:root:current mean train loss 1693.277011344648
INFO:root:current train perplexity3.803884267807007
INFO:root:current mean train loss 1695.0877316539056
INFO:root:current train perplexity3.8092784881591797
INFO:root:current mean train loss 1695.848751606321
INFO:root:current train perplexity3.8110601902008057
INFO:root:current mean train loss 1696.5744341219615
INFO:root:current train perplexity3.812342643737793
INFO:root:current mean train loss 1698.2920199542498
INFO:root:current train perplexity3.81512451171875
INFO:root:current mean train loss 1697.6123230711637
INFO:root:current train perplexity3.813986301422119
INFO:root:current mean train loss 1698.2241093676218
INFO:root:current train perplexity3.8155484199523926
INFO:root:current mean train loss 1698.6720200092807
INFO:root:current train perplexity3.8165690898895264

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.84s/it]
INFO:root:final mean train loss: 1698.558152714824
INFO:root:final train perplexity: 3.8174595832824707
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2103.6114636143893
INFO:root:eval perplexity: 5.480945587158203
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/63
 32%|â–ˆâ–ˆâ–ˆâ–      | 63/200 [3:28:44<7:33:34, 198.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1671.7755301339287
INFO:root:current train perplexity3.7472586631774902
INFO:root:current mean train loss 1676.754744944853
INFO:root:current train perplexity3.7527565956115723
INFO:root:current mean train loss 1679.2465472186054
INFO:root:current train perplexity3.7571256160736084
INFO:root:current mean train loss 1678.7662845096072
INFO:root:current train perplexity3.76418137550354
INFO:root:current mean train loss 1680.0463236058013
INFO:root:current train perplexity3.7683370113372803
INFO:root:current mean train loss 1680.5495723255895
INFO:root:current train perplexity3.769071578979492
INFO:root:current mean train loss 1682.9043447921524
INFO:root:current train perplexity3.7757489681243896
INFO:root:current mean train loss 1684.4554944323254
INFO:root:current train perplexity3.7795956134796143
INFO:root:current mean train loss 1686.0818400065104
INFO:root:current train perplexity3.781242847442627
INFO:root:current mean train loss 1688.5787266682103
INFO:root:current train perplexity3.784280300140381
INFO:root:current mean train loss 1687.822133857513
INFO:root:current train perplexity3.7840309143066406
INFO:root:current mean train loss 1688.4386364015759
INFO:root:current train perplexity3.7852039337158203
INFO:root:current mean train loss 1689.056831900529
INFO:root:current train perplexity3.788208484649658
INFO:root:current mean train loss 1689.7408494489907
INFO:root:current train perplexity3.787921190261841
INFO:root:current mean train loss 1689.8807861328125
INFO:root:current train perplexity3.7889907360076904
INFO:root:current mean train loss 1691.1004596685907
INFO:root:current train perplexity3.790956974029541
INFO:root:current mean train loss 1691.5715929956493
INFO:root:current train perplexity3.792602300643921
INFO:root:current mean train loss 1691.8815849002472
INFO:root:current train perplexity3.794665813446045
INFO:root:current mean train loss 1692.7702961804396
INFO:root:current train perplexity3.797395944595337
INFO:root:current mean train loss 1693.4062951722121
INFO:root:current train perplexity3.8004150390625

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.21s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.21s/it]
INFO:root:final mean train loss: 1693.0552885254644
INFO:root:final train perplexity: 3.8009281158447266
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.33s/it]
INFO:root:eval mean loss: 2105.4583787850456
INFO:root:eval perplexity: 5.489137172698975
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/64
 32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [3:32:03<7:30:24, 198.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1649.3136407260238
INFO:root:current train perplexity3.7271807193756104
INFO:root:current mean train loss 1669.2949336250836
INFO:root:current train perplexity3.7350966930389404
INFO:root:current mean train loss 1675.3049010167138
INFO:root:current train perplexity3.743962526321411
INFO:root:current mean train loss 1678.4855461810603
INFO:root:current train perplexity3.751087188720703
INFO:root:current mean train loss 1680.6541184066991
INFO:root:current train perplexity3.750086545944214
INFO:root:current mean train loss 1683.2996464328019
INFO:root:current train perplexity3.7570700645446777
INFO:root:current mean train loss 1682.1504449969295
INFO:root:current train perplexity3.7630035877227783
INFO:root:current mean train loss 1682.3410058221489
INFO:root:current train perplexity3.7649519443511963
INFO:root:current mean train loss 1684.1617738536675
INFO:root:current train perplexity3.773574113845825
INFO:root:current mean train loss 1684.8324891064306
INFO:root:current train perplexity3.779231071472168
INFO:root:current mean train loss 1685.5530341042218
INFO:root:current train perplexity3.778993606567383
INFO:root:current mean train loss 1684.769871442581
INFO:root:current train perplexity3.777541160583496
INFO:root:current mean train loss 1686.0168225600355
INFO:root:current train perplexity3.778818368911743
INFO:root:current mean train loss 1686.150841941916
INFO:root:current train perplexity3.778681516647339
INFO:root:current mean train loss 1687.0550988613556
INFO:root:current train perplexity3.7802693843841553
INFO:root:current mean train loss 1686.767387827613
INFO:root:current train perplexity3.7810773849487305
INFO:root:current mean train loss 1687.5248339641144
INFO:root:current train perplexity3.781585931777954
INFO:root:current mean train loss 1687.5744884386368
INFO:root:current train perplexity3.7811286449432373
INFO:root:current mean train loss 1688.229114526406
INFO:root:current train perplexity3.7819766998291016

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.75s/it]
INFO:root:final mean train loss: 1687.6475518284815
INFO:root:final train perplexity: 3.7847516536712646
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2106.801460428441
INFO:root:eval perplexity: 5.495102882385254
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/65
 32%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/200 [3:35:21<7:26:51, 198.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1669.2301635742188
INFO:root:current train perplexity3.6399636268615723
INFO:root:current mean train loss 1659.3006040132964
INFO:root:current train perplexity3.740731716156006
INFO:root:current mean train loss 1660.7647489659926
INFO:root:current train perplexity3.7388205528259277
INFO:root:current mean train loss 1665.8825812088817
INFO:root:current train perplexity3.7438695430755615
INFO:root:current mean train loss 1667.3346829556003
INFO:root:current train perplexity3.7437264919281006
INFO:root:current mean train loss 1667.3035518101283
INFO:root:current train perplexity3.750281810760498
INFO:root:current mean train loss 1669.6333705068423
INFO:root:current train perplexity3.7507660388946533
INFO:root:current mean train loss 1671.3492847789419
INFO:root:current train perplexity3.751749277114868
INFO:root:current mean train loss 1671.4887139619286
INFO:root:current train perplexity3.752054452896118
INFO:root:current mean train loss 1673.330638514156
INFO:root:current train perplexity3.7519049644470215
INFO:root:current mean train loss 1675.2874089579184
INFO:root:current train perplexity3.7564761638641357
INFO:root:current mean train loss 1677.3090005404708
INFO:root:current train perplexity3.7600276470184326
INFO:root:current mean train loss 1677.948364764749
INFO:root:current train perplexity3.7598423957824707
INFO:root:current mean train loss 1678.4077785936602
INFO:root:current train perplexity3.7598774433135986
INFO:root:current mean train loss 1679.3282817612346
INFO:root:current train perplexity3.762833595275879
INFO:root:current mean train loss 1679.8858494048422
INFO:root:current train perplexity3.7638721466064453
INFO:root:current mean train loss 1680.6149981491583
INFO:root:current train perplexity3.7658491134643555
INFO:root:current mean train loss 1681.0803690449172
INFO:root:current train perplexity3.7676615715026855
INFO:root:current mean train loss 1682.754946689648
INFO:root:current train perplexity3.7692930698394775
INFO:root:current mean train loss 1683.877569823706
INFO:root:current train perplexity3.7702319622039795

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.16s/it]
INFO:root:final mean train loss: 1683.3912027871675
INFO:root:final train perplexity: 3.7720682621002197
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2109.698216215093
INFO:root:eval perplexity: 5.507991313934326
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/66
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/200 [3:38:40<7:23:39, 198.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1684.6304117838542
INFO:root:current train perplexity3.7535853385925293
INFO:root:current mean train loss 1675.5826799376937
INFO:root:current train perplexity3.7368109226226807
INFO:root:current mean train loss 1671.0725986946761
INFO:root:current train perplexity3.730130434036255
INFO:root:current mean train loss 1673.359193225516
INFO:root:current train perplexity3.7354350090026855
INFO:root:current mean train loss 1674.8540534882534
INFO:root:current train perplexity3.736968994140625
INFO:root:current mean train loss 1674.4805130327045
INFO:root:current train perplexity3.7431328296661377
INFO:root:current mean train loss 1674.3738950769298
INFO:root:current train perplexity3.7387351989746094
INFO:root:current mean train loss 1674.9721627202346
INFO:root:current train perplexity3.7425036430358887
INFO:root:current mean train loss 1674.981009070969
INFO:root:current train perplexity3.7434041500091553
INFO:root:current mean train loss 1675.3785734394087
INFO:root:current train perplexity3.741171360015869
INFO:root:current mean train loss 1674.5663977612712
INFO:root:current train perplexity3.7411673069000244
INFO:root:current mean train loss 1674.6307257619103
INFO:root:current train perplexity3.7431695461273193
INFO:root:current mean train loss 1675.8259890194704
INFO:root:current train perplexity3.7438197135925293
INFO:root:current mean train loss 1676.6085650482294
INFO:root:current train perplexity3.745875835418701
INFO:root:current mean train loss 1676.9093687633324
INFO:root:current train perplexity3.7480006217956543
INFO:root:current mean train loss 1676.3867213182118
INFO:root:current train perplexity3.74814510345459
INFO:root:current mean train loss 1676.2085631910038
INFO:root:current train perplexity3.7497503757476807
INFO:root:current mean train loss 1677.523832579396
INFO:root:current train perplexity3.7520413398742676
INFO:root:current mean train loss 1678.5433704223299
INFO:root:current train perplexity3.7541370391845703
INFO:root:current mean train loss 1678.1909290891585
INFO:root:current train perplexity3.7550160884857178

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.02s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.02s/it]
INFO:root:final mean train loss: 1677.9502984536036
INFO:root:final train perplexity: 3.7559173107147217
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.35s/it]
INFO:root:eval mean loss: 2109.8099889011246
INFO:root:eval perplexity: 5.50848913192749
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/67
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/200 [3:41:59<7:20:21, 198.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1652.720359400699
INFO:root:current train perplexity3.7339298725128174
INFO:root:current mean train loss 1665.076226718184
INFO:root:current train perplexity3.704181432723999
INFO:root:current mean train loss 1666.5255398790375
INFO:root:current train perplexity3.698964834213257
INFO:root:current mean train loss 1668.4058920956222
INFO:root:current train perplexity3.7154653072357178
INFO:root:current mean train loss 1671.7284791153859
INFO:root:current train perplexity3.721289873123169
INFO:root:current mean train loss 1669.9015219309074
INFO:root:current train perplexity3.723635196685791
INFO:root:current mean train loss 1670.5671993243657
INFO:root:current train perplexity3.722588539123535
INFO:root:current mean train loss 1670.02088196297
INFO:root:current train perplexity3.7231974601745605
INFO:root:current mean train loss 1668.6851413335323
INFO:root:current train perplexity3.722304105758667
INFO:root:current mean train loss 1668.9482943732094
INFO:root:current train perplexity3.7262580394744873
INFO:root:current mean train loss 1670.7559433424403
INFO:root:current train perplexity3.726609945297241
INFO:root:current mean train loss 1670.52880859375
INFO:root:current train perplexity3.73038387298584
INFO:root:current mean train loss 1670.955915953308
INFO:root:current train perplexity3.731672525405884
INFO:root:current mean train loss 1671.6841993574249
INFO:root:current train perplexity3.734442710876465
INFO:root:current mean train loss 1671.7271551097715
INFO:root:current train perplexity3.735750436782837
INFO:root:current mean train loss 1672.6108637339737
INFO:root:current train perplexity3.7383971214294434
INFO:root:current mean train loss 1673.047619792859
INFO:root:current train perplexity3.7378244400024414
INFO:root:current mean train loss 1672.7408813195618
INFO:root:current train perplexity3.738384485244751
INFO:root:current mean train loss 1673.4138131126097
INFO:root:current train perplexity3.739814519882202
INFO:root:current mean train loss 1673.0824348126894
INFO:root:current train perplexity3.7413992881774902

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.16s/it]
INFO:root:final mean train loss: 1673.1647945217453
INFO:root:final train perplexity: 3.7417685985565186
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2113.1704785502548
INFO:root:eval perplexity: 5.5234808921813965
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/68
 34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [3:45:18<7:17:07, 198.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1654.5120871803977
INFO:root:current train perplexity3.6881306171417236
INFO:root:current mean train loss 1654.5984422253025
INFO:root:current train perplexity3.6813161373138428
INFO:root:current mean train loss 1655.2822179457721
INFO:root:current train perplexity3.684314727783203
INFO:root:current mean train loss 1655.732188394036
INFO:root:current train perplexity3.6983261108398438
INFO:root:current mean train loss 1657.3298903245193
INFO:root:current train perplexity3.6987826824188232
INFO:root:current mean train loss 1657.3115632478182
INFO:root:current train perplexity3.702061653137207
INFO:root:current mean train loss 1656.8142934085758
INFO:root:current train perplexity3.7059755325317383
INFO:root:current mean train loss 1657.6263427734375
INFO:root:current train perplexity3.7045414447784424
INFO:root:current mean train loss 1659.1932233130026
INFO:root:current train perplexity3.7067947387695312
INFO:root:current mean train loss 1659.2732653233393
INFO:root:current train perplexity3.7098424434661865
INFO:root:current mean train loss 1659.8851143642623
INFO:root:current train perplexity3.7089102268218994
INFO:root:current mean train loss 1661.4714903992492
INFO:root:current train perplexity3.712944746017456
INFO:root:current mean train loss 1662.4495660911043
INFO:root:current train perplexity3.714730978012085
INFO:root:current mean train loss 1663.3386460195168
INFO:root:current train perplexity3.7188427448272705
INFO:root:current mean train loss 1664.0328463944372
INFO:root:current train perplexity3.718508005142212
INFO:root:current mean train loss 1664.792302819157
INFO:root:current train perplexity3.718221426010132
INFO:root:current mean train loss 1665.379532975949
INFO:root:current train perplexity3.718862295150757
INFO:root:current mean train loss 1666.180286513978
INFO:root:current train perplexity3.7201645374298096
INFO:root:current mean train loss 1666.4216972577283
INFO:root:current train perplexity3.721491813659668
INFO:root:current mean train loss 1667.6024221122723
INFO:root:current train perplexity3.7242560386657715

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.57s/it]
INFO:root:final mean train loss: 1667.4782052167545
INFO:root:final train perplexity: 3.725025177001953
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2115.0954871211493
INFO:root:eval perplexity: 5.5320868492126465
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/69
 34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [3:48:37<7:14:07, 198.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1633.8929104275173
INFO:root:current train perplexity3.665954351425171
INFO:root:current mean train loss 1636.2427396552507
INFO:root:current train perplexity3.677838087081909
INFO:root:current mean train loss 1644.3247940961053
INFO:root:current train perplexity3.678551197052002
INFO:root:current mean train loss 1646.9357473722068
INFO:root:current train perplexity3.6764700412750244
INFO:root:current mean train loss 1651.2795940334513
INFO:root:current train perplexity3.6815733909606934
INFO:root:current mean train loss 1653.8408811342465
INFO:root:current train perplexity3.6888372898101807
INFO:root:current mean train loss 1654.6775935945057
INFO:root:current train perplexity3.690934181213379
INFO:root:current mean train loss 1655.698662950585
INFO:root:current train perplexity3.6936569213867188
INFO:root:current mean train loss 1655.6377433286893
INFO:root:current train perplexity3.6941232681274414
INFO:root:current mean train loss 1656.8728218235597
INFO:root:current train perplexity3.697960138320923
INFO:root:current mean train loss 1657.850929487997
INFO:root:current train perplexity3.6980137825012207
INFO:root:current mean train loss 1657.3639215983628
INFO:root:current train perplexity3.6966452598571777
INFO:root:current mean train loss 1658.495934636338
INFO:root:current train perplexity3.698370933532715
INFO:root:current mean train loss 1659.451628837919
INFO:root:current train perplexity3.7008156776428223
INFO:root:current mean train loss 1659.3088315051534
INFO:root:current train perplexity3.7035350799560547
INFO:root:current mean train loss 1660.4736780064707
INFO:root:current train perplexity3.7046279907226562
INFO:root:current mean train loss 1661.4257810309743
INFO:root:current train perplexity3.7069945335388184
INFO:root:current mean train loss 1661.90514640722
INFO:root:current train perplexity3.7091829776763916
INFO:root:current mean train loss 1662.721667428302
INFO:root:current train perplexity3.710114002227783
INFO:root:current mean train loss 1663.7736682698403
INFO:root:current train perplexity3.7133536338806152

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.65s/it]
INFO:root:final mean train loss: 1663.3884807053805
INFO:root:final train perplexity: 3.7130300998687744
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2116.7423121675533
INFO:root:eval perplexity: 5.53945779800415
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/70
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [3:51:56<7:11:05, 198.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1625.3743196980338
INFO:root:current train perplexity3.6167972087860107
INFO:root:current mean train loss 1630.5594618055557
INFO:root:current train perplexity3.6288087368011475
INFO:root:current mean train loss 1635.2800605536331
INFO:root:current train perplexity3.6300954818725586
INFO:root:current mean train loss 1639.1140871023458
INFO:root:current train perplexity3.645993232727051
INFO:root:current mean train loss 1641.1925375846754
INFO:root:current train perplexity3.6568243503570557
INFO:root:current mean train loss 1642.9474795071096
INFO:root:current train perplexity3.661733627319336
INFO:root:current mean train loss 1645.436086004116
INFO:root:current train perplexity3.6661393642425537
INFO:root:current mean train loss 1649.6273391394864
INFO:root:current train perplexity3.67138409614563
INFO:root:current mean train loss 1651.398452467001
INFO:root:current train perplexity3.672631025314331
INFO:root:current mean train loss 1653.061809173368
INFO:root:current train perplexity3.6748743057250977
INFO:root:current mean train loss 1653.2749177006212
INFO:root:current train perplexity3.678497314453125
INFO:root:current mean train loss 1652.9057616160837
INFO:root:current train perplexity3.6801888942718506
INFO:root:current mean train loss 1652.560463537626
INFO:root:current train perplexity3.683371067047119
INFO:root:current mean train loss 1653.1661835705488
INFO:root:current train perplexity3.685889959335327
INFO:root:current mean train loss 1653.738216894597
INFO:root:current train perplexity3.688659191131592
INFO:root:current mean train loss 1655.6311661256343
INFO:root:current train perplexity3.6923446655273438
INFO:root:current mean train loss 1657.219844151842
INFO:root:current train perplexity3.6946310997009277
INFO:root:current mean train loss 1657.512951121109
INFO:root:current train perplexity3.695204973220825
INFO:root:current mean train loss 1658.3386733871469
INFO:root:current train perplexity3.6979784965515137

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.84s/it]
INFO:root:final mean train loss: 1659.328371910753
INFO:root:final train perplexity: 3.701160192489624
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.36s/it]
INFO:root:eval mean loss: 2116.806936710439
INFO:root:eval perplexity: 5.5397491455078125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/71
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [3:55:14<7:07:28, 198.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1628.582255045573
INFO:root:current train perplexity3.59904146194458
INFO:root:current mean train loss 1639.2836603128685
INFO:root:current train perplexity3.6600005626678467
INFO:root:current mean train loss 1641.8341177042248
INFO:root:current train perplexity3.6666688919067383
INFO:root:current mean train loss 1642.078040428411
INFO:root:current train perplexity3.6617252826690674
INFO:root:current mean train loss 1639.7400831401055
INFO:root:current train perplexity3.6644654273986816
INFO:root:current mean train loss 1641.9891625204577
INFO:root:current train perplexity3.66420578956604
INFO:root:current mean train loss 1642.2118276463877
INFO:root:current train perplexity3.6600940227508545
INFO:root:current mean train loss 1644.2542036450957
INFO:root:current train perplexity3.6628451347351074
INFO:root:current mean train loss 1646.4428957804262
INFO:root:current train perplexity3.6638004779815674
INFO:root:current mean train loss 1647.0813802083333
INFO:root:current train perplexity3.667295455932617
INFO:root:current mean train loss 1646.9805893642053
INFO:root:current train perplexity3.667473316192627
INFO:root:current mean train loss 1648.600638363719
INFO:root:current train perplexity3.6687378883361816
INFO:root:current mean train loss 1649.2439061001955
INFO:root:current train perplexity3.66916823387146
INFO:root:current mean train loss 1650.3515310944679
INFO:root:current train perplexity3.6698074340820312
INFO:root:current mean train loss 1651.4819903746777
INFO:root:current train perplexity3.672671318054199
INFO:root:current mean train loss 1651.79083900401
INFO:root:current train perplexity3.674136161804199
INFO:root:current mean train loss 1652.2930155714898
INFO:root:current train perplexity3.6759564876556396
INFO:root:current mean train loss 1653.4988854107517
INFO:root:current train perplexity3.679152727127075
INFO:root:current mean train loss 1653.566746505788
INFO:root:current train perplexity3.680619716644287
INFO:root:current mean train loss 1653.3636927409536
INFO:root:current train perplexity3.6817901134490967

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.96s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.96s/it]
INFO:root:final mean train loss: 1653.8088457701006
INFO:root:final train perplexity: 3.6850833892822266
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.33s/it]
INFO:root:eval mean loss: 2120.3684692382812
INFO:root:eval perplexity: 5.555728435516357
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/72
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/200 [3:58:33<7:04:01, 198.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1622.0585831351902
INFO:root:current train perplexity3.605889081954956
INFO:root:current mean train loss 1627.6949969035823
INFO:root:current train perplexity3.6334760189056396
INFO:root:current mean train loss 1630.1508958756656
INFO:root:current train perplexity3.62800931930542
INFO:root:current mean train loss 1632.4108452103087
INFO:root:current train perplexity3.642651081085205
INFO:root:current mean train loss 1637.65856587295
INFO:root:current train perplexity3.646568775177002
INFO:root:current mean train loss 1639.8625999436097
INFO:root:current train perplexity3.648496389389038
INFO:root:current mean train loss 1643.6473539545295
INFO:root:current train perplexity3.654694080352783
INFO:root:current mean train loss 1644.0599150809344
INFO:root:current train perplexity3.658389091491699
INFO:root:current mean train loss 1644.7293650741856
INFO:root:current train perplexity3.65879487991333
INFO:root:current mean train loss 1645.225300269163
INFO:root:current train perplexity3.6602587699890137
INFO:root:current mean train loss 1646.0407878320123
INFO:root:current train perplexity3.6620771884918213
INFO:root:current mean train loss 1646.333630990685
INFO:root:current train perplexity3.66461181640625
INFO:root:current mean train loss 1645.8504071738616
INFO:root:current train perplexity3.664267063140869
INFO:root:current mean train loss 1645.8903651738474
INFO:root:current train perplexity3.665389060974121
INFO:root:current mean train loss 1645.3491403950995
INFO:root:current train perplexity3.6655209064483643
INFO:root:current mean train loss 1646.6358141632982
INFO:root:current train perplexity3.6667275428771973
INFO:root:current mean train loss 1647.5998562984971
INFO:root:current train perplexity3.667433500289917
INFO:root:current mean train loss 1648.6648625104287
INFO:root:current train perplexity3.6695432662963867
INFO:root:current mean train loss 1649.474983487362
INFO:root:current train perplexity3.6717188358306885
INFO:root:current mean train loss 1649.5233062252178
INFO:root:current train perplexity3.670927047729492

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.26s/it]
INFO:root:final mean train loss: 1649.5667374957648
INFO:root:final train perplexity: 3.6727755069732666
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2123.943488804161
INFO:root:eval perplexity: 5.5718159675598145
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/73
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/200 [4:01:52<7:00:47, 198.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1641.392562866211
INFO:root:current train perplexity3.5893454551696777
INFO:root:current mean train loss 1638.8199384416853
INFO:root:current train perplexity3.609745979309082
INFO:root:current mean train loss 1635.6306767781575
INFO:root:current train perplexity3.6158154010772705
INFO:root:current mean train loss 1633.8560202205883
INFO:root:current train perplexity3.6279709339141846
INFO:root:current mean train loss 1636.269999001243
INFO:root:current train perplexity3.635599374771118
INFO:root:current mean train loss 1635.7658006456163
INFO:root:current train perplexity3.637923240661621
INFO:root:current mean train loss 1637.756636428833
INFO:root:current train perplexity3.6431448459625244
INFO:root:current mean train loss 1637.602340945682
INFO:root:current train perplexity3.6460671424865723
INFO:root:current mean train loss 1637.3093731108165
INFO:root:current train perplexity3.6417062282562256
INFO:root:current mean train loss 1638.2906156499334
INFO:root:current train perplexity3.6433584690093994
INFO:root:current mean train loss 1639.2659084613506
INFO:root:current train perplexity3.647341251373291
INFO:root:current mean train loss 1639.057925682737
INFO:root:current train perplexity3.647207260131836
INFO:root:current mean train loss 1638.3947725357548
INFO:root:current train perplexity3.6490695476531982
INFO:root:current mean train loss 1639.386805747872
INFO:root:current train perplexity3.648608922958374
INFO:root:current mean train loss 1640.6958557128905
INFO:root:current train perplexity3.6505868434906006
INFO:root:current mean train loss 1640.8035323502181
INFO:root:current train perplexity3.6510825157165527
INFO:root:current mean train loss 1642.8262205542587
INFO:root:current train perplexity3.6545372009277344
INFO:root:current mean train loss 1644.056774130635
INFO:root:current train perplexity3.656430244445801
INFO:root:current mean train loss 1644.416255387016
INFO:root:current train perplexity3.658550977706909
INFO:root:current mean train loss 1645.24721446873
INFO:root:current train perplexity3.6595685482025146

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.29s/it]
INFO:root:final mean train loss: 1644.7627440359756
INFO:root:final train perplexity: 3.658886432647705
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it]
INFO:root:eval mean loss: 2125.6201370996787
INFO:root:eval perplexity: 5.579376220703125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/74
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/200 [4:05:11<6:57:32, 198.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1615.6327611019738
INFO:root:current train perplexity3.578909158706665
INFO:root:current mean train loss 1622.0141562686604
INFO:root:current train perplexity3.601395606994629
INFO:root:current mean train loss 1617.8775834257965
INFO:root:current train perplexity3.5964479446411133
INFO:root:current mean train loss 1624.2952966342787
INFO:root:current train perplexity3.6030778884887695
INFO:root:current mean train loss 1629.8250724408506
INFO:root:current train perplexity3.6069345474243164
INFO:root:current mean train loss 1632.796940308713
INFO:root:current train perplexity3.6096832752227783
INFO:root:current mean train loss 1632.7280072773972
INFO:root:current train perplexity3.617250680923462
INFO:root:current mean train loss 1634.272209379128
INFO:root:current train perplexity3.621821880340576
INFO:root:current mean train loss 1635.5211301289473
INFO:root:current train perplexity3.6253268718719482
INFO:root:current mean train loss 1636.4089129696072
INFO:root:current train perplexity3.6264967918395996
INFO:root:current mean train loss 1638.3837767053349
INFO:root:current train perplexity3.630138397216797
INFO:root:current mean train loss 1638.8596968984646
INFO:root:current train perplexity3.635026454925537
INFO:root:current mean train loss 1638.6530546129177
INFO:root:current train perplexity3.637075424194336
INFO:root:current mean train loss 1639.1118719990673
INFO:root:current train perplexity3.6382875442504883
INFO:root:current mean train loss 1640.3134611466198
INFO:root:current train perplexity3.64030385017395
INFO:root:current mean train loss 1640.6812912702715
INFO:root:current train perplexity3.6410889625549316
INFO:root:current mean train loss 1640.7728717921366
INFO:root:current train perplexity3.6441478729248047
INFO:root:current mean train loss 1641.2915138413978
INFO:root:current train perplexity3.644622564315796
INFO:root:current mean train loss 1640.8544409797598
INFO:root:current train perplexity3.645925283432007
INFO:root:current mean train loss 1641.0686041393874
INFO:root:current train perplexity3.6469268798828125

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.09s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.09s/it]
INFO:root:final mean train loss: 1640.620444616644
INFO:root:final train perplexity: 3.6469526290893555
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.33s/it]
INFO:root:eval mean loss: 2127.5050918903758
INFO:root:eval perplexity: 5.587886810302734
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/75
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/200 [4:08:30<6:54:12, 198.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1636.9675441432644
INFO:root:current train perplexity3.607146978378296
INFO:root:current mean train loss 1629.3244867434446
INFO:root:current train perplexity3.5987462997436523
INFO:root:current mean train loss 1623.9089667327212
INFO:root:current train perplexity3.5946812629699707
INFO:root:current mean train loss 1625.8535538127715
INFO:root:current train perplexity3.6042003631591797
INFO:root:current mean train loss 1624.4550652483847
INFO:root:current train perplexity3.603260040283203
INFO:root:current mean train loss 1627.4882493500925
INFO:root:current train perplexity3.607530117034912
INFO:root:current mean train loss 1628.6566024463325
INFO:root:current train perplexity3.6138010025024414
INFO:root:current mean train loss 1629.3432025761567
INFO:root:current train perplexity3.6141655445098877
INFO:root:current mean train loss 1630.0866749499428
INFO:root:current train perplexity3.6140801906585693
INFO:root:current mean train loss 1628.5986160184323
INFO:root:current train perplexity3.610818862915039
INFO:root:current mean train loss 1629.6039899261305
INFO:root:current train perplexity3.6133155822753906
INFO:root:current mean train loss 1630.729840514201
INFO:root:current train perplexity3.6178648471832275
INFO:root:current mean train loss 1631.9982884285776
INFO:root:current train perplexity3.619731903076172
INFO:root:current mean train loss 1632.632640499909
INFO:root:current train perplexity3.6223249435424805
INFO:root:current mean train loss 1633.0812186625467
INFO:root:current train perplexity3.6251540184020996
INFO:root:current mean train loss 1633.6302021807
INFO:root:current train perplexity3.6263058185577393
INFO:root:current mean train loss 1634.1727523894863
INFO:root:current train perplexity3.6287732124328613
INFO:root:current mean train loss 1635.0078508964118
INFO:root:current train perplexity3.6298627853393555
INFO:root:current mean train loss 1635.258186136773
INFO:root:current train perplexity3.6304707527160645
INFO:root:current mean train loss 1636.0734355582533
INFO:root:current train perplexity3.6322975158691406

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.23s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.23s/it]
INFO:root:final mean train loss: 1635.589024653894
INFO:root:final train perplexity: 3.632510185241699
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2131.1767534837654
INFO:root:eval perplexity: 5.604504108428955
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/76
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [4:11:48<6:50:53, 198.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1604.6585639702096
INFO:root:current train perplexity3.559967041015625
INFO:root:current mean train loss 1612.2878564964415
INFO:root:current train perplexity3.5700013637542725
INFO:root:current mean train loss 1620.522090951192
INFO:root:current train perplexity3.5854952335357666
INFO:root:current mean train loss 1624.481990102002
INFO:root:current train perplexity3.598222017288208
INFO:root:current mean train loss 1624.516490182663
INFO:root:current train perplexity3.598964214324951
INFO:root:current mean train loss 1624.4450809588488
INFO:root:current train perplexity3.599741220474243
INFO:root:current mean train loss 1623.9730853509973
INFO:root:current train perplexity3.60449481010437
INFO:root:current mean train loss 1623.7432406072219
INFO:root:current train perplexity3.602799654006958
INFO:root:current mean train loss 1625.9103568234427
INFO:root:current train perplexity3.6052043437957764
INFO:root:current mean train loss 1626.9982306579527
INFO:root:current train perplexity3.6081387996673584
INFO:root:current mean train loss 1627.5227168264134
INFO:root:current train perplexity3.609055280685425
INFO:root:current mean train loss 1627.6276395270847
INFO:root:current train perplexity3.608614921569824
INFO:root:current mean train loss 1627.3371064816215
INFO:root:current train perplexity3.607774496078491
INFO:root:current mean train loss 1628.010712174526
INFO:root:current train perplexity3.609642267227173
INFO:root:current mean train loss 1628.4051874724912
INFO:root:current train perplexity3.610929012298584
INFO:root:current mean train loss 1628.3365149363117
INFO:root:current train perplexity3.610382080078125
INFO:root:current mean train loss 1629.6286447120094
INFO:root:current train perplexity3.6136510372161865
INFO:root:current mean train loss 1630.9919760750367
INFO:root:current train perplexity3.615415096282959
INFO:root:current mean train loss 1630.7690116603937
INFO:root:current train perplexity3.6169674396514893

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.32s/it]
INFO:root:final mean train loss: 1630.8830973615084
INFO:root:final train perplexity: 3.6190531253814697
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2132.972417303856
INFO:root:eval perplexity: 5.612648963928223
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/77
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/200 [4:15:07<6:47:37, 198.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1617.8451843261719
INFO:root:current train perplexity3.5428435802459717
INFO:root:current mean train loss 1605.3239904333043
INFO:root:current train perplexity3.577909231185913
INFO:root:current mean train loss 1608.3249564537634
INFO:root:current train perplexity3.5731945037841797
INFO:root:current mean train loss 1611.1246119907923
INFO:root:current train perplexity3.5696122646331787
INFO:root:current mean train loss 1611.812328563017
INFO:root:current train perplexity3.569662094116211
INFO:root:current mean train loss 1613.5942128098857
INFO:root:current train perplexity3.5755224227905273
INFO:root:current mean train loss 1616.20284371627
INFO:root:current train perplexity3.5772626399993896
INFO:root:current mean train loss 1616.5123770331259
INFO:root:current train perplexity3.5820391178131104
INFO:root:current mean train loss 1619.2939268810915
INFO:root:current train perplexity3.586655855178833
INFO:root:current mean train loss 1620.5568019514042
INFO:root:current train perplexity3.5896666049957275
INFO:root:current mean train loss 1622.0860011993893
INFO:root:current train perplexity3.5910675525665283
INFO:root:current mean train loss 1621.810217020744
INFO:root:current train perplexity3.5933594703674316
INFO:root:current mean train loss 1622.9965833449205
INFO:root:current train perplexity3.596081018447876
INFO:root:current mean train loss 1623.5560688170453
INFO:root:current train perplexity3.5966572761535645
INFO:root:current mean train loss 1624.2512395165184
INFO:root:current train perplexity3.5988733768463135
INFO:root:current mean train loss 1624.2884831517065
INFO:root:current train perplexity3.6004748344421387
INFO:root:current mean train loss 1624.6142354177598
INFO:root:current train perplexity3.6026291847229004
INFO:root:current mean train loss 1625.814012585535
INFO:root:current train perplexity3.606017589569092
INFO:root:current mean train loss 1626.871492706569
INFO:root:current train perplexity3.6079137325286865
INFO:root:current mean train loss 1627.2424928677158
INFO:root:current train perplexity3.6072726249694824

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.26s/it]
INFO:root:final mean train loss: 1627.2857170778275
INFO:root:final train perplexity: 3.608800172805786
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2135.1173182277817
INFO:root:eval perplexity: 5.622394561767578
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/78
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [4:18:26<6:44:22, 198.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1633.0400341796876
INFO:root:current train perplexity3.5390048027038574
INFO:root:current mean train loss 1617.819896484375
INFO:root:current train perplexity3.5492637157440186
INFO:root:current mean train loss 1609.1011593967014
INFO:root:current train perplexity3.5493674278259277
INFO:root:current mean train loss 1612.5592116135817
INFO:root:current train perplexity3.5551414489746094
INFO:root:current mean train loss 1612.0609670840993
INFO:root:current train perplexity3.5561349391937256
INFO:root:current mean train loss 1612.7480666387648
INFO:root:current train perplexity3.5567831993103027
INFO:root:current mean train loss 1613.454353515625
INFO:root:current train perplexity3.560372829437256
INFO:root:current mean train loss 1614.1341823814655
INFO:root:current train perplexity3.564580202102661
INFO:root:current mean train loss 1614.5848273259944
INFO:root:current train perplexity3.5699384212493896
INFO:root:current mean train loss 1614.6836023279138
INFO:root:current train perplexity3.572960376739502
INFO:root:current mean train loss 1616.6877271103278
INFO:root:current train perplexity3.5795695781707764
INFO:root:current mean train loss 1617.2230631510417
INFO:root:current train perplexity3.582125663757324
INFO:root:current mean train loss 1618.5180373086735
INFO:root:current train perplexity3.5834414958953857
INFO:root:current mean train loss 1619.9961789688973
INFO:root:current train perplexity3.584242582321167
INFO:root:current mean train loss 1620.5855799410635
INFO:root:current train perplexity3.585726499557495
INFO:root:current mean train loss 1620.5560324346823
INFO:root:current train perplexity3.5884909629821777
INFO:root:current mean train loss 1620.7732475961539
INFO:root:current train perplexity3.5896012783050537
INFO:root:current mean train loss 1620.6007172780796
INFO:root:current train perplexity3.589576005935669
INFO:root:current mean train loss 1621.9754242695847
INFO:root:current train perplexity3.5925731658935547
INFO:root:current mean train loss 1623.0203131975447
INFO:root:current train perplexity3.595301628112793

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.05s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.05s/it]
INFO:root:final mean train loss: 1622.5597538180984
INFO:root:final train perplexity: 3.595374584197998
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it]
INFO:root:eval mean loss: 2137.7871699772827
INFO:root:eval perplexity: 5.634547233581543
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/79
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/200 [4:21:45<6:40:55, 198.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1611.82905796596
INFO:root:current train perplexity3.5731732845306396
INFO:root:current mean train loss 1614.4924333599251
INFO:root:current train perplexity3.5452356338500977
INFO:root:current mean train loss 1613.5393495165613
INFO:root:current train perplexity3.5593209266662598
INFO:root:current mean train loss 1614.2449901201571
INFO:root:current train perplexity3.5643234252929688
INFO:root:current mean train loss 1614.7880041890555
INFO:root:current train perplexity3.5650196075439453
INFO:root:current mean train loss 1614.9094429719933
INFO:root:current train perplexity3.569544792175293
INFO:root:current mean train loss 1615.627051617869
INFO:root:current train perplexity3.5684845447540283
INFO:root:current mean train loss 1615.6557446091645
INFO:root:current train perplexity3.5727076530456543
INFO:root:current mean train loss 1614.7283146874072
INFO:root:current train perplexity3.5730345249176025
INFO:root:current mean train loss 1614.698993269805
INFO:root:current train perplexity3.569056987762451
INFO:root:current mean train loss 1615.0271529126305
INFO:root:current train perplexity3.571876049041748
INFO:root:current mean train loss 1615.5562629766514
INFO:root:current train perplexity3.5727903842926025
INFO:root:current mean train loss 1616.2366244551065
INFO:root:current train perplexity3.5710296630859375
INFO:root:current mean train loss 1617.5419135967772
INFO:root:current train perplexity3.572567939758301
INFO:root:current mean train loss 1616.8128901678713
INFO:root:current train perplexity3.573880910873413
INFO:root:current mean train loss 1617.147957055949
INFO:root:current train perplexity3.575155019760132
INFO:root:current mean train loss 1617.5350597534923
INFO:root:current train perplexity3.5758535861968994
INFO:root:current mean train loss 1616.9580376643676
INFO:root:current train perplexity3.57765793800354
INFO:root:current mean train loss 1617.9260550798215
INFO:root:current train perplexity3.580512762069702
INFO:root:current mean train loss 1618.166826556566
INFO:root:current train perplexity3.5816571712493896

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.82s/it]
INFO:root:final mean train loss: 1617.878223044068
INFO:root:final train perplexity: 3.582124948501587
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it]
INFO:root:eval mean loss: 2140.6534839386636
INFO:root:eval perplexity: 5.6476240158081055
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/80
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/200 [4:25:03<6:37:22, 198.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1593.1920952231196
INFO:root:current train perplexity3.5026214122772217
INFO:root:current mean train loss 1598.5942973970616
INFO:root:current train perplexity3.539173126220703
INFO:root:current mean train loss 1599.0204465416869
INFO:root:current train perplexity3.53671932220459
INFO:root:current mean train loss 1603.6010051929186
INFO:root:current train perplexity3.539130926132202
INFO:root:current mean train loss 1604.3399099711498
INFO:root:current train perplexity3.538788080215454
INFO:root:current mean train loss 1605.7204749255786
INFO:root:current train perplexity3.5405399799346924
INFO:root:current mean train loss 1605.0335178404187
INFO:root:current train perplexity3.5432968139648438
INFO:root:current mean train loss 1606.5442499253747
INFO:root:current train perplexity3.5472073554992676
INFO:root:current mean train loss 1607.4362906654721
INFO:root:current train perplexity3.547698736190796
INFO:root:current mean train loss 1607.6320420186637
INFO:root:current train perplexity3.5522518157958984
INFO:root:current mean train loss 1608.1002274496134
INFO:root:current train perplexity3.5557918548583984
INFO:root:current mean train loss 1610.393422084805
INFO:root:current train perplexity3.5601840019226074
INFO:root:current mean train loss 1611.3394853926727
INFO:root:current train perplexity3.562305450439453
INFO:root:current mean train loss 1611.8119333925451
INFO:root:current train perplexity3.563586711883545
INFO:root:current mean train loss 1612.8064934545548
INFO:root:current train perplexity3.5641722679138184
INFO:root:current mean train loss 1613.5778137559382
INFO:root:current train perplexity3.5663201808929443
INFO:root:current mean train loss 1613.1735467525618
INFO:root:current train perplexity3.5673904418945312
INFO:root:current mean train loss 1614.4292362076508
INFO:root:current train perplexity3.569324254989624
INFO:root:current mean train loss 1614.6093547096684
INFO:root:current train perplexity3.572458505630493
INFO:root:current mean train loss 1614.5103301768281
INFO:root:current train perplexity3.5718019008636475

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.46s/it]
INFO:root:final mean train loss: 1614.4992471715145
INFO:root:final train perplexity: 3.5725910663604736
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2141.17734652039
INFO:root:eval perplexity: 5.650017738342285
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/81
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/200 [4:28:22<6:34:16, 198.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1610.580900493421
INFO:root:current train perplexity3.5376458168029785
INFO:root:current mean train loss 1598.5464838201349
INFO:root:current train perplexity3.5177252292633057
INFO:root:current mean train loss 1599.8156477333844
INFO:root:current train perplexity3.515655994415283
INFO:root:current mean train loss 1595.9929783597906
INFO:root:current train perplexity3.519108772277832
INFO:root:current mean train loss 1596.2479363449481
INFO:root:current train perplexity3.5264017581939697
INFO:root:current mean train loss 1596.183151457045
INFO:root:current train perplexity3.528278350830078
INFO:root:current mean train loss 1597.728280874399
INFO:root:current train perplexity3.5304720401763916
INFO:root:current mean train loss 1601.5129657234113
INFO:root:current train perplexity3.5374248027801514
INFO:root:current mean train loss 1603.2172639751
INFO:root:current train perplexity3.5387842655181885
INFO:root:current mean train loss 1604.2318365378458
INFO:root:current train perplexity3.540250539779663
INFO:root:current mean train loss 1604.903712616534
INFO:root:current train perplexity3.5418050289154053
INFO:root:current mean train loss 1606.4583498377378
INFO:root:current train perplexity3.5464694499969482
INFO:root:current mean train loss 1606.2097782146968
INFO:root:current train perplexity3.546543836593628
INFO:root:current mean train loss 1606.3988487775935
INFO:root:current train perplexity3.547435998916626
INFO:root:current mean train loss 1606.6362355136612
INFO:root:current train perplexity3.54940128326416
INFO:root:current mean train loss 1607.1062641434257
INFO:root:current train perplexity3.5528066158294678
INFO:root:current mean train loss 1608.0014499855497
INFO:root:current train perplexity3.5547893047332764
INFO:root:current mean train loss 1608.6575838381107
INFO:root:current train perplexity3.5567259788513184
INFO:root:current mean train loss 1609.874657474347
INFO:root:current train perplexity3.5576748847961426
INFO:root:current mean train loss 1610.6744350788565
INFO:root:current train perplexity3.560241222381592

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.40s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.40s/it]
INFO:root:final mean train loss: 1610.1524867193903
INFO:root:final train perplexity: 3.5603652000427246
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it]
INFO:root:eval mean loss: 2146.669505017869
INFO:root:eval perplexity: 5.675168991088867
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/82
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/200 [4:31:41<6:31:05, 198.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1589.8285502772178
INFO:root:current train perplexity3.519810676574707
INFO:root:current mean train loss 1594.079366575251
INFO:root:current train perplexity3.5146124362945557
INFO:root:current mean train loss 1587.3801473676142
INFO:root:current train perplexity3.508474111557007
INFO:root:current mean train loss 1588.6263145077926
INFO:root:current train perplexity3.510089635848999
INFO:root:current mean train loss 1589.5413464281187
INFO:root:current train perplexity3.507692813873291
INFO:root:current mean train loss 1591.5205423956577
INFO:root:current train perplexity3.508080005645752
INFO:root:current mean train loss 1593.402804376071
INFO:root:current train perplexity3.513916254043579
INFO:root:current mean train loss 1595.1026346560234
INFO:root:current train perplexity3.519812822341919
INFO:root:current mean train loss 1596.5712998615534
INFO:root:current train perplexity3.5210249423980713
INFO:root:current mean train loss 1596.662966325804
INFO:root:current train perplexity3.5246856212615967
INFO:root:current mean train loss 1597.3705379000958
INFO:root:current train perplexity3.5277655124664307
INFO:root:current mean train loss 1598.1289908704093
INFO:root:current train perplexity3.530963659286499
INFO:root:current mean train loss 1598.7835973752901
INFO:root:current train perplexity3.534858465194702
INFO:root:current mean train loss 1600.5281297145605
INFO:root:current train perplexity3.537240505218506
INFO:root:current mean train loss 1601.0885291843708
INFO:root:current train perplexity3.536888360977173
INFO:root:current mean train loss 1602.1358758288213
INFO:root:current train perplexity3.5374972820281982
INFO:root:current mean train loss 1603.6377166549764
INFO:root:current train perplexity3.540081262588501
INFO:root:current mean train loss 1603.9818279311037
INFO:root:current train perplexity3.542402982711792
INFO:root:current mean train loss 1605.413414876044
INFO:root:current train perplexity3.5452401638031006

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.16s/it]
INFO:root:final mean train loss: 1605.5263193874002
INFO:root:final train perplexity: 3.547398805618286
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2146.4105588223074
INFO:root:eval perplexity: 5.673981189727783
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/83
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/200 [4:35:00<6:27:43, 198.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1599.9983764648437
INFO:root:current train perplexity3.4467480182647705
INFO:root:current mean train loss 1594.5120749733665
INFO:root:current train perplexity3.506359577178955
INFO:root:current mean train loss 1592.8814115978423
INFO:root:current train perplexity3.505229949951172
INFO:root:current mean train loss 1596.2403627457156
INFO:root:current train perplexity3.504709243774414
INFO:root:current mean train loss 1594.4144757526676
INFO:root:current train perplexity3.50415301322937
INFO:root:current mean train loss 1594.001604386872
INFO:root:current train perplexity3.509338617324829
INFO:root:current mean train loss 1595.1862422755507
INFO:root:current train perplexity3.5105690956115723
INFO:root:current mean train loss 1594.9416957801498
INFO:root:current train perplexity3.5145764350891113
INFO:root:current mean train loss 1594.703921169705
INFO:root:current train perplexity3.514948844909668
INFO:root:current mean train loss 1595.5475741543612
INFO:root:current train perplexity3.5177218914031982
INFO:root:current mean train loss 1595.5254149182008
INFO:root:current train perplexity3.5211353302001953
INFO:root:current mean train loss 1595.7572596644495
INFO:root:current train perplexity3.5218186378479004
INFO:root:current mean train loss 1596.404518518369
INFO:root:current train perplexity3.5248684883117676
INFO:root:current mean train loss 1597.5871317390267
INFO:root:current train perplexity3.526838541030884
INFO:root:current mean train loss 1598.2817211394615
INFO:root:current train perplexity3.52817702293396
INFO:root:current mean train loss 1598.7648384144763
INFO:root:current train perplexity3.529465913772583
INFO:root:current mean train loss 1599.702991480857
INFO:root:current train perplexity3.531548261642456
INFO:root:current mean train loss 1600.8033408717106
INFO:root:current train perplexity3.53316330909729
INFO:root:current mean train loss 1601.416212556112
INFO:root:current train perplexity3.534550428390503
INFO:root:current mean train loss 1601.6294703298838
INFO:root:current train perplexity3.5349392890930176

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.14s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.14s/it]
INFO:root:final mean train loss: 1601.38166443579
INFO:root:final train perplexity: 3.5358223915100098
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it]
INFO:root:eval mean loss: 2148.033650716146
INFO:root:eval perplexity: 5.681432247161865
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/84
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/200 [4:38:19<6:24:21, 198.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1575.0962547019676
INFO:root:current train perplexity3.4993598461151123
INFO:root:current mean train loss 1571.1651736281988
INFO:root:current train perplexity3.4764320850372314
INFO:root:current mean train loss 1574.7820975013767
INFO:root:current train perplexity3.487941026687622
INFO:root:current mean train loss 1581.001473803039
INFO:root:current train perplexity3.488856554031372
INFO:root:current mean train loss 1585.795177164904
INFO:root:current train perplexity3.4936060905456543
INFO:root:current mean train loss 1587.7662508709382
INFO:root:current train perplexity3.5013327598571777
INFO:root:current mean train loss 1588.0442229397179
INFO:root:current train perplexity3.5054569244384766
INFO:root:current mean train loss 1589.3897654705231
INFO:root:current train perplexity3.509753704071045
INFO:root:current mean train loss 1590.172950458642
INFO:root:current train perplexity3.5117383003234863
INFO:root:current mean train loss 1591.876064921917
INFO:root:current train perplexity3.5126986503601074
INFO:root:current mean train loss 1593.4005779500364
INFO:root:current train perplexity3.514364004135132
INFO:root:current mean train loss 1594.2493256346356
INFO:root:current train perplexity3.5166077613830566
INFO:root:current mean train loss 1593.7893232549218
INFO:root:current train perplexity3.515728712081909
INFO:root:current mean train loss 1594.6311858463935
INFO:root:current train perplexity3.517031192779541
INFO:root:current mean train loss 1594.8605628544915
INFO:root:current train perplexity3.5174074172973633
INFO:root:current mean train loss 1595.9895993215864
INFO:root:current train perplexity3.520681142807007
INFO:root:current mean train loss 1596.6666630403206
INFO:root:current train perplexity3.52193284034729
INFO:root:current mean train loss 1597.2050173372359
INFO:root:current train perplexity3.5235073566436768
INFO:root:current mean train loss 1598.065303140608
INFO:root:current train perplexity3.52579665184021
INFO:root:current mean train loss 1598.5558456793065
INFO:root:current train perplexity3.526773691177368

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.24s/it]
INFO:root:final mean train loss: 1598.2869191595357
INFO:root:final train perplexity: 3.52720308303833
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.33s/it]
INFO:root:eval mean loss: 2151.6010517093305
INFO:root:eval perplexity: 5.697847366333008
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/85
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/200 [4:41:38<6:21:06, 198.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1585.3669572310014
INFO:root:current train perplexity3.4590418338775635
INFO:root:current mean train loss 1591.5409206814236
INFO:root:current train perplexity3.4879775047302246
INFO:root:current mean train loss 1584.071954946049
INFO:root:current train perplexity3.4804301261901855
INFO:root:current mean train loss 1585.1558795307958
INFO:root:current train perplexity3.4879133701324463
INFO:root:current mean train loss 1584.407509193764
INFO:root:current train perplexity3.4912779331207275
INFO:root:current mean train loss 1585.418463089887
INFO:root:current train perplexity3.4953291416168213
INFO:root:current mean train loss 1586.5948002975178
INFO:root:current train perplexity3.4968631267547607
INFO:root:current mean train loss 1588.135040611349
INFO:root:current train perplexity3.496755361557007
INFO:root:current mean train loss 1590.2227404264477
INFO:root:current train perplexity3.499424457550049
INFO:root:current mean train loss 1590.5241211713371
INFO:root:current train perplexity3.503390312194824
INFO:root:current mean train loss 1590.305889845808
INFO:root:current train perplexity3.501678705215454
INFO:root:current mean train loss 1591.6868698013413
INFO:root:current train perplexity3.50232195854187
INFO:root:current mean train loss 1592.6616949835775
INFO:root:current train perplexity3.5041069984436035
INFO:root:current mean train loss 1592.6723035176594
INFO:root:current train perplexity3.505413770675659
INFO:root:current mean train loss 1593.300025073445
INFO:root:current train perplexity3.506890058517456
INFO:root:current mean train loss 1592.600300843234
INFO:root:current train perplexity3.5074849128723145
INFO:root:current mean train loss 1593.308609342923
INFO:root:current train perplexity3.5108487606048584
INFO:root:current mean train loss 1594.4707335025892
INFO:root:current train perplexity3.513522148132324
INFO:root:current mean train loss 1594.579921631389
INFO:root:current train perplexity3.5148849487304688
INFO:root:current mean train loss 1594.6068082581821
INFO:root:current train perplexity3.5156750679016113

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.14s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.14s/it]
INFO:root:final mean train loss: 1594.1915205639539
INFO:root:final train perplexity: 3.51582932472229
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it]
INFO:root:eval mean loss: 2154.4537535322474
INFO:root:eval perplexity: 5.711009979248047
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/86
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/200 [4:44:57<6:17:45, 198.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1567.2033351210298
INFO:root:current train perplexity3.41410756111145
INFO:root:current mean train loss 1576.7877053207492
INFO:root:current train perplexity3.459073066711426
INFO:root:current mean train loss 1578.3967766889666
INFO:root:current train perplexity3.4693307876586914
INFO:root:current mean train loss 1578.6588554065313
INFO:root:current train perplexity3.4737443923950195
INFO:root:current mean train loss 1579.3257810381642
INFO:root:current train perplexity3.4781241416931152
INFO:root:current mean train loss 1578.2065582003397
INFO:root:current train perplexity3.4819369316101074
INFO:root:current mean train loss 1577.8951046665209
INFO:root:current train perplexity3.479914903640747
INFO:root:current mean train loss 1578.1259850641118
INFO:root:current train perplexity3.4797005653381348
INFO:root:current mean train loss 1580.531617203379
INFO:root:current train perplexity3.481902599334717
INFO:root:current mean train loss 1581.6538352688442
INFO:root:current train perplexity3.481268882751465
INFO:root:current mean train loss 1584.6601565951564
INFO:root:current train perplexity3.487244129180908
INFO:root:current mean train loss 1584.3296943552837
INFO:root:current train perplexity3.488820791244507
INFO:root:current mean train loss 1585.2943078448714
INFO:root:current train perplexity3.4907987117767334
INFO:root:current mean train loss 1585.9347792043131
INFO:root:current train perplexity3.490736484527588
INFO:root:current mean train loss 1587.1492437823515
INFO:root:current train perplexity3.4929819107055664
INFO:root:current mean train loss 1588.0946493790289
INFO:root:current train perplexity3.4960849285125732
INFO:root:current mean train loss 1588.7379869877898
INFO:root:current train perplexity3.4983420372009277
INFO:root:current mean train loss 1589.2503918588736
INFO:root:current train perplexity3.501019239425659
INFO:root:current mean train loss 1589.670403072105
INFO:root:current train perplexity3.5029714107513428
INFO:root:current mean train loss 1590.4933259099312
INFO:root:current train perplexity3.5046253204345703

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.16s/it]
INFO:root:final mean train loss: 1590.1442026204672
INFO:root:final train perplexity: 3.504624605178833
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2154.8215470550754
INFO:root:eval perplexity: 5.712709426879883
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/87
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/200 [4:48:15<6:14:24, 198.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1590.3493401943108
INFO:root:current train perplexity3.4633500576019287
INFO:root:current mean train loss 1576.0930923290466
INFO:root:current train perplexity3.462099313735962
INFO:root:current mean train loss 1577.531603476984
INFO:root:current train perplexity3.459622383117676
INFO:root:current mean train loss 1580.69854025866
INFO:root:current train perplexity3.471574306488037
INFO:root:current mean train loss 1578.1394666089175
INFO:root:current train perplexity3.470241069793701
INFO:root:current mean train loss 1578.6188764209153
INFO:root:current train perplexity3.4749772548675537
INFO:root:current mean train loss 1579.5679018813953
INFO:root:current train perplexity3.4772262573242188
INFO:root:current mean train loss 1580.4183169171251
INFO:root:current train perplexity3.4791603088378906
INFO:root:current mean train loss 1581.4501128663778
INFO:root:current train perplexity3.479275703430176
INFO:root:current mean train loss 1584.0649401580872
INFO:root:current train perplexity3.4813344478607178
INFO:root:current mean train loss 1584.9037880977141
INFO:root:current train perplexity3.4833896160125732
INFO:root:current mean train loss 1584.7504803021209
INFO:root:current train perplexity3.485283851623535
INFO:root:current mean train loss 1585.0361057812806
INFO:root:current train perplexity3.4858036041259766
INFO:root:current mean train loss 1584.523083513811
INFO:root:current train perplexity3.486654281616211
INFO:root:current mean train loss 1585.2087232205154
INFO:root:current train perplexity3.489086151123047
INFO:root:current mean train loss 1585.486809211992
INFO:root:current train perplexity3.4908523559570312
INFO:root:current mean train loss 1585.8358417642842
INFO:root:current train perplexity3.4908902645111084
INFO:root:current mean train loss 1585.984953701161
INFO:root:current train perplexity3.493403673171997
INFO:root:current mean train loss 1586.1242193480016
INFO:root:current train perplexity3.4937074184417725
INFO:root:current mean train loss 1586.8656311343727
INFO:root:current train perplexity3.494325876235962

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.47s/it]
INFO:root:final mean train loss: 1586.4151072999894
INFO:root:final train perplexity: 3.4943325519561768
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2158.1488595516125
INFO:root:eval perplexity: 5.7281012535095215
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/88
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/200 [4:51:34<6:11:15, 198.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1572.4697805304277
INFO:root:current train perplexity3.485776662826538
INFO:root:current mean train loss 1569.1225805038061
INFO:root:current train perplexity3.467740774154663
INFO:root:current mean train loss 1567.3094172073622
INFO:root:current train perplexity3.46765398979187
INFO:root:current mean train loss 1571.0798531447786
INFO:root:current train perplexity3.4669981002807617
INFO:root:current mean train loss 1570.79755785393
INFO:root:current train perplexity3.4625349044799805
INFO:root:current mean train loss 1571.5473483045562
INFO:root:current train perplexity3.4652585983276367
INFO:root:current mean train loss 1575.3745310392312
INFO:root:current train perplexity3.467076539993286
INFO:root:current mean train loss 1577.0028016288325
INFO:root:current train perplexity3.469965696334839
INFO:root:current mean train loss 1577.3030152049143
INFO:root:current train perplexity3.4728565216064453
INFO:root:current mean train loss 1577.7852677695116
INFO:root:current train perplexity3.4758076667785645
INFO:root:current mean train loss 1577.9600394192353
INFO:root:current train perplexity3.476236343383789
INFO:root:current mean train loss 1579.8633183307727
INFO:root:current train perplexity3.4773738384246826
INFO:root:current mean train loss 1579.9505648226352
INFO:root:current train perplexity3.4790666103363037
INFO:root:current mean train loss 1580.6037773542507
INFO:root:current train perplexity3.479517936706543
INFO:root:current mean train loss 1581.3266363954065
INFO:root:current train perplexity3.480097770690918
INFO:root:current mean train loss 1580.6801060595856
INFO:root:current train perplexity3.479671001434326
INFO:root:current mean train loss 1581.0449899318999
INFO:root:current train perplexity3.479515552520752
INFO:root:current mean train loss 1581.850943239685
INFO:root:current train perplexity3.4801559448242188
INFO:root:current mean train loss 1583.122696381823
INFO:root:current train perplexity3.4835853576660156

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.94s/it]
INFO:root:final mean train loss: 1582.5298000226041
INFO:root:final train perplexity: 3.4836413860321045
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2160.916551955203
INFO:root:eval perplexity: 5.740937232971191
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/89
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/200 [4:54:53<6:07:45, 198.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1537.4790954589844
INFO:root:current train perplexity3.435558557510376
INFO:root:current mean train loss 1562.5918786185127
INFO:root:current train perplexity3.4326298236846924
INFO:root:current mean train loss 1562.2688345279334
INFO:root:current train perplexity3.4319329261779785
INFO:root:current mean train loss 1563.2193748278496
INFO:root:current train perplexity3.4360859394073486
INFO:root:current mean train loss 1564.5643058702783
INFO:root:current train perplexity3.4363763332366943
INFO:root:current mean train loss 1566.2071385383606
INFO:root:current train perplexity3.440009117126465
INFO:root:current mean train loss 1568.2229799756817
INFO:root:current train perplexity3.442305088043213
INFO:root:current mean train loss 1570.913652398613
INFO:root:current train perplexity3.4464175701141357
INFO:root:current mean train loss 1571.0396557136123
INFO:root:current train perplexity3.4463226795196533
INFO:root:current mean train loss 1571.0765348735608
INFO:root:current train perplexity3.448573112487793
INFO:root:current mean train loss 1572.9243712896414
INFO:root:current train perplexity3.4531102180480957
INFO:root:current mean train loss 1573.220580725361
INFO:root:current train perplexity3.455894947052002
INFO:root:current mean train loss 1573.1962367898166
INFO:root:current train perplexity3.4582085609436035
INFO:root:current mean train loss 1574.558592819586
INFO:root:current train perplexity3.4599924087524414
INFO:root:current mean train loss 1575.2007280301102
INFO:root:current train perplexity3.4634852409362793
INFO:root:current mean train loss 1576.1947056200138
INFO:root:current train perplexity3.464599370956421
INFO:root:current mean train loss 1576.7683933153933
INFO:root:current train perplexity3.4668235778808594
INFO:root:current mean train loss 1577.696708465291
INFO:root:current train perplexity3.4690287113189697
INFO:root:current mean train loss 1578.1749715573478
INFO:root:current train perplexity3.4708638191223145
INFO:root:current mean train loss 1578.1335047638067
INFO:root:current train perplexity3.4712414741516113

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.55s/it]
INFO:root:final mean train loss: 1578.6758207397154
INFO:root:final train perplexity: 3.473069429397583
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2160.90791439841
INFO:root:eval perplexity: 5.740896224975586
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/90
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/200 [4:58:12<6:04:38, 198.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1590.086354222791
INFO:root:current train perplexity3.4407761096954346
INFO:root:current mean train loss 1567.1889752528464
INFO:root:current train perplexity3.442230701446533
INFO:root:current mean train loss 1571.719105549775
INFO:root:current train perplexity3.438002347946167
INFO:root:current mean train loss 1570.8417334281203
INFO:root:current train perplexity3.4430360794067383
INFO:root:current mean train loss 1569.2559068942126
INFO:root:current train perplexity3.4360275268554688
INFO:root:current mean train loss 1567.6997146462222
INFO:root:current train perplexity3.4368975162506104
INFO:root:current mean train loss 1567.9479437071493
INFO:root:current train perplexity3.4352943897247314
INFO:root:current mean train loss 1569.804688002347
INFO:root:current train perplexity3.4385552406311035
INFO:root:current mean train loss 1571.711077976572
INFO:root:current train perplexity3.4443812370300293
INFO:root:current mean train loss 1572.2562965417703
INFO:root:current train perplexity3.446070909500122
INFO:root:current mean train loss 1573.8978568059479
INFO:root:current train perplexity3.4496636390686035
INFO:root:current mean train loss 1573.7994426933403
INFO:root:current train perplexity3.44915771484375
INFO:root:current mean train loss 1575.0832430138832
INFO:root:current train perplexity3.4530813694000244
INFO:root:current mean train loss 1573.9654429875611
INFO:root:current train perplexity3.4541854858398438
INFO:root:current mean train loss 1574.6131758372878
INFO:root:current train perplexity3.455303430557251
INFO:root:current mean train loss 1575.1385910004242
INFO:root:current train perplexity3.455339193344116
INFO:root:current mean train loss 1574.4849675917933
INFO:root:current train perplexity3.456270694732666
INFO:root:current mean train loss 1574.7055497442525
INFO:root:current train perplexity3.4568593502044678
INFO:root:current mean train loss 1574.9071228461164
INFO:root:current train perplexity3.4596517086029053
INFO:root:current mean train loss 1574.722473366017
INFO:root:current train perplexity3.460238218307495

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.26s/it]
INFO:root:final mean train loss: 1574.454210120743
INFO:root:final train perplexity: 3.4615249633789062
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2165.0476485448526
INFO:root:eval perplexity: 5.760148048400879
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/91
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/200 [5:01:31<6:01:18, 198.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1543.5080221424932
INFO:root:current train perplexity3.3967268466949463
INFO:root:current mean train loss 1563.1650566205587
INFO:root:current train perplexity3.4205360412597656
INFO:root:current mean train loss 1559.998880029694
INFO:root:current train perplexity3.4078619480133057
INFO:root:current mean train loss 1561.4971927356169
INFO:root:current train perplexity3.4108612537384033
INFO:root:current mean train loss 1561.4299204189147
INFO:root:current train perplexity3.4181411266326904
INFO:root:current mean train loss 1563.0287535950379
INFO:root:current train perplexity3.4224085807800293
INFO:root:current mean train loss 1563.5089236043925
INFO:root:current train perplexity3.424220085144043
INFO:root:current mean train loss 1563.2912995284748
INFO:root:current train perplexity3.426894426345825
INFO:root:current mean train loss 1562.1326728261672
INFO:root:current train perplexity3.430349588394165
INFO:root:current mean train loss 1562.7733501410132
INFO:root:current train perplexity3.431835174560547
INFO:root:current mean train loss 1562.4351929177747
INFO:root:current train perplexity3.4325790405273438
INFO:root:current mean train loss 1564.475787470686
INFO:root:current train perplexity3.437540054321289
INFO:root:current mean train loss 1564.7724107769864
INFO:root:current train perplexity3.438732385635376
INFO:root:current mean train loss 1566.8008802847557
INFO:root:current train perplexity3.4415221214294434
INFO:root:current mean train loss 1568.5529189155968
INFO:root:current train perplexity3.444697856903076
INFO:root:current mean train loss 1569.9282468966032
INFO:root:current train perplexity3.4461681842803955
INFO:root:current mean train loss 1570.8884784610418
INFO:root:current train perplexity3.448319435119629
INFO:root:current mean train loss 1570.737883088242
INFO:root:current train perplexity3.4493210315704346
INFO:root:current mean train loss 1570.6810286202642
INFO:root:current train perplexity3.4508702754974365
INFO:root:current mean train loss 1571.626056353578
INFO:root:current train perplexity3.4518730640411377

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.45s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.45s/it]
INFO:root:final mean train loss: 1570.8469803387386
INFO:root:final train perplexity: 3.4516918659210205
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2167.1442663314497
INFO:root:eval perplexity: 5.769925117492676
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/92
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/200 [5:04:50<5:58:04, 198.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1544.1327795603918
INFO:root:current train perplexity3.404405117034912
INFO:root:current mean train loss 1554.6826224297834
INFO:root:current train perplexity3.398935317993164
INFO:root:current mean train loss 1558.9138378534933
INFO:root:current train perplexity3.4090378284454346
INFO:root:current mean train loss 1556.4821316637613
INFO:root:current train perplexity3.404599905014038
INFO:root:current mean train loss 1558.3621694346484
INFO:root:current train perplexity3.4069604873657227
INFO:root:current mean train loss 1559.3794140451544
INFO:root:current train perplexity3.4149346351623535
INFO:root:current mean train loss 1560.3303180309083
INFO:root:current train perplexity3.4206247329711914
INFO:root:current mean train loss 1562.2205695356017
INFO:root:current train perplexity3.4239799976348877
INFO:root:current mean train loss 1562.2814422289161
INFO:root:current train perplexity3.423582077026367
INFO:root:current mean train loss 1563.6603476582782
INFO:root:current train perplexity3.427830696105957
INFO:root:current mean train loss 1564.367148685545
INFO:root:current train perplexity3.4292924404144287
INFO:root:current mean train loss 1564.6399680454979
INFO:root:current train perplexity3.4325499534606934
INFO:root:current mean train loss 1565.1709657066508
INFO:root:current train perplexity3.43269944190979
INFO:root:current mean train loss 1565.0715230828423
INFO:root:current train perplexity3.435469627380371
INFO:root:current mean train loss 1565.1745673888202
INFO:root:current train perplexity3.435694932937622
INFO:root:current mean train loss 1564.7912076729197
INFO:root:current train perplexity3.4357447624206543
INFO:root:current mean train loss 1566.0280131915213
INFO:root:current train perplexity3.43835186958313
INFO:root:current mean train loss 1566.001565657349
INFO:root:current train perplexity3.4394328594207764
INFO:root:current mean train loss 1566.439514323965
INFO:root:current train perplexity3.4404475688934326
INFO:root:current mean train loss 1567.1239417256352
INFO:root:current train perplexity3.4400696754455566

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.92s/it]
INFO:root:final mean train loss: 1566.6729551037333
INFO:root:final train perplexity: 3.440347909927368
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it]
INFO:root:eval mean loss: 2171.0336424915504
INFO:root:eval perplexity: 5.788102626800537
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/93
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/200 [5:08:09<5:54:33, 198.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1553.047184753418
INFO:root:current train perplexity3.4079744815826416
INFO:root:current mean train loss 1555.8714497884114
INFO:root:current train perplexity3.406481981277466
INFO:root:current mean train loss 1557.0158591134207
INFO:root:current train perplexity3.4082188606262207
INFO:root:current mean train loss 1559.0149629291734
INFO:root:current train perplexity3.41119384765625
INFO:root:current mean train loss 1559.9097915649413
INFO:root:current train perplexity3.4148247241973877
INFO:root:current mean train loss 1558.6238472774112
INFO:root:current train perplexity3.412168025970459
INFO:root:current mean train loss 1559.9794338450713
INFO:root:current train perplexity3.413618803024292
INFO:root:current mean train loss 1559.2879486866486
INFO:root:current train perplexity3.4137179851531982
INFO:root:current mean train loss 1559.9550888061524
INFO:root:current train perplexity3.414454221725464
INFO:root:current mean train loss 1559.9971665985731
INFO:root:current train perplexity3.417971611022949
INFO:root:current mean train loss 1560.8480277732567
INFO:root:current train perplexity3.419132709503174
INFO:root:current mean train loss 1561.3455094676906
INFO:root:current train perplexity3.4204351902008057
INFO:root:current mean train loss 1561.5808364868165
INFO:root:current train perplexity3.423004150390625
INFO:root:current mean train loss 1561.9530607804008
INFO:root:current train perplexity3.424280881881714
INFO:root:current mean train loss 1562.15940271326
INFO:root:current train perplexity3.425778865814209
INFO:root:current mean train loss 1562.383815871613
INFO:root:current train perplexity3.426362991333008
INFO:root:current mean train loss 1562.9895257132393
INFO:root:current train perplexity3.4284262657165527
INFO:root:current mean train loss 1563.6598631440922
INFO:root:current train perplexity3.42952823638916
INFO:root:current mean train loss 1563.5575775795794
INFO:root:current train perplexity3.4299166202545166
INFO:root:current mean train loss 1563.680819794626
INFO:root:current train perplexity3.4309096336364746

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.97s/it]
INFO:root:final mean train loss: 1563.228258495552
INFO:root:final train perplexity: 3.4310142993927
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2173.908113087323
INFO:root:eval perplexity: 5.80157470703125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/94
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/200 [5:11:27<5:51:07, 198.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1538.51549789586
INFO:root:current train perplexity3.3654022216796875
INFO:root:current mean train loss 1550.993415019234
INFO:root:current train perplexity3.3890252113342285
INFO:root:current mean train loss 1549.5531120942499
INFO:root:current train perplexity3.3953006267547607
INFO:root:current mean train loss 1552.59144972794
INFO:root:current train perplexity3.402355194091797
INFO:root:current mean train loss 1552.5674778652383
INFO:root:current train perplexity3.4029805660247803
INFO:root:current mean train loss 1552.983838667622
INFO:root:current train perplexity3.4010400772094727
INFO:root:current mean train loss 1554.2519198490181
INFO:root:current train perplexity3.4030046463012695
INFO:root:current mean train loss 1553.1066557574302
INFO:root:current train perplexity3.405240535736084
INFO:root:current mean train loss 1553.1526411007612
INFO:root:current train perplexity3.4089715480804443
INFO:root:current mean train loss 1555.1298479177767
INFO:root:current train perplexity3.4118666648864746
INFO:root:current mean train loss 1555.1914111461656
INFO:root:current train perplexity3.4114317893981934
INFO:root:current mean train loss 1555.007693998995
INFO:root:current train perplexity3.412182092666626
INFO:root:current mean train loss 1555.1126734396082
INFO:root:current train perplexity3.412597179412842
INFO:root:current mean train loss 1555.373553243977
INFO:root:current train perplexity3.414024591445923
INFO:root:current mean train loss 1556.906515668055
INFO:root:current train perplexity3.4170925617218018
INFO:root:current mean train loss 1557.5085229843799
INFO:root:current train perplexity3.417257785797119
INFO:root:current mean train loss 1558.0939388241427
INFO:root:current train perplexity3.4172754287719727
INFO:root:current mean train loss 1558.9749164867878
INFO:root:current train perplexity3.418142318725586
INFO:root:current mean train loss 1559.20491860349
INFO:root:current train perplexity3.4200279712677

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.24s/it]
INFO:root:final mean train loss: 1559.5249435571006
INFO:root:final train perplexity: 3.4210076332092285
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2175.014825915614
INFO:root:eval perplexity: 5.80676794052124
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/95
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/200 [5:14:46<5:47:52, 198.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1527.6317749023438
INFO:root:current train perplexity3.338667869567871
INFO:root:current mean train loss 1535.5190493935033
INFO:root:current train perplexity3.3430588245391846
INFO:root:current mean train loss 1542.4211243246202
INFO:root:current train perplexity3.365888833999634
INFO:root:current mean train loss 1541.700889635997
INFO:root:current train perplexity3.361999750137329
INFO:root:current mean train loss 1543.201293060745
INFO:root:current train perplexity3.381035804748535
INFO:root:current mean train loss 1545.310586773468
INFO:root:current train perplexity3.382683277130127
INFO:root:current mean train loss 1545.6642491840773
INFO:root:current train perplexity3.3859925270080566
INFO:root:current mean train loss 1546.021081577162
INFO:root:current train perplexity3.388953924179077
INFO:root:current mean train loss 1547.815969256277
INFO:root:current train perplexity3.3915090560913086
INFO:root:current mean train loss 1550.2091879145582
INFO:root:current train perplexity3.395237445831299
INFO:root:current mean train loss 1551.3427172177408
INFO:root:current train perplexity3.3982465267181396
INFO:root:current mean train loss 1553.3734465073426
INFO:root:current train perplexity3.4020416736602783
INFO:root:current mean train loss 1554.1143518287622
INFO:root:current train perplexity3.4056835174560547
INFO:root:current mean train loss 1554.7879467736277
INFO:root:current train perplexity3.4070050716400146
INFO:root:current mean train loss 1555.4057308990177
INFO:root:current train perplexity3.4083878993988037
INFO:root:current mean train loss 1555.4767827748308
INFO:root:current train perplexity3.409958839416504
INFO:root:current mean train loss 1556.0433193050796
INFO:root:current train perplexity3.4096076488494873
INFO:root:current mean train loss 1555.6609353919011
INFO:root:current train perplexity3.410616159439087
INFO:root:current mean train loss 1556.3599311803378
INFO:root:current train perplexity3.4109914302825928
INFO:root:current mean train loss 1556.4750190822558
INFO:root:current train perplexity3.411529779434204

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.33s/it]
INFO:root:final mean train loss: 1556.5041666420434
INFO:root:final train perplexity: 3.412867307662964
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it]
INFO:root:eval mean loss: 2175.9117574488864
INFO:root:eval perplexity: 5.8109822273254395
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/96
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/200 [5:18:05<5:44:38, 198.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1540.6282565209174
INFO:root:current train perplexity3.3705239295959473
INFO:root:current mean train loss 1543.1585609494275
INFO:root:current train perplexity3.3791286945343018
INFO:root:current mean train loss 1540.3777172534496
INFO:root:current train perplexity3.3730366230010986
INFO:root:current mean train loss 1537.7123459184998
INFO:root:current train perplexity3.370723009109497
INFO:root:current mean train loss 1541.8558222157772
INFO:root:current train perplexity3.3743464946746826
INFO:root:current mean train loss 1543.1486701462452
INFO:root:current train perplexity3.3819143772125244
INFO:root:current mean train loss 1544.422801651025
INFO:root:current train perplexity3.3847107887268066
INFO:root:current mean train loss 1545.1055772940535
INFO:root:current train perplexity3.3858754634857178
INFO:root:current mean train loss 1546.2408445796668
INFO:root:current train perplexity3.3896780014038086
INFO:root:current mean train loss 1546.9618924281267
INFO:root:current train perplexity3.393305540084839
INFO:root:current mean train loss 1547.4832342168177
INFO:root:current train perplexity3.3972666263580322
INFO:root:current mean train loss 1547.8211493993838
INFO:root:current train perplexity3.3987221717834473
INFO:root:current mean train loss 1549.217824506721
INFO:root:current train perplexity3.3990893363952637
INFO:root:current mean train loss 1550.7503851955767
INFO:root:current train perplexity3.401280403137207
INFO:root:current mean train loss 1551.9050290409623
INFO:root:current train perplexity3.4036102294921875
INFO:root:current mean train loss 1552.5667163293242
INFO:root:current train perplexity3.4042060375213623
INFO:root:current mean train loss 1553.069448577488
INFO:root:current train perplexity3.4040188789367676
INFO:root:current mean train loss 1553.7321653228355
INFO:root:current train perplexity3.402982711791992
INFO:root:current mean train loss 1553.7458460759362
INFO:root:current train perplexity3.4037575721740723
INFO:root:current mean train loss 1553.9251923919155
INFO:root:current train perplexity3.404510736465454

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.37s/it]
INFO:root:final mean train loss: 1553.4111111439422
INFO:root:final train perplexity: 3.404552459716797
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2177.714457626884
INFO:root:eval perplexity: 5.819461345672607
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/97
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/200 [5:21:24<5:41:24, 198.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1535.430793762207
INFO:root:current train perplexity3.3761119842529297
INFO:root:current mean train loss 1539.8754132245037
INFO:root:current train perplexity3.359323263168335
INFO:root:current mean train loss 1537.6945421772618
INFO:root:current train perplexity3.3671000003814697
INFO:root:current mean train loss 1537.7365133351293
INFO:root:current train perplexity3.370788097381592
INFO:root:current mean train loss 1539.3394497462682
INFO:root:current train perplexity3.3759615421295166
INFO:root:current mean train loss 1539.7480506618529
INFO:root:current train perplexity3.379013776779175
INFO:root:current mean train loss 1543.6068322452497
INFO:root:current train perplexity3.3821120262145996
INFO:root:current mean train loss 1544.8552681826015
INFO:root:current train perplexity3.3841397762298584
INFO:root:current mean train loss 1544.5642685800228
INFO:root:current train perplexity3.383681535720825
INFO:root:current mean train loss 1546.0357672453933
INFO:root:current train perplexity3.3863961696624756
INFO:root:current mean train loss 1546.2794270988638
INFO:root:current train perplexity3.3869152069091797
INFO:root:current mean train loss 1548.2764844728265
INFO:root:current train perplexity3.3873064517974854
INFO:root:current mean train loss 1548.17558493981
INFO:root:current train perplexity3.3884475231170654
INFO:root:current mean train loss 1548.3966824223096
INFO:root:current train perplexity3.389666795730591
INFO:root:current mean train loss 1549.2304864535674
INFO:root:current train perplexity3.3914577960968018
INFO:root:current mean train loss 1549.0652320723816
INFO:root:current train perplexity3.389997720718384
INFO:root:current mean train loss 1548.6269093485712
INFO:root:current train perplexity3.3914055824279785
INFO:root:current mean train loss 1549.6427798063858
INFO:root:current train perplexity3.3925232887268066
INFO:root:current mean train loss 1549.5891191887133
INFO:root:current train perplexity3.3924124240875244
INFO:root:current mean train loss 1549.4548780374703
INFO:root:current train perplexity3.3927512168884277

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.77s/it]
INFO:root:final mean train loss: 1549.5498830513466
INFO:root:final train perplexity: 3.394200325012207
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2180.150300587323
INFO:root:eval perplexity: 5.830937385559082
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/98
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [5:24:42<5:37:49, 198.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1526.443584735577
INFO:root:current train perplexity3.3633718490600586
INFO:root:current mean train loss 1528.91992335464
INFO:root:current train perplexity3.359084129333496
INFO:root:current mean train loss 1536.198462835348
INFO:root:current train perplexity3.3644001483917236
INFO:root:current mean train loss 1541.5837813703981
INFO:root:current train perplexity3.366872549057007
INFO:root:current mean train loss 1540.36759125084
INFO:root:current train perplexity3.3644754886627197
INFO:root:current mean train loss 1540.4160635889104
INFO:root:current train perplexity3.3688552379608154
INFO:root:current mean train loss 1541.3454389758576
INFO:root:current train perplexity3.370313882827759
INFO:root:current mean train loss 1542.3479707605698
INFO:root:current train perplexity3.3743529319763184
INFO:root:current mean train loss 1542.5228308176029
INFO:root:current train perplexity3.3727192878723145
INFO:root:current mean train loss 1543.2917194583872
INFO:root:current train perplexity3.3740241527557373
INFO:root:current mean train loss 1542.9119648391652
INFO:root:current train perplexity3.3777222633361816
INFO:root:current mean train loss 1542.5074143307404
INFO:root:current train perplexity3.378272771835327
INFO:root:current mean train loss 1543.6607844537425
INFO:root:current train perplexity3.377924680709839
INFO:root:current mean train loss 1544.1442975725447
INFO:root:current train perplexity3.378317356109619
INFO:root:current mean train loss 1544.2450778583618
INFO:root:current train perplexity3.379883289337158
INFO:root:current mean train loss 1544.3083481273711
INFO:root:current train perplexity3.380889892578125
INFO:root:current mean train loss 1545.1870092993383
INFO:root:current train perplexity3.382645845413208
INFO:root:current mean train loss 1546.2425552324937
INFO:root:current train perplexity3.382776975631714
INFO:root:current mean train loss 1546.2408476719588
INFO:root:current train perplexity3.3840909004211426
INFO:root:current mean train loss 1546.746606321068
INFO:root:current train perplexity3.385535955429077

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.29s/it]
INFO:root:final mean train loss: 1546.4015538017977
INFO:root:final train perplexity: 3.3857834339141846
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it]
INFO:root:eval mean loss: 2182.1767270784853
INFO:root:eval perplexity: 5.8404998779296875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/99
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/200 [5:28:01<5:34:35, 198.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1528.9360083603278
INFO:root:current train perplexity3.3524603843688965
INFO:root:current mean train loss 1527.885478596111
INFO:root:current train perplexity3.3448657989501953
INFO:root:current mean train loss 1532.278198675061
INFO:root:current train perplexity3.3517074584960938
INFO:root:current mean train loss 1530.340321166353
INFO:root:current train perplexity3.3496041297912598
INFO:root:current mean train loss 1530.4648926287766
INFO:root:current train perplexity3.345412015914917
INFO:root:current mean train loss 1532.842643187218
INFO:root:current train perplexity3.3503384590148926
INFO:root:current mean train loss 1533.741739312225
INFO:root:current train perplexity3.3521974086761475
INFO:root:current mean train loss 1533.0461291535125
INFO:root:current train perplexity3.3524839878082275
INFO:root:current mean train loss 1534.6960882416117
INFO:root:current train perplexity3.3538589477539062
INFO:root:current mean train loss 1535.6024588839343
INFO:root:current train perplexity3.35672926902771
INFO:root:current mean train loss 1536.8646543717869
INFO:root:current train perplexity3.361060857772827
INFO:root:current mean train loss 1537.6172353160362
INFO:root:current train perplexity3.3617401123046875
INFO:root:current mean train loss 1539.0712594494992
INFO:root:current train perplexity3.3652429580688477
INFO:root:current mean train loss 1539.453423551126
INFO:root:current train perplexity3.366471529006958
INFO:root:current mean train loss 1539.6779591589966
INFO:root:current train perplexity3.3678345680236816
INFO:root:current mean train loss 1540.0453522847365
INFO:root:current train perplexity3.3699772357940674
INFO:root:current mean train loss 1541.2436930580457
INFO:root:current train perplexity3.3716890811920166
INFO:root:current mean train loss 1541.855747826573
INFO:root:current train perplexity3.374208450317383
INFO:root:current mean train loss 1542.8660337344747
INFO:root:current train perplexity3.3755276203155518
INFO:root:current mean train loss 1543.40229343141
INFO:root:current train perplexity3.376582622528076

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.22s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.22s/it]
INFO:root:final mean train loss: 1542.945271378987
INFO:root:final train perplexity: 3.3765668869018555
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it]
INFO:root:eval mean loss: 2184.1688154504654
INFO:root:eval perplexity: 5.849917411804199
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/100
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/200 [5:31:20<5:31:19, 198.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1528.7180890940656
INFO:root:current train perplexity3.349207639694214
INFO:root:current mean train loss 1527.9971715265783
INFO:root:current train perplexity3.3457210063934326
INFO:root:current mean train loss 1524.8555414206207
INFO:root:current train perplexity3.3408854007720947
INFO:root:current mean train loss 1527.6648818090148
INFO:root:current train perplexity3.3440604209899902
INFO:root:current mean train loss 1528.9309666696674
INFO:root:current train perplexity3.344456195831299
INFO:root:current mean train loss 1531.9416738264947
INFO:root:current train perplexity3.346193790435791
INFO:root:current mean train loss 1533.1923952116304
INFO:root:current train perplexity3.3482580184936523
INFO:root:current mean train loss 1534.2092932938633
INFO:root:current train perplexity3.3488922119140625
INFO:root:current mean train loss 1534.8913162791557
INFO:root:current train perplexity3.352979898452759
INFO:root:current mean train loss 1534.9021206264858
INFO:root:current train perplexity3.3555057048797607
INFO:root:current mean train loss 1536.3899702188423
INFO:root:current train perplexity3.357788324356079
INFO:root:current mean train loss 1536.4649618497185
INFO:root:current train perplexity3.3583691120147705
INFO:root:current mean train loss 1537.4626142517982
INFO:root:current train perplexity3.3603198528289795
INFO:root:current mean train loss 1538.6790601336334
INFO:root:current train perplexity3.362074375152588
INFO:root:current mean train loss 1539.7126192852527
INFO:root:current train perplexity3.3624916076660156
INFO:root:current mean train loss 1539.4215306991186
INFO:root:current train perplexity3.3639044761657715
INFO:root:current mean train loss 1539.6342184999678
INFO:root:current train perplexity3.3648157119750977
INFO:root:current mean train loss 1540.2630473418392
INFO:root:current train perplexity3.3662078380584717
INFO:root:current mean train loss 1540.6903528069872
INFO:root:current train perplexity3.3677117824554443

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.66s/it]
INFO:root:final mean train loss: 1540.104257372973
INFO:root:final train perplexity: 3.369009494781494
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.34s/it]
INFO:root:eval mean loss: 2187.044429697889
INFO:root:eval perplexity: 5.863537311553955
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/101
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/200 [5:34:39<5:28:14, 198.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1544.669906616211
INFO:root:current train perplexity3.363534688949585
INFO:root:current mean train loss 1531.9030656485722
INFO:root:current train perplexity3.3502047061920166
INFO:root:current mean train loss 1526.9332111499928
INFO:root:current train perplexity3.336972951889038
INFO:root:current mean train loss 1527.0785719473151
INFO:root:current train perplexity3.344151258468628
INFO:root:current mean train loss 1525.2968257023738
INFO:root:current train perplexity3.3381452560424805
INFO:root:current mean train loss 1530.3474830804869
INFO:root:current train perplexity3.3455698490142822
INFO:root:current mean train loss 1530.9390163669339
INFO:root:current train perplexity3.341588020324707
INFO:root:current mean train loss 1529.5461504206312
INFO:root:current train perplexity3.340442180633545
INFO:root:current mean train loss 1529.1781779270545
INFO:root:current train perplexity3.3429343700408936
INFO:root:current mean train loss 1528.958983841942
INFO:root:current train perplexity3.345414876937866
INFO:root:current mean train loss 1529.8654743104469
INFO:root:current train perplexity3.345515727996826
INFO:root:current mean train loss 1532.041025250616
INFO:root:current train perplexity3.3502349853515625
INFO:root:current mean train loss 1532.8598463158858
INFO:root:current train perplexity3.3518757820129395
INFO:root:current mean train loss 1532.3754139816144
INFO:root:current train perplexity3.353975296020508
INFO:root:current mean train loss 1533.343694482146
INFO:root:current train perplexity3.3537089824676514
INFO:root:current mean train loss 1533.0613855044885
INFO:root:current train perplexity3.354097604751587
INFO:root:current mean train loss 1533.6072704957264
INFO:root:current train perplexity3.3538105487823486
INFO:root:current mean train loss 1534.8750379869155
INFO:root:current train perplexity3.355297327041626
INFO:root:current mean train loss 1535.611888599816
INFO:root:current train perplexity3.3568007946014404
INFO:root:current mean train loss 1536.6420203987393
INFO:root:current train perplexity3.3582849502563477

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.26s/it]
INFO:root:final mean train loss: 1536.1434894048139
INFO:root:final train perplexity: 3.3585023880004883
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it]
INFO:root:eval mean loss: 2188.20331200133
INFO:root:eval perplexity: 5.869036674499512
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/102
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/200 [5:37:58<5:24:53, 198.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1512.5605986624053
INFO:root:current train perplexity3.366004467010498
INFO:root:current mean train loss 1529.4470343338817
INFO:root:current train perplexity3.3221802711486816
INFO:root:current mean train loss 1524.256742682068
INFO:root:current train perplexity3.326119899749756
INFO:root:current mean train loss 1523.6775404554946
INFO:root:current train perplexity3.325901746749878
INFO:root:current mean train loss 1526.925875410472
INFO:root:current train perplexity3.3269600868225098
INFO:root:current mean train loss 1528.5414794005776
INFO:root:current train perplexity3.3265957832336426
INFO:root:current mean train loss 1529.0891190418888
INFO:root:current train perplexity3.331970691680908
INFO:root:current mean train loss 1530.258008678483
INFO:root:current train perplexity3.336686372756958
INFO:root:current mean train loss 1530.0266960299746
INFO:root:current train perplexity3.3367762565612793
INFO:root:current mean train loss 1529.5583380957764
INFO:root:current train perplexity3.3361852169036865
INFO:root:current mean train loss 1529.6340596733573
INFO:root:current train perplexity3.339221477508545
INFO:root:current mean train loss 1531.315426132054
INFO:root:current train perplexity3.3418965339660645
INFO:root:current mean train loss 1531.9728066350808
INFO:root:current train perplexity3.3424673080444336
INFO:root:current mean train loss 1532.5728422767313
INFO:root:current train perplexity3.34397029876709
INFO:root:current mean train loss 1532.4146495108328
INFO:root:current train perplexity3.3446946144104004
INFO:root:current mean train loss 1532.8247422269956
INFO:root:current train perplexity3.3457984924316406
INFO:root:current mean train loss 1533.4503482554635
INFO:root:current train perplexity3.3471932411193848
INFO:root:current mean train loss 1533.173631037444
INFO:root:current train perplexity3.349640369415283
INFO:root:current mean train loss 1534.4978996980276
INFO:root:current train perplexity3.350881338119507
INFO:root:current mean train loss 1534.2902595847613
INFO:root:current train perplexity3.351351499557495

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.13s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.13s/it]
INFO:root:final mean train loss: 1533.5391576385114
INFO:root:final train perplexity: 3.3516111373901367
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2194.231249220828
INFO:root:eval perplexity: 5.8977179527282715
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/103
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/200 [5:41:17<5:21:30, 198.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1520.60978515625
INFO:root:current train perplexity3.334202527999878
INFO:root:current mean train loss 1516.0041967773439
INFO:root:current train perplexity3.311203718185425
INFO:root:current mean train loss 1515.5165888671875
INFO:root:current train perplexity3.311485528945923
INFO:root:current mean train loss 1518.1932767159599
INFO:root:current train perplexity3.3127102851867676
INFO:root:current mean train loss 1519.0885826280382
INFO:root:current train perplexity3.3179519176483154
INFO:root:current mean train loss 1521.5721404474432
INFO:root:current train perplexity3.3242897987365723
INFO:root:current mean train loss 1521.9225769981972
INFO:root:current train perplexity3.326317310333252
INFO:root:current mean train loss 1522.989633626302
INFO:root:current train perplexity3.3285481929779053
INFO:root:current mean train loss 1522.9012903550092
INFO:root:current train perplexity3.3296446800231934
INFO:root:current mean train loss 1522.5858533357318
INFO:root:current train perplexity3.3278403282165527
INFO:root:current mean train loss 1524.487984328497
INFO:root:current train perplexity3.3308825492858887
INFO:root:current mean train loss 1524.6177524201767
INFO:root:current train perplexity3.3313140869140625
INFO:root:current mean train loss 1525.9081118164063
INFO:root:current train perplexity3.332803249359131
INFO:root:current mean train loss 1526.125288990162
INFO:root:current train perplexity3.332214117050171
INFO:root:current mean train loss 1527.73606386382
INFO:root:current train perplexity3.335350275039673
INFO:root:current mean train loss 1528.770476231729
INFO:root:current train perplexity3.3375144004821777
INFO:root:current mean train loss 1529.6724012340198
INFO:root:current train perplexity3.3396248817443848
INFO:root:current mean train loss 1530.0503422154018
INFO:root:current train perplexity3.341728687286377
INFO:root:current mean train loss 1530.474619140625
INFO:root:current train perplexity3.3438148498535156
INFO:root:current mean train loss 1530.7553709059496
INFO:root:current train perplexity3.3437371253967285

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.41s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.41s/it]
INFO:root:final mean train loss: 1530.388412160099
INFO:root:final train perplexity: 3.3432939052581787
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2193.7551247887577
INFO:root:eval perplexity: 5.895448207855225
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/104
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/200 [5:44:36<5:18:15, 198.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1525.7210748017724
INFO:root:current train perplexity3.3472118377685547
INFO:root:current mean train loss 1513.460841013286
INFO:root:current train perplexity3.3177225589752197
INFO:root:current mean train loss 1516.202447898379
INFO:root:current train perplexity3.3164615631103516
INFO:root:current mean train loss 1519.3952999270905
INFO:root:current train perplexity3.320251941680908
INFO:root:current mean train loss 1525.0422820718181
INFO:root:current train perplexity3.322035551071167
INFO:root:current mean train loss 1524.0413191860946
INFO:root:current train perplexity3.322606325149536
INFO:root:current mean train loss 1524.3494785200173
INFO:root:current train perplexity3.3187036514282227
INFO:root:current mean train loss 1525.138912355107
INFO:root:current train perplexity3.320681095123291
INFO:root:current mean train loss 1524.8882438545272
INFO:root:current train perplexity3.3215017318725586
INFO:root:current mean train loss 1524.9752729981983
INFO:root:current train perplexity3.321716070175171
INFO:root:current mean train loss 1525.3123655739296
INFO:root:current train perplexity3.3232433795928955
INFO:root:current mean train loss 1524.9992259466046
INFO:root:current train perplexity3.3218610286712646
INFO:root:current mean train loss 1524.4420305717244
INFO:root:current train perplexity3.323038101196289
INFO:root:current mean train loss 1524.1236993752
INFO:root:current train perplexity3.3248093128204346
INFO:root:current mean train loss 1524.3677299515114
INFO:root:current train perplexity3.325932025909424
INFO:root:current mean train loss 1524.8710271449474
INFO:root:current train perplexity3.327559232711792
INFO:root:current mean train loss 1525.6821759915595
INFO:root:current train perplexity3.327955484390259
INFO:root:current mean train loss 1525.9578922360242
INFO:root:current train perplexity3.329723358154297
INFO:root:current mean train loss 1526.7258764347675
INFO:root:current train perplexity3.3321640491485596
INFO:root:current mean train loss 1527.2180319758436
INFO:root:current train perplexity3.332949638366699

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.13s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.13s/it]
INFO:root:final mean train loss: 1526.6136581720996
INFO:root:final train perplexity: 3.333355188369751
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2197.585647041916
INFO:root:eval perplexity: 5.913739204406738
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/105
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/200 [5:47:55<5:14:50, 198.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1509.4387497674852
INFO:root:current train perplexity3.2968714237213135
INFO:root:current mean train loss 1516.4724340024202
INFO:root:current train perplexity3.2987937927246094
INFO:root:current mean train loss 1514.7806091308594
INFO:root:current train perplexity3.3082172870635986
INFO:root:current mean train loss 1516.2613248825073
INFO:root:current train perplexity3.3034181594848633
INFO:root:current mean train loss 1515.0043367748417
INFO:root:current train perplexity3.303560256958008
INFO:root:current mean train loss 1515.403117767752
INFO:root:current train perplexity3.3022892475128174
INFO:root:current mean train loss 1516.0940837302403
INFO:root:current train perplexity3.3079073429107666
INFO:root:current mean train loss 1518.7060121808734
INFO:root:current train perplexity3.310765504837036
INFO:root:current mean train loss 1518.9129066985117
INFO:root:current train perplexity3.3117048740386963
INFO:root:current mean train loss 1520.1080608833126
INFO:root:current train perplexity3.313892364501953
INFO:root:current mean train loss 1520.2581914359794
INFO:root:current train perplexity3.315885305404663
INFO:root:current mean train loss 1521.2057401296254
INFO:root:current train perplexity3.317772388458252
INFO:root:current mean train loss 1521.93777941172
INFO:root:current train perplexity3.3179497718811035
INFO:root:current mean train loss 1522.8840698065785
INFO:root:current train perplexity3.319690465927124
INFO:root:current mean train loss 1522.2958327136591
INFO:root:current train perplexity3.3213608264923096
INFO:root:current mean train loss 1522.3413690123896
INFO:root:current train perplexity3.322906494140625
INFO:root:current mean train loss 1522.712271937282
INFO:root:current train perplexity3.323734998703003
INFO:root:current mean train loss 1523.8828590974679
INFO:root:current train perplexity3.3250186443328857
INFO:root:current mean train loss 1524.0346644699193
INFO:root:current train perplexity3.325076103210449

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.30s/it]
INFO:root:final mean train loss: 1523.6570181565278
INFO:root:final train perplexity: 3.3255910873413086
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2196.7300341450577
INFO:root:eval perplexity: 5.9096479415893555
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/106
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/200 [5:51:14<5:11:33, 198.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1558.968994140625
INFO:root:current train perplexity3.480497360229492
INFO:root:current mean train loss 1509.3659559193225
INFO:root:current train perplexity3.2895522117614746
INFO:root:current mean train loss 1507.2794098355878
INFO:root:current train perplexity3.2883551120758057
INFO:root:current mean train loss 1507.1519657781355
INFO:root:current train perplexity3.2894105911254883
INFO:root:current mean train loss 1510.8882475208463
INFO:root:current train perplexity3.290306329727173
INFO:root:current mean train loss 1511.9716562967815
INFO:root:current train perplexity3.2900326251983643
INFO:root:current mean train loss 1513.5133123667586
INFO:root:current train perplexity3.293550491333008
INFO:root:current mean train loss 1513.8040217727464
INFO:root:current train perplexity3.2921009063720703
INFO:root:current mean train loss 1514.5400786858224
INFO:root:current train perplexity3.2962942123413086
INFO:root:current mean train loss 1514.5826417370456
INFO:root:current train perplexity3.2994070053100586
INFO:root:current mean train loss 1514.4682040371738
INFO:root:current train perplexity3.3007874488830566
INFO:root:current mean train loss 1514.2144650105017
INFO:root:current train perplexity3.301057815551758
INFO:root:current mean train loss 1514.7410028792738
INFO:root:current train perplexity3.303260564804077
INFO:root:current mean train loss 1515.3406402564433
INFO:root:current train perplexity3.306410312652588
INFO:root:current mean train loss 1516.366289180998
INFO:root:current train perplexity3.308279275894165
INFO:root:current mean train loss 1517.2742064372449
INFO:root:current train perplexity3.308943033218384
INFO:root:current mean train loss 1518.0424967854563
INFO:root:current train perplexity3.309579849243164
INFO:root:current mean train loss 1519.2075083360892
INFO:root:current train perplexity3.312833547592163
INFO:root:current mean train loss 1519.7808918547855
INFO:root:current train perplexity3.3133180141448975
INFO:root:current mean train loss 1520.1185838919073
INFO:root:current train perplexity3.315453290939331

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.89s/it]
INFO:root:final mean train loss: 1519.720009700373
INFO:root:final train perplexity: 3.315281629562378
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2200.7969728293992
INFO:root:eval perplexity: 5.929117679595947
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/107
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/200 [5:54:32<5:08:03, 198.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1512.7719319661458
INFO:root:current train perplexity3.3048617839813232
INFO:root:current mean train loss 1495.9929944054554
INFO:root:current train perplexity3.2694661617279053
INFO:root:current mean train loss 1502.425645740754
INFO:root:current train perplexity3.2783055305480957
INFO:root:current mean train loss 1504.9086069550904
INFO:root:current train perplexity3.284992218017578
INFO:root:current mean train loss 1506.0263344796651
INFO:root:current train perplexity3.2862722873687744
INFO:root:current mean train loss 1506.6469217543436
INFO:root:current train perplexity3.2865002155303955
INFO:root:current mean train loss 1508.1040493369487
INFO:root:current train perplexity3.28454327583313
INFO:root:current mean train loss 1507.4216470107394
INFO:root:current train perplexity3.2877349853515625
INFO:root:current mean train loss 1508.2645436778919
INFO:root:current train perplexity3.289076566696167
INFO:root:current mean train loss 1510.105913016791
INFO:root:current train perplexity3.290264844894409
INFO:root:current mean train loss 1511.5097768967184
INFO:root:current train perplexity3.292567491531372
INFO:root:current mean train loss 1512.1988444592744
INFO:root:current train perplexity3.2923333644866943
INFO:root:current mean train loss 1513.5732574212336
INFO:root:current train perplexity3.2937417030334473
INFO:root:current mean train loss 1513.8861930511428
INFO:root:current train perplexity3.2960288524627686
INFO:root:current mean train loss 1514.9705774390648
INFO:root:current train perplexity3.2991490364074707
INFO:root:current mean train loss 1515.1103448880363
INFO:root:current train perplexity3.301180839538574
INFO:root:current mean train loss 1515.7900756534157
INFO:root:current train perplexity3.30187726020813
INFO:root:current mean train loss 1515.9621059075778
INFO:root:current train perplexity3.303853750228882
INFO:root:current mean train loss 1516.6134360201156
INFO:root:current train perplexity3.3059263229370117
INFO:root:current mean train loss 1517.1610122696575
INFO:root:current train perplexity3.308229446411133

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.09s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.09s/it]
INFO:root:final mean train loss: 1517.2038738579686
INFO:root:final train perplexity: 3.3087098598480225
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.33s/it]
INFO:root:eval mean loss: 2201.319863610234
INFO:root:eval perplexity: 5.9316253662109375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/108
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/200 [5:57:51<5:04:44, 198.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1494.6318812779018
INFO:root:current train perplexity3.2899105548858643
INFO:root:current mean train loss 1506.5133056640625
INFO:root:current train perplexity3.269538164138794
INFO:root:current mean train loss 1503.1339116522606
INFO:root:current train perplexity3.274160861968994
INFO:root:current mean train loss 1503.8251577804338
INFO:root:current train perplexity3.2870876789093018
INFO:root:current mean train loss 1503.1629677958872
INFO:root:current train perplexity3.2841484546661377
INFO:root:current mean train loss 1503.7829074182243
INFO:root:current train perplexity3.282804012298584
INFO:root:current mean train loss 1506.7323271023006
INFO:root:current train perplexity3.2815167903900146
INFO:root:current mean train loss 1506.8645080981612
INFO:root:current train perplexity3.2847068309783936
INFO:root:current mean train loss 1508.4083113070733
INFO:root:current train perplexity3.288219928741455
INFO:root:current mean train loss 1508.7542543135862
INFO:root:current train perplexity3.289349317550659
INFO:root:current mean train loss 1510.1218054140247
INFO:root:current train perplexity3.289938449859619
INFO:root:current mean train loss 1511.0004661257572
INFO:root:current train perplexity3.2908034324645996
INFO:root:current mean train loss 1510.9259772543965
INFO:root:current train perplexity3.2910587787628174
INFO:root:current mean train loss 1511.200953520014
INFO:root:current train perplexity3.2931065559387207
INFO:root:current mean train loss 1512.0110591449804
INFO:root:current train perplexity3.2938759326934814
INFO:root:current mean train loss 1512.441104374491
INFO:root:current train perplexity3.2930381298065186
INFO:root:current mean train loss 1512.4607377078555
INFO:root:current train perplexity3.2963640689849854
INFO:root:current mean train loss 1513.2728126547865
INFO:root:current train perplexity3.2982430458068848
INFO:root:current mean train loss 1514.6878841057135
INFO:root:current train perplexity3.2995376586914062
INFO:root:current mean train loss 1514.3363329825784
INFO:root:current train perplexity3.299410104751587

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.18s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.18s/it]
INFO:root:final mean train loss: 1513.788761171619
INFO:root:final train perplexity: 3.2998101711273193
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2203.5836856923206
INFO:root:eval perplexity: 5.942494869232178
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/109
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 109/200 [6:01:10<5:01:27, 198.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1493.7348163311299
INFO:root:current train perplexity3.2410078048706055
INFO:root:current mean train loss 1494.8123329564144
INFO:root:current train perplexity3.259392261505127
INFO:root:current mean train loss 1498.2322382851253
INFO:root:current train perplexity3.2659687995910645
INFO:root:current mean train loss 1500.654872894287
INFO:root:current train perplexity3.264176368713379
INFO:root:current mean train loss 1498.8574116124516
INFO:root:current train perplexity3.2659780979156494
INFO:root:current mean train loss 1502.269457830899
INFO:root:current train perplexity3.2721128463745117
INFO:root:current mean train loss 1502.7550715347008
INFO:root:current train perplexity3.274378776550293
INFO:root:current mean train loss 1507.158597256275
INFO:root:current train perplexity3.2790613174438477
INFO:root:current mean train loss 1508.171113493297
INFO:root:current train perplexity3.2806057929992676
INFO:root:current mean train loss 1507.9400893780364
INFO:root:current train perplexity3.2809393405914307
INFO:root:current mean train loss 1508.7129819456616
INFO:root:current train perplexity3.282958507537842
INFO:root:current mean train loss 1508.4039398829143
INFO:root:current train perplexity3.28202223777771
INFO:root:current mean train loss 1509.3150505090293
INFO:root:current train perplexity3.284329414367676
INFO:root:current mean train loss 1509.840235964081
INFO:root:current train perplexity3.2846879959106445
INFO:root:current mean train loss 1510.310197478155
INFO:root:current train perplexity3.2886135578155518
INFO:root:current mean train loss 1510.1277679757973
INFO:root:current train perplexity3.289395332336426
INFO:root:current mean train loss 1510.8663730575156
INFO:root:current train perplexity3.2903082370758057
INFO:root:current mean train loss 1511.2629856475412
INFO:root:current train perplexity3.290724515914917
INFO:root:current mean train loss 1511.927058506218
INFO:root:current train perplexity3.29321026802063
INFO:root:current mean train loss 1512.314781814325
INFO:root:current train perplexity3.2942824363708496

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.27s/it]
INFO:root:final mean train loss: 1511.7157192951613
INFO:root:final train perplexity: 3.294419050216675
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it]
INFO:root:eval mean loss: 2205.8632583077074
INFO:root:eval perplexity: 5.953459739685059
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/110
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/200 [6:04:29<4:58:12, 198.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1494.8323443868885
INFO:root:current train perplexity3.278095245361328
INFO:root:current mean train loss 1491.5919536161705
INFO:root:current train perplexity3.257457733154297
INFO:root:current mean train loss 1492.6713467849675
INFO:root:current train perplexity3.2637150287628174
INFO:root:current mean train loss 1496.1301305920774
INFO:root:current train perplexity3.2660746574401855
INFO:root:current mean train loss 1497.790487000683
INFO:root:current train perplexity3.2691476345062256
INFO:root:current mean train loss 1499.9384711991295
INFO:root:current train perplexity3.269082546234131
INFO:root:current mean train loss 1502.3478631673906
INFO:root:current train perplexity3.273353338241577
INFO:root:current mean train loss 1502.7495028293645
INFO:root:current train perplexity3.2735118865966797
INFO:root:current mean train loss 1503.0626302176981
INFO:root:current train perplexity3.2734954357147217
INFO:root:current mean train loss 1504.2436658231343
INFO:root:current train perplexity3.2733187675476074
INFO:root:current mean train loss 1505.5140830772407
INFO:root:current train perplexity3.2751548290252686
INFO:root:current mean train loss 1506.9077318646746
INFO:root:current train perplexity3.276175022125244
INFO:root:current mean train loss 1506.9854879657335
INFO:root:current train perplexity3.2762105464935303
INFO:root:current mean train loss 1507.2830605461616
INFO:root:current train perplexity3.2773852348327637
INFO:root:current mean train loss 1506.7018488708038
INFO:root:current train perplexity3.2783942222595215
INFO:root:current mean train loss 1507.070846606243
INFO:root:current train perplexity3.2793281078338623
INFO:root:current mean train loss 1507.8798918818343
INFO:root:current train perplexity3.2820167541503906
INFO:root:current mean train loss 1508.0726067180212
INFO:root:current train perplexity3.282621145248413
INFO:root:current mean train loss 1508.1057054449238
INFO:root:current train perplexity3.283281087875366
INFO:root:current mean train loss 1508.569130730423
INFO:root:current train perplexity3.2847585678100586

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.84s/it]
INFO:root:final mean train loss: 1508.255671806547
INFO:root:final train perplexity: 3.2854418754577637
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2208.238051394199
INFO:root:eval perplexity: 5.964906215667725
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/111
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/200 [6:07:47<4:54:44, 198.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1486.5185262990553
INFO:root:current train perplexity3.250145673751831
INFO:root:current mean train loss 1490.0136731875841
INFO:root:current train perplexity3.259594202041626
INFO:root:current mean train loss 1495.2962898307746
INFO:root:current train perplexity3.2595696449279785
INFO:root:current mean train loss 1496.671794357695
INFO:root:current train perplexity3.260730266571045
INFO:root:current mean train loss 1499.0070953997074
INFO:root:current train perplexity3.2633304595947266
INFO:root:current mean train loss 1501.8106043688674
INFO:root:current train perplexity3.2664382457733154
INFO:root:current mean train loss 1502.6806492930598
INFO:root:current train perplexity3.268747091293335
INFO:root:current mean train loss 1502.2501500253459
INFO:root:current train perplexity3.270742416381836
INFO:root:current mean train loss 1503.4648522921664
INFO:root:current train perplexity3.270230531692505
INFO:root:current mean train loss 1502.3995808258985
INFO:root:current train perplexity3.270097494125366
INFO:root:current mean train loss 1501.323539495029
INFO:root:current train perplexity3.2709615230560303
INFO:root:current mean train loss 1502.5987737182822
INFO:root:current train perplexity3.2734782695770264
INFO:root:current mean train loss 1503.661086775096
INFO:root:current train perplexity3.273731231689453
INFO:root:current mean train loss 1504.550725939642
INFO:root:current train perplexity3.2725138664245605
INFO:root:current mean train loss 1504.442909702799
INFO:root:current train perplexity3.273866891860962
INFO:root:current mean train loss 1504.705273699189
INFO:root:current train perplexity3.275848627090454
INFO:root:current mean train loss 1504.8780475584779
INFO:root:current train perplexity3.274925947189331
INFO:root:current mean train loss 1505.254638193436
INFO:root:current train perplexity3.278313398361206
INFO:root:current mean train loss 1506.076679573585
INFO:root:current train perplexity3.278277635574341

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.15s/it]
INFO:root:final mean train loss: 1506.5638099628088
INFO:root:final train perplexity: 3.2810609340667725
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2209.7830174222904
INFO:root:eval perplexity: 5.972363471984863
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/112
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/200 [6:11:06<4:51:26, 198.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1538.0986735026042
INFO:root:current train perplexity3.514716386795044
INFO:root:current mean train loss 1487.7072943530036
INFO:root:current train perplexity3.2382242679595947
INFO:root:current mean train loss 1486.9602495766626
INFO:root:current train perplexity3.2391226291656494
INFO:root:current mean train loss 1488.9256954381963
INFO:root:current train perplexity3.24172306060791
INFO:root:current mean train loss 1489.2600730725612
INFO:root:current train perplexity3.245784044265747
INFO:root:current mean train loss 1491.6029661872515
INFO:root:current train perplexity3.2447338104248047
INFO:root:current mean train loss 1494.5450698574186
INFO:root:current train perplexity3.2468795776367188
INFO:root:current mean train loss 1495.5522683199235
INFO:root:current train perplexity3.2474300861358643
INFO:root:current mean train loss 1498.05072956394
INFO:root:current train perplexity3.2505273818969727
INFO:root:current mean train loss 1497.8049653012094
INFO:root:current train perplexity3.252678394317627
INFO:root:current mean train loss 1496.7508355061768
INFO:root:current train perplexity3.2524356842041016
INFO:root:current mean train loss 1497.8251587910104
INFO:root:current train perplexity3.256850004196167
INFO:root:current mean train loss 1498.6929567357647
INFO:root:current train perplexity3.2608954906463623
INFO:root:current mean train loss 1499.1576607880552
INFO:root:current train perplexity3.2609376907348633
INFO:root:current mean train loss 1499.5654987707703
INFO:root:current train perplexity3.262627601623535
INFO:root:current mean train loss 1500.7779886191158
INFO:root:current train perplexity3.2639527320861816
INFO:root:current mean train loss 1500.6271173069051
INFO:root:current train perplexity3.263218641281128
INFO:root:current mean train loss 1501.217726272341
INFO:root:current train perplexity3.265622615814209
INFO:root:current mean train loss 1501.857439545744
INFO:root:current train perplexity3.26782488822937
INFO:root:current mean train loss 1502.3027676669035
INFO:root:current train perplexity3.2697811126708984

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.06s/it]
INFO:root:final mean train loss: 1502.467629575513
INFO:root:final train perplexity: 3.2704784870147705
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2212.5997353411735
INFO:root:eval perplexity: 5.985983848571777
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/113
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/200 [6:14:24<4:48:06, 198.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1474.273260498047
INFO:root:current train perplexity3.197601795196533
INFO:root:current mean train loss 1496.0118357340496
INFO:root:current train perplexity3.243363618850708
INFO:root:current mean train loss 1495.3163685191762
INFO:root:current train perplexity3.2421467304229736
INFO:root:current mean train loss 1491.651490020752
INFO:root:current train perplexity3.2343835830688477
INFO:root:current mean train loss 1496.5199683779763
INFO:root:current train perplexity3.2402877807617188
INFO:root:current mean train loss 1497.1257197453426
INFO:root:current train perplexity3.243840456008911
INFO:root:current mean train loss 1498.1051360099545
INFO:root:current train perplexity3.2444865703582764
INFO:root:current mean train loss 1498.5773008558485
INFO:root:current train perplexity3.248070478439331
INFO:root:current mean train loss 1498.4432032143197
INFO:root:current train perplexity3.250783920288086
INFO:root:current mean train loss 1498.8175915261972
INFO:root:current train perplexity3.251350164413452
INFO:root:current mean train loss 1499.4463816923253
INFO:root:current train perplexity3.252933979034424
INFO:root:current mean train loss 1499.6151383536203
INFO:root:current train perplexity3.254507541656494
INFO:root:current mean train loss 1499.68707175333
INFO:root:current train perplexity3.2555043697357178
INFO:root:current mean train loss 1500.197005670721
INFO:root:current train perplexity3.25742244720459
INFO:root:current mean train loss 1500.2211955325704
INFO:root:current train perplexity3.2579638957977295
INFO:root:current mean train loss 1500.6807917544716
INFO:root:current train perplexity3.2601475715637207
INFO:root:current mean train loss 1500.7349680205923
INFO:root:current train perplexity3.2607338428497314
INFO:root:current mean train loss 1500.8093659866688
INFO:root:current train perplexity3.2619307041168213
INFO:root:current mean train loss 1500.8427077744034
INFO:root:current train perplexity3.26336407661438
INFO:root:current mean train loss 1500.8261437098186
INFO:root:current train perplexity3.2651467323303223

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.07s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.08s/it]
INFO:root:final mean train loss: 1500.443525213337
INFO:root:final train perplexity: 3.2652621269226074
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2213.178349055297
INFO:root:eval perplexity: 5.988786220550537
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/114
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/200 [6:17:43<4:44:48, 198.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1476.1192923880913
INFO:root:current train perplexity3.223503589630127
INFO:root:current mean train loss 1482.1462972599224
INFO:root:current train perplexity3.227882146835327
INFO:root:current mean train loss 1484.1158735701806
INFO:root:current train perplexity3.2359120845794678
INFO:root:current mean train loss 1483.1901583798915
INFO:root:current train perplexity3.232633352279663
INFO:root:current mean train loss 1485.9996485939287
INFO:root:current train perplexity3.2338368892669678
INFO:root:current mean train loss 1484.691174384602
INFO:root:current train perplexity3.233377456665039
INFO:root:current mean train loss 1487.0737352595786
INFO:root:current train perplexity3.2340853214263916
INFO:root:current mean train loss 1489.1678324353904
INFO:root:current train perplexity3.2403719425201416
INFO:root:current mean train loss 1490.069068170363
INFO:root:current train perplexity3.2406060695648193
INFO:root:current mean train loss 1490.8304619234425
INFO:root:current train perplexity3.240285873413086
INFO:root:current mean train loss 1490.6362002160304
INFO:root:current train perplexity3.242807626724243
INFO:root:current mean train loss 1492.1383836086948
INFO:root:current train perplexity3.24519419670105
INFO:root:current mean train loss 1493.2534608956587
INFO:root:current train perplexity3.246400833129883
INFO:root:current mean train loss 1493.9825390150231
INFO:root:current train perplexity3.247950792312622
INFO:root:current mean train loss 1493.8213239761385
INFO:root:current train perplexity3.2468271255493164
INFO:root:current mean train loss 1494.7802149041102
INFO:root:current train perplexity3.2484607696533203
INFO:root:current mean train loss 1495.6810351950262
INFO:root:current train perplexity3.2506792545318604
INFO:root:current mean train loss 1496.4335219976747
INFO:root:current train perplexity3.251251697540283
INFO:root:current mean train loss 1496.9891067695928
INFO:root:current train perplexity3.254974603652954
INFO:root:current mean train loss 1497.8620335741884
INFO:root:current train perplexity3.2574386596679688

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.98s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.99s/it]
INFO:root:final mean train loss: 1497.4551539649522
INFO:root:final train perplexity: 3.257575035095215
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it]
INFO:root:eval mean loss: 2219.0389798211713
INFO:root:eval perplexity: 6.017239093780518
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/115
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/200 [6:21:02<4:41:28, 198.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1485.9896760163483
INFO:root:current train perplexity3.1901681423187256
INFO:root:current mean train loss 1486.7062956574675
INFO:root:current train perplexity3.204972743988037
INFO:root:current mean train loss 1488.878277155358
INFO:root:current train perplexity3.213956117630005
INFO:root:current mean train loss 1493.1323007702154
INFO:root:current train perplexity3.228194236755371
INFO:root:current mean train loss 1492.26123046875
INFO:root:current train perplexity3.2342517375946045
INFO:root:current mean train loss 1491.705675255951
INFO:root:current train perplexity3.232004404067993
INFO:root:current mean train loss 1493.1983129285527
INFO:root:current train perplexity3.2355313301086426
INFO:root:current mean train loss 1491.2216591265853
INFO:root:current train perplexity3.2349112033843994
INFO:root:current mean train loss 1490.653653647358
INFO:root:current train perplexity3.2361457347869873
INFO:root:current mean train loss 1490.8174537002915
INFO:root:current train perplexity3.237567663192749
INFO:root:current mean train loss 1490.7835744318518
INFO:root:current train perplexity3.239875316619873
INFO:root:current mean train loss 1491.5663731408079
INFO:root:current train perplexity3.238342046737671
INFO:root:current mean train loss 1492.1115186286695
INFO:root:current train perplexity3.2404584884643555
INFO:root:current mean train loss 1492.4974428343103
INFO:root:current train perplexity3.2429778575897217
INFO:root:current mean train loss 1491.816503469685
INFO:root:current train perplexity3.2442309856414795
INFO:root:current mean train loss 1492.4695144712234
INFO:root:current train perplexity3.2454347610473633
INFO:root:current mean train loss 1492.8056177879619
INFO:root:current train perplexity3.247168779373169
INFO:root:current mean train loss 1493.3405845929171
INFO:root:current train perplexity3.248466730117798
INFO:root:current mean train loss 1493.840302797583
INFO:root:current train perplexity3.2492780685424805
INFO:root:current mean train loss 1494.9340227453101
INFO:root:current train perplexity3.250051259994507

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.31s/it]
INFO:root:final mean train loss: 1494.6923918615853
INFO:root:final train perplexity: 3.2504851818084717
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2220.000531568595
INFO:root:eval perplexity: 6.021919250488281
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/116
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/200 [6:24:21<4:38:15, 198.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1484.92500068772
INFO:root:current train perplexity3.2043378353118896
INFO:root:current mean train loss 1484.7291630973593
INFO:root:current train perplexity3.224773406982422
INFO:root:current mean train loss 1484.6028557246023
INFO:root:current train perplexity3.2285945415496826
INFO:root:current mean train loss 1483.7761181114176
INFO:root:current train perplexity3.228907346725464
INFO:root:current mean train loss 1482.3227362825105
INFO:root:current train perplexity3.2241671085357666
INFO:root:current mean train loss 1482.394663581915
INFO:root:current train perplexity3.220921039581299
INFO:root:current mean train loss 1484.2024478827077
INFO:root:current train perplexity3.220689058303833
INFO:root:current mean train loss 1484.8262907787673
INFO:root:current train perplexity3.2250187397003174
INFO:root:current mean train loss 1486.9058645885657
INFO:root:current train perplexity3.2266669273376465
INFO:root:current mean train loss 1487.6477779934507
INFO:root:current train perplexity3.2278618812561035
INFO:root:current mean train loss 1488.1015365130427
INFO:root:current train perplexity3.232288360595703
INFO:root:current mean train loss 1489.20652097316
INFO:root:current train perplexity3.234351634979248
INFO:root:current mean train loss 1489.1264090429227
INFO:root:current train perplexity3.2355408668518066
INFO:root:current mean train loss 1489.4811731250854
INFO:root:current train perplexity3.2357113361358643
INFO:root:current mean train loss 1489.81353639438
INFO:root:current train perplexity3.2360284328460693
INFO:root:current mean train loss 1490.5121427714482
INFO:root:current train perplexity3.238178014755249
INFO:root:current mean train loss 1490.5568019243717
INFO:root:current train perplexity3.2394192218780518
INFO:root:current mean train loss 1490.5064280944514
INFO:root:current train perplexity3.2398433685302734
INFO:root:current mean train loss 1490.7901632205999
INFO:root:current train perplexity3.239776372909546
INFO:root:current mean train loss 1491.714537490388
INFO:root:current train perplexity3.2415456771850586

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.05s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.05s/it]
INFO:root:final mean train loss: 1491.1989462925098
INFO:root:final train perplexity: 3.241542339324951
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2221.8855404684728
INFO:root:eval perplexity: 6.031105995178223
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/117
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 117/200 [6:27:39<4:34:53, 198.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1485.008149580522
INFO:root:current train perplexity3.2211127281188965
INFO:root:current mean train loss 1479.3007500831118
INFO:root:current train perplexity3.2117063999176025
INFO:root:current mean train loss 1479.7105734083389
INFO:root:current train perplexity3.209160566329956
INFO:root:current mean train loss 1476.635070486167
INFO:root:current train perplexity3.205612897872925
INFO:root:current mean train loss 1478.8005553698931
INFO:root:current train perplexity3.2116405963897705
INFO:root:current mean train loss 1478.0610669194436
INFO:root:current train perplexity3.213383436203003
INFO:root:current mean train loss 1479.9215419680572
INFO:root:current train perplexity3.2173383235931396
INFO:root:current mean train loss 1480.6278289949832
INFO:root:current train perplexity3.219252109527588
INFO:root:current mean train loss 1481.5031971974415
INFO:root:current train perplexity3.2192471027374268
INFO:root:current mean train loss 1481.6668457772569
INFO:root:current train perplexity3.2197299003601074
INFO:root:current mean train loss 1482.3111207625445
INFO:root:current train perplexity3.223332405090332
INFO:root:current mean train loss 1484.4596630571666
INFO:root:current train perplexity3.225154399871826
INFO:root:current mean train loss 1484.4415859435655
INFO:root:current train perplexity3.2254080772399902
INFO:root:current mean train loss 1485.3917599548868
INFO:root:current train perplexity3.2277162075042725
INFO:root:current mean train loss 1486.1556491646716
INFO:root:current train perplexity3.2293832302093506
INFO:root:current mean train loss 1486.2491435091802
INFO:root:current train perplexity3.2310547828674316
INFO:root:current mean train loss 1487.1959203204838
INFO:root:current train perplexity3.2322795391082764
INFO:root:current mean train loss 1487.8421044403008
INFO:root:current train perplexity3.232907295227051
INFO:root:current mean train loss 1488.2492685479633
INFO:root:current train perplexity3.233952283859253

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.84s/it]
INFO:root:final mean train loss: 1488.6605504084523
INFO:root:final train perplexity: 3.2350594997406006
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2223.0604611660574
INFO:root:eval perplexity: 6.036839485168457
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/118
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/200 [6:30:58<4:31:27, 198.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1470.28173828125
INFO:root:current train perplexity3.2001497745513916
INFO:root:current mean train loss 1462.71862327939
INFO:root:current train perplexity3.186981678009033
INFO:root:current mean train loss 1472.2894102515245
INFO:root:current train perplexity3.195936918258667
INFO:root:current mean train loss 1476.5201828253073
INFO:root:current train perplexity3.2023704051971436
INFO:root:current mean train loss 1478.154361074942
INFO:root:current train perplexity3.2007579803466797
INFO:root:current mean train loss 1478.3383392636138
INFO:root:current train perplexity3.201634168624878
INFO:root:current mean train loss 1481.9333703915936
INFO:root:current train perplexity3.208705425262451
INFO:root:current mean train loss 1482.3293178260749
INFO:root:current train perplexity3.2084193229675293
INFO:root:current mean train loss 1482.9327877826572
INFO:root:current train perplexity3.2110939025878906
INFO:root:current mean train loss 1483.4279160641834
INFO:root:current train perplexity3.2135980129241943
INFO:root:current mean train loss 1483.98408203125
INFO:root:current train perplexity3.2163643836975098
INFO:root:current mean train loss 1484.911628053415
INFO:root:current train perplexity3.219303846359253
INFO:root:current mean train loss 1485.4906131475298
INFO:root:current train perplexity3.2202823162078857
INFO:root:current mean train loss 1485.7902908734434
INFO:root:current train perplexity3.221485137939453
INFO:root:current mean train loss 1486.7908634063613
INFO:root:current train perplexity3.2235219478607178
INFO:root:current mean train loss 1486.9043091225863
INFO:root:current train perplexity3.223349094390869
INFO:root:current mean train loss 1486.252149274119
INFO:root:current train perplexity3.224691867828369
INFO:root:current mean train loss 1486.517089557368
INFO:root:current train perplexity3.2276344299316406
INFO:root:current mean train loss 1486.1659729511123
INFO:root:current train perplexity3.227935314178467
INFO:root:current mean train loss 1486.4878201382053
INFO:root:current train perplexity3.2281529903411865

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.35s/it]
INFO:root:final mean train loss: 1486.118622057016
INFO:root:final train perplexity: 3.2285807132720947
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it]
INFO:root:eval mean loss: 2223.205245214151
INFO:root:eval perplexity: 6.0375471115112305
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/119
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 119/200 [6:34:17<4:28:16, 198.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1498.449307528409
INFO:root:current train perplexity3.1887447834014893
INFO:root:current mean train loss 1479.9949480900998
INFO:root:current train perplexity3.1834053993225098
INFO:root:current mean train loss 1481.4072265625
INFO:root:current train perplexity3.191052198410034
INFO:root:current mean train loss 1477.0629151148826
INFO:root:current train perplexity3.192854642868042
INFO:root:current mean train loss 1476.4655996024326
INFO:root:current train perplexity3.194654941558838
INFO:root:current mean train loss 1477.206741040694
INFO:root:current train perplexity3.199446439743042
INFO:root:current mean train loss 1480.2520657750954
INFO:root:current train perplexity3.201817750930786
INFO:root:current mean train loss 1480.835688963491
INFO:root:current train perplexity3.1999549865722656
INFO:root:current mean train loss 1481.1765003065123
INFO:root:current train perplexity3.2048110961914062
INFO:root:current mean train loss 1480.4085676147727
INFO:root:current train perplexity3.2067885398864746
INFO:root:current mean train loss 1481.3996539031923
INFO:root:current train perplexity3.209925889968872
INFO:root:current mean train loss 1482.094256885549
INFO:root:current train perplexity3.210742950439453
INFO:root:current mean train loss 1483.7185998595094
INFO:root:current train perplexity3.2138290405273438
INFO:root:current mean train loss 1483.717428556549
INFO:root:current train perplexity3.217006206512451
INFO:root:current mean train loss 1484.0420427496758
INFO:root:current train perplexity3.2182302474975586
INFO:root:current mean train loss 1483.3643818077057
INFO:root:current train perplexity3.219101905822754
INFO:root:current mean train loss 1484.296061749817
INFO:root:current train perplexity3.218888521194458
INFO:root:current mean train loss 1484.5829036061357
INFO:root:current train perplexity3.2200679779052734
INFO:root:current mean train loss 1484.0680860232574
INFO:root:current train perplexity3.221237897872925
INFO:root:current mean train loss 1484.683192924952
INFO:root:current train perplexity3.2226390838623047

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.74s/it]
INFO:root:final mean train loss: 1484.2186969058778
INFO:root:final train perplexity: 3.2237462997436523
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2226.770734638187
INFO:root:eval perplexity: 6.054982662200928
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/120
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/200 [6:37:35<4:24:48, 198.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1454.4086256760818
INFO:root:current train perplexity3.2172038555145264
INFO:root:current mean train loss 1466.7716995349033
INFO:root:current train perplexity3.1876139640808105
INFO:root:current mean train loss 1467.2467327038114
INFO:root:current train perplexity3.1927568912506104
INFO:root:current mean train loss 1469.8058459076558
INFO:root:current train perplexity3.1951749324798584
INFO:root:current mean train loss 1468.3090027828694
INFO:root:current train perplexity3.1945302486419678
INFO:root:current mean train loss 1470.0113176618304
INFO:root:current train perplexity3.1951966285705566
INFO:root:current mean train loss 1470.7890162699287
INFO:root:current train perplexity3.198025941848755
INFO:root:current mean train loss 1473.9709854229216
INFO:root:current train perplexity3.2010202407836914
INFO:root:current mean train loss 1475.8024033738548
INFO:root:current train perplexity3.203507900238037
INFO:root:current mean train loss 1474.9753143668047
INFO:root:current train perplexity3.205024003982544
INFO:root:current mean train loss 1475.2250903719773
INFO:root:current train perplexity3.206172227859497
INFO:root:current mean train loss 1475.4128630171751
INFO:root:current train perplexity3.2081027030944824
INFO:root:current mean train loss 1475.6005010104545
INFO:root:current train perplexity3.210541248321533
INFO:root:current mean train loss 1476.590347529348
INFO:root:current train perplexity3.212639331817627
INFO:root:current mean train loss 1477.4151732634969
INFO:root:current train perplexity3.2136600017547607
INFO:root:current mean train loss 1477.7973762100746
INFO:root:current train perplexity3.214189052581787
INFO:root:current mean train loss 1478.8955796842824
INFO:root:current train perplexity3.2145209312438965
INFO:root:current mean train loss 1479.7301664171443
INFO:root:current train perplexity3.2147958278656006
INFO:root:current mean train loss 1480.1006959269007
INFO:root:current train perplexity3.2144761085510254
INFO:root:current mean train loss 1480.4332899277576
INFO:root:current train perplexity3.2148361206054688

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.14s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.14s/it]
INFO:root:final mean train loss: 1480.7566935836937
INFO:root:final train perplexity: 3.214956283569336
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it]
INFO:root:eval mean loss: 2229.493612086519
INFO:root:eval perplexity: 6.068331241607666
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/121
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/200 [6:40:54<4:21:33, 198.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1457.313973563058
INFO:root:current train perplexity3.1778507232666016
INFO:root:current mean train loss 1470.277106651893
INFO:root:current train perplexity3.19484543800354
INFO:root:current mean train loss 1466.64284324646
INFO:root:current train perplexity3.19608736038208
INFO:root:current mean train loss 1471.7771483003423
INFO:root:current train perplexity3.1972663402557373
INFO:root:current mean train loss 1472.2007641708642
INFO:root:current train perplexity3.197490930557251
INFO:root:current mean train loss 1471.4421683112494
INFO:root:current train perplexity3.198503017425537
INFO:root:current mean train loss 1473.6848663702244
INFO:root:current train perplexity3.1995818614959717
INFO:root:current mean train loss 1474.6291594328704
INFO:root:current train perplexity3.200380563735962
INFO:root:current mean train loss 1475.1789389637029
INFO:root:current train perplexity3.202359914779663
INFO:root:current mean train loss 1475.0072191310228
INFO:root:current train perplexity3.201667070388794
INFO:root:current mean train loss 1475.7150860410748
INFO:root:current train perplexity3.2003848552703857
INFO:root:current mean train loss 1475.8219213782709
INFO:root:current train perplexity3.1994011402130127
INFO:root:current mean train loss 1476.440266117169
INFO:root:current train perplexity3.20125675201416
INFO:root:current mean train loss 1476.7777383179791
INFO:root:current train perplexity3.201289653778076
INFO:root:current mean train loss 1476.4090072296478
INFO:root:current train perplexity3.203103542327881
INFO:root:current mean train loss 1476.84915114062
INFO:root:current train perplexity3.2052159309387207
INFO:root:current mean train loss 1477.074753102473
INFO:root:current train perplexity3.2047970294952393
INFO:root:current mean train loss 1477.5003475806166
INFO:root:current train perplexity3.206760883331299
INFO:root:current mean train loss 1478.035211628881
INFO:root:current train perplexity3.2075936794281006
INFO:root:current mean train loss 1478.8247894099884
INFO:root:current train perplexity3.209404706954956

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.84s/it]
INFO:root:final mean train loss: 1478.5952549798285
INFO:root:final train perplexity: 3.2094805240631104
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2232.652448938248
INFO:root:eval perplexity: 6.083852767944336
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/122
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/200 [6:44:12<4:18:09, 198.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1472.9267009578339
INFO:root:current train perplexity3.170290946960449
INFO:root:current mean train loss 1470.8911400943823
INFO:root:current train perplexity3.179800271987915
INFO:root:current mean train loss 1470.9129093156193
INFO:root:current train perplexity3.183779239654541
INFO:root:current mean train loss 1470.637934216865
INFO:root:current train perplexity3.1832401752471924
INFO:root:current mean train loss 1470.3472711994582
INFO:root:current train perplexity3.183131694793701
INFO:root:current mean train loss 1469.6713208902568
INFO:root:current train perplexity3.1857149600982666
INFO:root:current mean train loss 1469.389235611302
INFO:root:current train perplexity3.1882548332214355
INFO:root:current mean train loss 1468.078422200942
INFO:root:current train perplexity3.1875946521759033
INFO:root:current mean train loss 1470.5750739413302
INFO:root:current train perplexity3.190343141555786
INFO:root:current mean train loss 1471.2401271086926
INFO:root:current train perplexity3.192981243133545
INFO:root:current mean train loss 1471.2680097510631
INFO:root:current train perplexity3.1923933029174805
INFO:root:current mean train loss 1471.6809241253397
INFO:root:current train perplexity3.1930227279663086
INFO:root:current mean train loss 1472.0541892459987
INFO:root:current train perplexity3.194434881210327
INFO:root:current mean train loss 1472.9953964466781
INFO:root:current train perplexity3.1953022480010986
INFO:root:current mean train loss 1473.84737050768
INFO:root:current train perplexity3.1971805095672607
INFO:root:current mean train loss 1474.459628484087
INFO:root:current train perplexity3.2001941204071045
INFO:root:current mean train loss 1475.1540102687911
INFO:root:current train perplexity3.200437068939209
INFO:root:current mean train loss 1475.3366893374578
INFO:root:current train perplexity3.201758623123169
INFO:root:current mean train loss 1476.1440118157284
INFO:root:current train perplexity3.202406406402588
INFO:root:current mean train loss 1476.4733803812405
INFO:root:current train perplexity3.2029430866241455

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.16s/it]
INFO:root:final mean train loss: 1476.1438036296804
INFO:root:final train perplexity: 3.203281879425049
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2230.6933706297095
INFO:root:eval perplexity: 6.074222087860107
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/123
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 123/200 [6:47:31<4:14:55, 198.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1468.591684299045
INFO:root:current train perplexity3.19437313079834
INFO:root:current mean train loss 1463.8278545178864
INFO:root:current train perplexity3.182593584060669
INFO:root:current mean train loss 1465.6123707738416
INFO:root:current train perplexity3.1803290843963623
INFO:root:current mean train loss 1465.9932072566105
INFO:root:current train perplexity3.180884599685669
INFO:root:current mean train loss 1465.3631250498247
INFO:root:current train perplexity3.1800966262817383
INFO:root:current mean train loss 1465.9198728399763
INFO:root:current train perplexity3.1827898025512695
INFO:root:current mean train loss 1465.1956245754077
INFO:root:current train perplexity3.18377947807312
INFO:root:current mean train loss 1466.1048458823675
INFO:root:current train perplexity3.182523012161255
INFO:root:current mean train loss 1468.4685239641854
INFO:root:current train perplexity3.1842281818389893
INFO:root:current mean train loss 1469.1672898417771
INFO:root:current train perplexity3.1862332820892334
INFO:root:current mean train loss 1469.1854972629371
INFO:root:current train perplexity3.1859617233276367
INFO:root:current mean train loss 1470.0507272928703
INFO:root:current train perplexity3.1891982555389404
INFO:root:current mean train loss 1470.2265801008357
INFO:root:current train perplexity3.189544916152954
INFO:root:current mean train loss 1471.443365785887
INFO:root:current train perplexity3.192103385925293
INFO:root:current mean train loss 1471.6674726857434
INFO:root:current train perplexity3.1929147243499756
INFO:root:current mean train loss 1472.1529789762676
INFO:root:current train perplexity3.192769765853882
INFO:root:current mean train loss 1472.9954060390855
INFO:root:current train perplexity3.1929996013641357
INFO:root:current mean train loss 1473.4169795712944
INFO:root:current train perplexity3.1935789585113525
INFO:root:current mean train loss 1473.532617575025
INFO:root:current train perplexity3.195512533187866

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.34s/it]
INFO:root:final mean train loss: 1473.302185243269
INFO:root:final train perplexity: 3.196110963821411
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.33s/it]
INFO:root:eval mean loss: 2234.100837869847
INFO:root:eval perplexity: 6.09098482131958
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/124
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/200 [6:50:50<4:11:44, 198.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1445.5401436941963
INFO:root:current train perplexity3.091629981994629
INFO:root:current mean train loss 1466.4353597765771
INFO:root:current train perplexity3.1569228172302246
INFO:root:current mean train loss 1461.3024041364733
INFO:root:current train perplexity3.1580193042755127
INFO:root:current mean train loss 1466.9763302880701
INFO:root:current train perplexity3.1640400886535645
INFO:root:current mean train loss 1467.854798113099
INFO:root:current train perplexity3.1700901985168457
INFO:root:current mean train loss 1468.7566481370193
INFO:root:current train perplexity3.172698497772217
INFO:root:current mean train loss 1466.8402716999587
INFO:root:current train perplexity3.174903154373169
INFO:root:current mean train loss 1468.3590723277825
INFO:root:current train perplexity3.175328254699707
INFO:root:current mean train loss 1468.3973936248742
INFO:root:current train perplexity3.177520275115967
INFO:root:current mean train loss 1469.0950432454606
INFO:root:current train perplexity3.179661989212036
INFO:root:current mean train loss 1469.3784617298054
INFO:root:current train perplexity3.179382562637329
INFO:root:current mean train loss 1469.4148539170126
INFO:root:current train perplexity3.1809585094451904
INFO:root:current mean train loss 1470.6497630804358
INFO:root:current train perplexity3.1830365657806396
INFO:root:current mean train loss 1470.7680958264095
INFO:root:current train perplexity3.18218994140625
INFO:root:current mean train loss 1470.6955631475712
INFO:root:current train perplexity3.184633493423462
INFO:root:current mean train loss 1470.1509769189097
INFO:root:current train perplexity3.186387300491333
INFO:root:current mean train loss 1469.9129248229183
INFO:root:current train perplexity3.1872689723968506
INFO:root:current mean train loss 1470.5162013120378
INFO:root:current train perplexity3.1884524822235107
INFO:root:current mean train loss 1471.0898530724699
INFO:root:current train perplexity3.189434289932251
INFO:root:current mean train loss 1471.2267645849379
INFO:root:current train perplexity3.190639019012451

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.25s/it]
INFO:root:final mean train loss: 1471.2178385970692
INFO:root:final train perplexity: 3.190861463546753
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2233.6139050206393
INFO:root:eval perplexity: 6.088585376739502
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/125
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 125/200 [6:54:09<4:08:27, 198.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1459.9649759928386
INFO:root:current train perplexity3.2162163257598877
INFO:root:current mean train loss 1459.8313372212071
INFO:root:current train perplexity3.1661324501037598
INFO:root:current mean train loss 1459.9009748186384
INFO:root:current train perplexity3.1608948707580566
INFO:root:current mean train loss 1460.19346034674
INFO:root:current train perplexity3.15958571434021
INFO:root:current mean train loss 1462.324049463812
INFO:root:current train perplexity3.163151741027832
INFO:root:current mean train loss 1459.5067450836414
INFO:root:current train perplexity3.16125750541687
INFO:root:current mean train loss 1460.9824637388572
INFO:root:current train perplexity3.165024757385254
INFO:root:current mean train loss 1461.8584780192507
INFO:root:current train perplexity3.166849136352539
INFO:root:current mean train loss 1462.6354715291736
INFO:root:current train perplexity3.1684203147888184
INFO:root:current mean train loss 1463.3883135907063
INFO:root:current train perplexity3.170161724090576
INFO:root:current mean train loss 1463.6759742498398
INFO:root:current train perplexity3.1726653575897217
INFO:root:current mean train loss 1464.1357094978523
INFO:root:current train perplexity3.1736233234405518
INFO:root:current mean train loss 1465.4580359365425
INFO:root:current train perplexity3.1748476028442383
INFO:root:current mean train loss 1465.9207136724651
INFO:root:current train perplexity3.176306962966919
INFO:root:current mean train loss 1467.3615317183933
INFO:root:current train perplexity3.1773343086242676
INFO:root:current mean train loss 1468.2146682539012
INFO:root:current train perplexity3.1780340671539307
INFO:root:current mean train loss 1467.878265005027
INFO:root:current train perplexity3.1806249618530273
INFO:root:current mean train loss 1468.5392516240167
INFO:root:current train perplexity3.180874824523926
INFO:root:current mean train loss 1468.6121707715486
INFO:root:current train perplexity3.1818103790283203
INFO:root:current mean train loss 1468.768734937894
INFO:root:current train perplexity3.1830368041992188

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.23s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.23s/it]
INFO:root:final mean train loss: 1468.2153769996632
INFO:root:final train perplexity: 3.183314561843872
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.33s/it]
INFO:root:eval mean loss: 2237.98070596465
INFO:root:eval perplexity: 6.1101274490356445
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/126
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/200 [6:57:28<4:05:10, 198.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1452.3179217082698
INFO:root:current train perplexity3.13584566116333
INFO:root:current mean train loss 1455.3374248531693
INFO:root:current train perplexity3.142746686935425
INFO:root:current mean train loss 1455.063030322063
INFO:root:current train perplexity3.1559512615203857
INFO:root:current mean train loss 1457.4414807093108
INFO:root:current train perplexity3.1591522693634033
INFO:root:current mean train loss 1459.5900015279549
INFO:root:current train perplexity3.1586544513702393
INFO:root:current mean train loss 1461.46263791012
INFO:root:current train perplexity3.1569831371307373
INFO:root:current mean train loss 1460.6997028416292
INFO:root:current train perplexity3.160069465637207
INFO:root:current mean train loss 1462.2739574108048
INFO:root:current train perplexity3.164222002029419
INFO:root:current mean train loss 1463.1906144621823
INFO:root:current train perplexity3.1668896675109863
INFO:root:current mean train loss 1463.4274079893398
INFO:root:current train perplexity3.1681272983551025
INFO:root:current mean train loss 1463.7342144675717
INFO:root:current train perplexity3.168060779571533
INFO:root:current mean train loss 1464.509782207733
INFO:root:current train perplexity3.1684467792510986
INFO:root:current mean train loss 1463.333984375
INFO:root:current train perplexity3.1688499450683594
INFO:root:current mean train loss 1463.3593554286972
INFO:root:current train perplexity3.1701550483703613
INFO:root:current mean train loss 1464.0865374319592
INFO:root:current train perplexity3.1701512336730957
INFO:root:current mean train loss 1464.4257804578501
INFO:root:current train perplexity3.171736478805542
INFO:root:current mean train loss 1465.483344729538
INFO:root:current train perplexity3.1723732948303223
INFO:root:current mean train loss 1466.1474601662344
INFO:root:current train perplexity3.173673629760742
INFO:root:current mean train loss 1466.1833086319425
INFO:root:current train perplexity3.1752374172210693
INFO:root:current mean train loss 1466.2515273568313
INFO:root:current train perplexity3.1775479316711426

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.35s/it]
INFO:root:final mean train loss: 1465.9651647462908
INFO:root:final train perplexity: 3.1776700019836426
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it]
INFO:root:eval mean loss: 2240.472224242298
INFO:root:eval perplexity: 6.122450828552246
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/127
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 127/200 [7:00:47<4:01:54, 198.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1456.484720164332
INFO:root:current train perplexity3.123302936553955
INFO:root:current mean train loss 1457.242963959899
INFO:root:current train perplexity3.1458964347839355
INFO:root:current mean train loss 1458.0088718621305
INFO:root:current train perplexity3.1548571586608887
INFO:root:current mean train loss 1459.0488700653589
INFO:root:current train perplexity3.154132127761841
INFO:root:current mean train loss 1457.7474170668156
INFO:root:current train perplexity3.151604175567627
INFO:root:current mean train loss 1458.2095424419663
INFO:root:current train perplexity3.1557397842407227
INFO:root:current mean train loss 1459.6355653896157
INFO:root:current train perplexity3.158168315887451
INFO:root:current mean train loss 1460.0601629493733
INFO:root:current train perplexity3.160565137863159
INFO:root:current mean train loss 1459.1129819074156
INFO:root:current train perplexity3.160885810852051
INFO:root:current mean train loss 1459.082034817817
INFO:root:current train perplexity3.1614990234375
INFO:root:current mean train loss 1459.2965999379726
INFO:root:current train perplexity3.1625559329986572
INFO:root:current mean train loss 1459.6541172482189
INFO:root:current train perplexity3.1641221046447754
INFO:root:current mean train loss 1460.2620171333156
INFO:root:current train perplexity3.1651108264923096
INFO:root:current mean train loss 1460.4088870063972
INFO:root:current train perplexity3.1649255752563477
INFO:root:current mean train loss 1461.319270716119
INFO:root:current train perplexity3.1666529178619385
INFO:root:current mean train loss 1461.5360359710967
INFO:root:current train perplexity3.167541980743408
INFO:root:current mean train loss 1462.2955828805884
INFO:root:current train perplexity3.1689229011535645
INFO:root:current mean train loss 1463.142774909565
INFO:root:current train perplexity3.1689023971557617
INFO:root:current mean train loss 1463.5037261667242
INFO:root:current train perplexity3.1702890396118164
INFO:root:current mean train loss 1464.064872391012
INFO:root:current train perplexity3.172229528427124

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.21s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.21s/it]
INFO:root:final mean train loss: 1463.8748060602525
INFO:root:final train perplexity: 3.172435998916626
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2242.1429248566324
INFO:root:eval perplexity: 6.130728721618652
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/128
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/200 [7:04:05<3:58:35, 198.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1442.8819694010417
INFO:root:current train perplexity3.129830837249756
INFO:root:current mean train loss 1446.5778920200894
INFO:root:current train perplexity3.1378400325775146
INFO:root:current mean train loss 1450.1423610617899
INFO:root:current train perplexity3.1372101306915283
INFO:root:current mean train loss 1453.0646754557292
INFO:root:current train perplexity3.14495849609375
INFO:root:current mean train loss 1455.0243071546054
INFO:root:current train perplexity3.151716470718384
INFO:root:current mean train loss 1456.9322541610054
INFO:root:current train perplexity3.1558926105499268
INFO:root:current mean train loss 1456.7576197193287
INFO:root:current train perplexity3.1566789150238037
INFO:root:current mean train loss 1456.0332796748992
INFO:root:current train perplexity3.1573450565338135
INFO:root:current mean train loss 1456.901732282366
INFO:root:current train perplexity3.1592984199523926
INFO:root:current mean train loss 1457.0458270733172
INFO:root:current train perplexity3.1582441329956055
INFO:root:current mean train loss 1456.276737713481
INFO:root:current train perplexity3.1589651107788086
INFO:root:current mean train loss 1456.9966605718084
INFO:root:current train perplexity3.160550594329834
INFO:root:current mean train loss 1457.5847781671262
INFO:root:current train perplexity3.161668300628662
INFO:root:current mean train loss 1458.058854847301
INFO:root:current train perplexity3.162081003189087
INFO:root:current mean train loss 1458.8466816737289
INFO:root:current train perplexity3.161849021911621
INFO:root:current mean train loss 1459.532796921503
INFO:root:current train perplexity3.1633031368255615
INFO:root:current mean train loss 1459.7042076725747
INFO:root:current train perplexity3.1640961170196533
INFO:root:current mean train loss 1460.72029124945
INFO:root:current train perplexity3.1655378341674805
INFO:root:current mean train loss 1461.4995378255207
INFO:root:current train perplexity3.1660594940185547
INFO:root:current mean train loss 1462.027996316258
INFO:root:current train perplexity3.166687488555908

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.55s/it]
INFO:root:final mean train loss: 1461.5657761689695
INFO:root:final train perplexity: 3.1666641235351562
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2243.401761535212
INFO:root:eval perplexity: 6.1369733810424805
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/129
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 129/200 [7:07:24<3:55:23, 198.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1462.2618355129075
INFO:root:current train perplexity3.156919002532959
INFO:root:current mean train loss 1451.9015572865803
INFO:root:current train perplexity3.1465275287628174
INFO:root:current mean train loss 1449.0238797958582
INFO:root:current train perplexity3.137784242630005
INFO:root:current mean train loss 1450.9921516885563
INFO:root:current train perplexity3.1351983547210693
INFO:root:current mean train loss 1453.0705355512418
INFO:root:current train perplexity3.1406514644622803
INFO:root:current mean train loss 1453.8714725391285
INFO:root:current train perplexity3.143336772918701
INFO:root:current mean train loss 1453.6534041035382
INFO:root:current train perplexity3.1436002254486084
INFO:root:current mean train loss 1455.1416507297092
INFO:root:current train perplexity3.145430326461792
INFO:root:current mean train loss 1454.3745303303672
INFO:root:current train perplexity3.1470868587493896
INFO:root:current mean train loss 1454.6970616002236
INFO:root:current train perplexity3.1471104621887207
INFO:root:current mean train loss 1454.945056174701
INFO:root:current train perplexity3.1489899158477783
INFO:root:current mean train loss 1455.2436063625669
INFO:root:current train perplexity3.150416612625122
INFO:root:current mean train loss 1456.7606822861237
INFO:root:current train perplexity3.1522457599639893
INFO:root:current mean train loss 1457.5739491780598
INFO:root:current train perplexity3.1543829441070557
INFO:root:current mean train loss 1458.321198573381
INFO:root:current train perplexity3.1554670333862305
INFO:root:current mean train loss 1458.4245243551743
INFO:root:current train perplexity3.1561203002929688
INFO:root:current mean train loss 1459.3926723471207
INFO:root:current train perplexity3.1580233573913574
INFO:root:current mean train loss 1459.206567628043
INFO:root:current train perplexity3.1582272052764893
INFO:root:current mean train loss 1459.3558988994573
INFO:root:current train perplexity3.1597559452056885

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.08s/it]
INFO:root:final mean train loss: 1459.436217984706
INFO:root:final train perplexity: 3.1613500118255615
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2245.162231012439
INFO:root:eval perplexity: 6.145716667175293
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/130
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/200 [7:10:43<3:51:58, 198.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1439.69970703125
INFO:root:current train perplexity3.0280981063842773
INFO:root:current mean train loss 1449.1158525659403
INFO:root:current train perplexity3.1436617374420166
INFO:root:current mean train loss 1450.955656352796
INFO:root:current train perplexity3.142277240753174
INFO:root:current mean train loss 1450.6763690837379
INFO:root:current train perplexity3.1451797485351562
INFO:root:current mean train loss 1449.9081643251452
INFO:root:current train perplexity3.1418771743774414
INFO:root:current mean train loss 1449.9458122927922
INFO:root:current train perplexity3.1440391540527344
INFO:root:current mean train loss 1449.80730429463
INFO:root:current train perplexity3.146270275115967
INFO:root:current mean train loss 1449.512671552693
INFO:root:current train perplexity3.1476383209228516
INFO:root:current mean train loss 1449.691582188176
INFO:root:current train perplexity3.1470909118652344
INFO:root:current mean train loss 1450.8514684964591
INFO:root:current train perplexity3.1486129760742188
INFO:root:current mean train loss 1450.9878511608415
INFO:root:current train perplexity3.147953987121582
INFO:root:current mean train loss 1452.3468215708479
INFO:root:current train perplexity3.150566816329956
INFO:root:current mean train loss 1452.1746571732515
INFO:root:current train perplexity3.150641918182373
INFO:root:current mean train loss 1453.1478425354517
INFO:root:current train perplexity3.1509902477264404
INFO:root:current mean train loss 1453.3452288788037
INFO:root:current train perplexity3.1520941257476807
INFO:root:current mean train loss 1453.8254155891475
INFO:root:current train perplexity3.1529364585876465
INFO:root:current mean train loss 1454.6802147769868
INFO:root:current train perplexity3.1532042026519775
INFO:root:current mean train loss 1455.464585538075
INFO:root:current train perplexity3.15386700630188
INFO:root:current mean train loss 1456.499616176928
INFO:root:current train perplexity3.1545329093933105
INFO:root:current mean train loss 1457.1675142315185
INFO:root:current train perplexity3.1548736095428467

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.00s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.00s/it]
INFO:root:final mean train loss: 1457.0297304923884
INFO:root:final train perplexity: 3.15535569190979
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2248.6200094020114
INFO:root:eval perplexity: 6.162927627563477
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/131
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/200 [7:14:02<3:48:36, 198.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1438.9786611703726
INFO:root:current train perplexity3.1068313121795654
INFO:root:current mean train loss 1443.7635904947917
INFO:root:current train perplexity3.105280637741089
INFO:root:current mean train loss 1448.2919306122096
INFO:root:current train perplexity3.1240339279174805
INFO:root:current mean train loss 1447.8331770633627
INFO:root:current train perplexity3.130091905593872
INFO:root:current mean train loss 1449.023097365115
INFO:root:current train perplexity3.13169002532959
INFO:root:current mean train loss 1449.5953879700867
INFO:root:current train perplexity3.1348788738250732
INFO:root:current mean train loss 1450.0791454376124
INFO:root:current train perplexity3.136476755142212
INFO:root:current mean train loss 1451.0138586795692
INFO:root:current train perplexity3.1393537521362305
INFO:root:current mean train loss 1450.3529299535128
INFO:root:current train perplexity3.138592004776001
INFO:root:current mean train loss 1450.6652255954289
INFO:root:current train perplexity3.140066146850586
INFO:root:current mean train loss 1451.0695409347206
INFO:root:current train perplexity3.140707015991211
INFO:root:current mean train loss 1450.918128547194
INFO:root:current train perplexity3.143263578414917
INFO:root:current mean train loss 1451.0047907121418
INFO:root:current train perplexity3.1434266567230225
INFO:root:current mean train loss 1451.456920963247
INFO:root:current train perplexity3.1440675258636475
INFO:root:current mean train loss 1452.632450398021
INFO:root:current train perplexity3.1451239585876465
INFO:root:current mean train loss 1453.7171646058168
INFO:root:current train perplexity3.1463279724121094
INFO:root:current mean train loss 1453.5845305382986
INFO:root:current train perplexity3.146000862121582
INFO:root:current mean train loss 1453.7846063677985
INFO:root:current train perplexity3.146235466003418
INFO:root:current mean train loss 1454.1322018141814
INFO:root:current train perplexity3.1463468074798584
INFO:root:current mean train loss 1454.665220013041
INFO:root:current train perplexity3.1480305194854736

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.71s/it]
INFO:root:final mean train loss: 1454.2972682966347
INFO:root:final train perplexity: 3.1485633850097656
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.35s/it]
INFO:root:eval mean loss: 2249.0242725128824
INFO:root:eval perplexity: 6.164941310882568
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/132
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/200 [7:17:20<3:45:08, 198.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1467.9282851108285
INFO:root:current train perplexity3.1840038299560547
INFO:root:current mean train loss 1454.707440142865
INFO:root:current train perplexity3.1584277153015137
INFO:root:current mean train loss 1452.4531074178562
INFO:root:current train perplexity3.1322829723358154
INFO:root:current mean train loss 1450.5220868969798
INFO:root:current train perplexity3.1253645420074463
INFO:root:current mean train loss 1451.203336900836
INFO:root:current train perplexity3.122424364089966
INFO:root:current mean train loss 1450.48114766776
INFO:root:current train perplexity3.126376152038574
INFO:root:current mean train loss 1451.831048232771
INFO:root:current train perplexity3.127894878387451
INFO:root:current mean train loss 1450.104878606578
INFO:root:current train perplexity3.1287243366241455
INFO:root:current mean train loss 1450.826690854834
INFO:root:current train perplexity3.1307294368743896
INFO:root:current mean train loss 1451.4590624326866
INFO:root:current train perplexity3.1313788890838623
INFO:root:current mean train loss 1450.8926303238106
INFO:root:current train perplexity3.131645679473877
INFO:root:current mean train loss 1451.4945912064948
INFO:root:current train perplexity3.1328375339508057
INFO:root:current mean train loss 1451.768019954715
INFO:root:current train perplexity3.134150505065918
INFO:root:current mean train loss 1452.4294336337432
INFO:root:current train perplexity3.135565757751465
INFO:root:current mean train loss 1451.8662188048174
INFO:root:current train perplexity3.1373648643493652
INFO:root:current mean train loss 1451.942813341755
INFO:root:current train perplexity3.136820077896118
INFO:root:current mean train loss 1452.0380684776562
INFO:root:current train perplexity3.138176679611206
INFO:root:current mean train loss 1451.9379379263708
INFO:root:current train perplexity3.139590263366699
INFO:root:current mean train loss 1452.274405915648
INFO:root:current train perplexity3.1406819820404053
INFO:root:current mean train loss 1452.5769235843613
INFO:root:current train perplexity3.1421239376068115

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.52s/it]
INFO:root:final mean train loss: 1452.2058488977598
INFO:root:final train perplexity: 3.1433749198913574
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2249.0739884613254
INFO:root:eval perplexity: 6.165190696716309
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/133
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/200 [7:20:39<3:41:59, 198.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1449.3674641927084
INFO:root:current train perplexity3.120361089706421
INFO:root:current mean train loss 1441.9988533020019
INFO:root:current train perplexity3.125215768814087
INFO:root:current mean train loss 1442.4239501953125
INFO:root:current train perplexity3.1289470195770264
INFO:root:current mean train loss 1441.4770436604817
INFO:root:current train perplexity3.1323437690734863
INFO:root:current mean train loss 1441.1583761463994
INFO:root:current train perplexity3.1293275356292725
INFO:root:current mean train loss 1440.5016187395368
INFO:root:current train perplexity3.132185459136963
INFO:root:current mean train loss 1440.4121617172705
INFO:root:current train perplexity3.1311872005462646
INFO:root:current mean train loss 1443.0573889481393
INFO:root:current train perplexity3.131004810333252
INFO:root:current mean train loss 1444.2988887343295
INFO:root:current train perplexity3.1306581497192383
INFO:root:current mean train loss 1444.703850809733
INFO:root:current train perplexity3.130521059036255
INFO:root:current mean train loss 1446.7636291503907
INFO:root:current train perplexity3.129974842071533
INFO:root:current mean train loss 1446.9362124739023
INFO:root:current train perplexity3.1301023960113525
INFO:root:current mean train loss 1446.745941452753
INFO:root:current train perplexity3.1301016807556152
INFO:root:current mean train loss 1447.1246862972484
INFO:root:current train perplexity3.132608652114868
INFO:root:current mean train loss 1447.2789068352686
INFO:root:current train perplexity3.134035110473633
INFO:root:current mean train loss 1447.5501712896885
INFO:root:current train perplexity3.134216070175171
INFO:root:current mean train loss 1448.0872913038875
INFO:root:current train perplexity3.1346564292907715
INFO:root:current mean train loss 1448.6246481461958
INFO:root:current train perplexity3.1354031562805176
INFO:root:current mean train loss 1448.8823191653016
INFO:root:current train perplexity3.1352410316467285
INFO:root:current mean train loss 1449.49722526706
INFO:root:current train perplexity3.1361658573150635

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.22s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.22s/it]
INFO:root:final mean train loss: 1449.3863005529915
INFO:root:final train perplexity: 3.13639235496521
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2254.0554406998003
INFO:root:eval perplexity: 6.190077781677246
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/134
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 134/200 [7:23:58<3:38:41, 198.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1440.2664144937094
INFO:root:current train perplexity3.133352279663086
INFO:root:current mean train loss 1440.5805333024364
INFO:root:current train perplexity3.1263554096221924
INFO:root:current mean train loss 1441.0133933607851
INFO:root:current train perplexity3.123342514038086
INFO:root:current mean train loss 1442.3737611644149
INFO:root:current train perplexity3.1241300106048584
INFO:root:current mean train loss 1442.5551778285508
INFO:root:current train perplexity3.123415946960449
INFO:root:current mean train loss 1444.1885497623755
INFO:root:current train perplexity3.1222476959228516
INFO:root:current mean train loss 1445.5318836116369
INFO:root:current train perplexity3.1202309131622314
INFO:root:current mean train loss 1444.9931218013494
INFO:root:current train perplexity3.121769666671753
INFO:root:current mean train loss 1444.6463357192488
INFO:root:current train perplexity3.124749183654785
INFO:root:current mean train loss 1444.766083669516
INFO:root:current train perplexity3.126767873764038
INFO:root:current mean train loss 1444.6110271995778
INFO:root:current train perplexity3.1265029907226562
INFO:root:current mean train loss 1444.411102113424
INFO:root:current train perplexity3.126297950744629
INFO:root:current mean train loss 1445.377055503469
INFO:root:current train perplexity3.1268980503082275
INFO:root:current mean train loss 1446.0400918089313
INFO:root:current train perplexity3.128253698348999
INFO:root:current mean train loss 1447.0480882648528
INFO:root:current train perplexity3.129629373550415
INFO:root:current mean train loss 1447.7995609339084
INFO:root:current train perplexity3.1304075717926025
INFO:root:current mean train loss 1448.016971849727
INFO:root:current train perplexity3.132011890411377
INFO:root:current mean train loss 1448.4643970289858
INFO:root:current train perplexity3.13339900970459
INFO:root:current mean train loss 1448.8069159823272
INFO:root:current train perplexity3.1339001655578613
INFO:root:current mean train loss 1448.9391448310848
INFO:root:current train perplexity3.134218215942383

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.34s/it]
INFO:root:final mean train loss: 1448.5047802254219
INFO:root:final train perplexity: 3.1342124938964844
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it]
INFO:root:eval mean loss: 2254.2173418626717
INFO:root:eval perplexity: 6.190887928009033
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/135
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/200 [7:27:17<3:35:26, 198.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1441.6732359541224
INFO:root:current train perplexity3.112342119216919
INFO:root:current mean train loss 1443.2999588484618
INFO:root:current train perplexity3.098890781402588
INFO:root:current mean train loss 1442.1670278951424
INFO:root:current train perplexity3.1004931926727295
INFO:root:current mean train loss 1440.929948680897
INFO:root:current train perplexity3.1079907417297363
INFO:root:current mean train loss 1441.8322447494938
INFO:root:current train perplexity3.107473850250244
INFO:root:current mean train loss 1441.7052108816024
INFO:root:current train perplexity3.1091561317443848
INFO:root:current mean train loss 1441.4172716827832
INFO:root:current train perplexity3.1115500926971436
INFO:root:current mean train loss 1442.9093323522611
INFO:root:current train perplexity3.112692356109619
INFO:root:current mean train loss 1443.1257597306696
INFO:root:current train perplexity3.1143338680267334
INFO:root:current mean train loss 1443.2798326826191
INFO:root:current train perplexity3.117020845413208
INFO:root:current mean train loss 1443.3292366878643
INFO:root:current train perplexity3.1166629791259766
INFO:root:current mean train loss 1444.9591269948376
INFO:root:current train perplexity3.1174087524414062
INFO:root:current mean train loss 1445.7363241629032
INFO:root:current train perplexity3.1197471618652344
INFO:root:current mean train loss 1445.4529410188475
INFO:root:current train perplexity3.121934652328491
INFO:root:current mean train loss 1444.678547360013
INFO:root:current train perplexity3.1220812797546387
INFO:root:current mean train loss 1445.3427149295208
INFO:root:current train perplexity3.1238038539886475
INFO:root:current mean train loss 1445.4795098422974
INFO:root:current train perplexity3.1263587474823
INFO:root:current mean train loss 1445.8618143649405
INFO:root:current train perplexity3.12689208984375
INFO:root:current mean train loss 1446.6319013553284
INFO:root:current train perplexity3.128659725189209

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.00s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.00s/it]
INFO:root:final mean train loss: 1446.0406057075966
INFO:root:final train perplexity: 3.1281275749206543
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it]
INFO:root:eval mean loss: 2256.258322857796
INFO:root:eval perplexity: 6.20111608505249
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/136
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/200 [7:30:36<3:32:02, 198.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1462.1463734019887
INFO:root:current train perplexity3.103618860244751
INFO:root:current mean train loss 1436.3116125158363
INFO:root:current train perplexity3.1031198501586914
INFO:root:current mean train loss 1436.6113072978376
INFO:root:current train perplexity3.10884165763855
INFO:root:current mean train loss 1437.726826658586
INFO:root:current train perplexity3.1108765602111816
INFO:root:current mean train loss 1434.665546946282
INFO:root:current train perplexity3.1031088829040527
INFO:root:current mean train loss 1436.4935902336106
INFO:root:current train perplexity3.1072463989257812
INFO:root:current mean train loss 1436.4091243462944
INFO:root:current train perplexity3.1080286502838135
INFO:root:current mean train loss 1437.7016129419942
INFO:root:current train perplexity3.108999013900757
INFO:root:current mean train loss 1438.0268915931335
INFO:root:current train perplexity3.111854076385498
INFO:root:current mean train loss 1437.8562421746365
INFO:root:current train perplexity3.1121976375579834
INFO:root:current mean train loss 1437.5461203615696
INFO:root:current train perplexity3.111555814743042
INFO:root:current mean train loss 1438.3130284952324
INFO:root:current train perplexity3.113386631011963
INFO:root:current mean train loss 1439.7768812738698
INFO:root:current train perplexity3.1170756816864014
INFO:root:current mean train loss 1440.8362008217725
INFO:root:current train perplexity3.118724822998047
INFO:root:current mean train loss 1442.5729764185419
INFO:root:current train perplexity3.1184194087982178
INFO:root:current mean train loss 1442.8433910599613
INFO:root:current train perplexity3.119337558746338
INFO:root:current mean train loss 1443.028732517652
INFO:root:current train perplexity3.1203203201293945
INFO:root:current mean train loss 1443.678750961723
INFO:root:current train perplexity3.1214582920074463
INFO:root:current mean train loss 1443.2734022472262
INFO:root:current train perplexity3.1218907833099365
INFO:root:current mean train loss 1443.4286607934941
INFO:root:current train perplexity3.1222198009490967

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.79s/it]
INFO:root:final mean train loss: 1443.8598269617924
INFO:root:final train perplexity: 3.1227519512176514
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.38s/it]
INFO:root:eval mean loss: 2256.8968947390294
INFO:root:eval perplexity: 6.204320907592773
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/137
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 137/200 [7:33:55<3:28:57, 199.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1414.3326851981026
INFO:root:current train perplexity3.115739345550537
INFO:root:current mean train loss 1435.3756895065308
INFO:root:current train perplexity3.1225075721740723
INFO:root:current mean train loss 1436.9172336511444
INFO:root:current train perplexity3.1023080348968506
INFO:root:current mean train loss 1438.1892030297256
INFO:root:current train perplexity3.103271484375
INFO:root:current mean train loss 1439.4792517546182
INFO:root:current train perplexity3.1068227291107178
INFO:root:current mean train loss 1438.8699454105263
INFO:root:current train perplexity3.108595848083496
INFO:root:current mean train loss 1440.0069288508907
INFO:root:current train perplexity3.1090149879455566
INFO:root:current mean train loss 1439.616983099298
INFO:root:current train perplexity3.110276460647583
INFO:root:current mean train loss 1439.2157648796044
INFO:root:current train perplexity3.1103503704071045
INFO:root:current mean train loss 1439.6819310681574
INFO:root:current train perplexity3.112781047821045
INFO:root:current mean train loss 1440.019534218636
INFO:root:current train perplexity3.1130900382995605
INFO:root:current mean train loss 1440.8963157707917
INFO:root:current train perplexity3.113192081451416
INFO:root:current mean train loss 1441.7611624751883
INFO:root:current train perplexity3.1142942905426025
INFO:root:current mean train loss 1441.5658632761024
INFO:root:current train perplexity3.1161346435546875
INFO:root:current mean train loss 1441.707450033236
INFO:root:current train perplexity3.117623805999756
INFO:root:current mean train loss 1442.5835717485838
INFO:root:current train perplexity3.1167314052581787
INFO:root:current mean train loss 1442.507442015102
INFO:root:current train perplexity3.117565870285034
INFO:root:current mean train loss 1442.2715160935013
INFO:root:current train perplexity3.1186110973358154
INFO:root:current mean train loss 1442.0719888966767
INFO:root:current train perplexity3.1183252334594727
INFO:root:current mean train loss 1442.3319334291323
INFO:root:current train perplexity3.118938684463501

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.79s/it]
INFO:root:final mean train loss: 1442.098836601592
INFO:root:final train perplexity: 3.1184184551239014
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.38s/it]
INFO:root:eval mean loss: 2258.40122910087
INFO:root:eval perplexity: 6.21187162399292
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/138
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/200 [7:37:16<3:26:05, 199.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1433.3343315972222
INFO:root:current train perplexity3.0649335384368896
INFO:root:current mean train loss 1423.5590231007543
INFO:root:current train perplexity3.091357707977295
INFO:root:current mean train loss 1431.7689024633291
INFO:root:current train perplexity3.0961837768554688
INFO:root:current mean train loss 1434.5205453181613
INFO:root:current train perplexity3.0976316928863525
INFO:root:current mean train loss 1434.7146020782127
INFO:root:current train perplexity3.1014556884765625
INFO:root:current mean train loss 1434.8749039116256
INFO:root:current train perplexity3.100933313369751
INFO:root:current mean train loss 1434.7952118156493
INFO:root:current train perplexity3.1029715538024902
INFO:root:current mean train loss 1435.7545619887794
INFO:root:current train perplexity3.103933572769165
INFO:root:current mean train loss 1437.1660125913
INFO:root:current train perplexity3.1057729721069336
INFO:root:current mean train loss 1437.2272212921628
INFO:root:current train perplexity3.106706142425537
INFO:root:current mean train loss 1437.7715610047846
INFO:root:current train perplexity3.1063084602355957
INFO:root:current mean train loss 1438.2997256882848
INFO:root:current train perplexity3.1058056354522705
INFO:root:current mean train loss 1437.843851185994
INFO:root:current train perplexity3.107421398162842
INFO:root:current mean train loss 1438.6864383059362
INFO:root:current train perplexity3.108464002609253
INFO:root:current mean train loss 1439.0328621729022
INFO:root:current train perplexity3.108510732650757
INFO:root:current mean train loss 1438.5802190786812
INFO:root:current train perplexity3.1089768409729004
INFO:root:current mean train loss 1438.7624552532532
INFO:root:current train perplexity3.1094775199890137
INFO:root:current mean train loss 1439.3508791860672
INFO:root:current train perplexity3.111539602279663
INFO:root:current mean train loss 1439.9708589383258
INFO:root:current train perplexity3.1124682426452637
INFO:root:current mean train loss 1439.7767138169786
INFO:root:current train perplexity3.111874580383301

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.71s/it]
INFO:root:final mean train loss: 1439.442692543603
INFO:root:final train perplexity: 3.1118927001953125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.39s/it]
INFO:root:eval mean loss: 2261.0161041909078
INFO:root:eval perplexity: 6.22502326965332
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/139
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/200 [7:40:36<3:23:03, 199.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1429.6610087733116
INFO:root:current train perplexity3.0578911304473877
INFO:root:current mean train loss 1432.6776725863233
INFO:root:current train perplexity3.0858001708984375
INFO:root:current mean train loss 1431.1142279937976
INFO:root:current train perplexity3.0857994556427
INFO:root:current mean train loss 1429.8569416868095
INFO:root:current train perplexity3.0920605659484863
INFO:root:current mean train loss 1429.964868058374
INFO:root:current train perplexity3.09147310256958
INFO:root:current mean train loss 1430.2222670151245
INFO:root:current train perplexity3.091935157775879
INFO:root:current mean train loss 1430.4781921939907
INFO:root:current train perplexity3.090315341949463
INFO:root:current mean train loss 1432.7943120040293
INFO:root:current train perplexity3.095226287841797
INFO:root:current mean train loss 1433.865902787848
INFO:root:current train perplexity3.096547842025757
INFO:root:current mean train loss 1435.1604345246312
INFO:root:current train perplexity3.099247694015503
INFO:root:current mean train loss 1435.72885769774
INFO:root:current train perplexity3.099809408187866
INFO:root:current mean train loss 1436.5824283461973
INFO:root:current train perplexity3.100724458694458
INFO:root:current mean train loss 1436.1238414927632
INFO:root:current train perplexity3.0997958183288574
INFO:root:current mean train loss 1436.8509255295808
INFO:root:current train perplexity3.101097822189331
INFO:root:current mean train loss 1436.5594439839208
INFO:root:current train perplexity3.102140188217163
INFO:root:current mean train loss 1436.8771914043743
INFO:root:current train perplexity3.1037988662719727
INFO:root:current mean train loss 1437.6499316494387
INFO:root:current train perplexity3.1052017211914062
INFO:root:current mean train loss 1437.370977914834
INFO:root:current train perplexity3.1058273315429688
INFO:root:current mean train loss 1437.3831051540683
INFO:root:current train perplexity3.1063694953918457
INFO:root:current mean train loss 1437.7801801737903
INFO:root:current train perplexity3.107407569885254

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.40s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.40s/it]
INFO:root:final mean train loss: 1437.6553374582388
INFO:root:final train perplexity: 3.107508659362793
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.38s/it]
INFO:root:eval mean loss: 2261.2526812181404
INFO:root:eval perplexity: 6.226213455200195
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/140
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/200 [7:43:56<3:19:51, 199.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1430.5149126656447
INFO:root:current train perplexity3.0731656551361084
INFO:root:current mean train loss 1431.2839553236295
INFO:root:current train perplexity3.0719573497772217
INFO:root:current mean train loss 1426.7750983562948
INFO:root:current train perplexity3.0777738094329834
INFO:root:current mean train loss 1428.1446362497938
INFO:root:current train perplexity3.081434965133667
INFO:root:current mean train loss 1429.5803304206354
INFO:root:current train perplexity3.086671829223633
INFO:root:current mean train loss 1429.5884006216943
INFO:root:current train perplexity3.0895838737487793
INFO:root:current mean train loss 1430.185400714228
INFO:root:current train perplexity3.092705011367798
INFO:root:current mean train loss 1430.6677645682064
INFO:root:current train perplexity3.0940279960632324
INFO:root:current mean train loss 1431.7706605739832
INFO:root:current train perplexity3.093928098678589
INFO:root:current mean train loss 1431.877199510023
INFO:root:current train perplexity3.094486951828003
INFO:root:current mean train loss 1432.875477646765
INFO:root:current train perplexity3.096088171005249
INFO:root:current mean train loss 1433.7978667824627
INFO:root:current train perplexity3.0984997749328613
INFO:root:current mean train loss 1434.1527973858056
INFO:root:current train perplexity3.0976295471191406
INFO:root:current mean train loss 1434.5013510944016
INFO:root:current train perplexity3.0999560356140137
INFO:root:current mean train loss 1434.634586522513
INFO:root:current train perplexity3.099670886993408
INFO:root:current mean train loss 1434.2596419312065
INFO:root:current train perplexity3.099949359893799
INFO:root:current mean train loss 1434.9112594631756
INFO:root:current train perplexity3.1018354892730713
INFO:root:current mean train loss 1435.485897276494
INFO:root:current train perplexity3.1017649173736572
INFO:root:current mean train loss 1435.7813683023091
INFO:root:current train perplexity3.1035523414611816
INFO:root:current mean train loss 1436.0794568555773
INFO:root:current train perplexity3.10284423828125

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.93s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.93s/it]
INFO:root:final mean train loss: 1435.8575252315582
INFO:root:final train perplexity: 3.1031062602996826
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.38s/it]
INFO:root:eval mean loss: 2262.571546622202
INFO:root:eval perplexity: 6.232858657836914
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/141
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/200 [7:47:17<3:16:45, 200.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1434.3679033915203
INFO:root:current train perplexity3.1015803813934326
INFO:root:current mean train loss 1431.1942935865752
INFO:root:current train perplexity3.0895233154296875
INFO:root:current mean train loss 1431.2262420654297
INFO:root:current train perplexity3.085026502609253
INFO:root:current mean train loss 1431.0020801274463
INFO:root:current train perplexity3.083418607711792
INFO:root:current mean train loss 1430.1865689677577
INFO:root:current train perplexity3.0825018882751465
INFO:root:current mean train loss 1431.3206334466101
INFO:root:current train perplexity3.080815076828003
INFO:root:current mean train loss 1431.379880006286
INFO:root:current train perplexity3.082974910736084
INFO:root:current mean train loss 1431.9787451969319
INFO:root:current train perplexity3.0860722064971924
INFO:root:current mean train loss 1432.0143162863594
INFO:root:current train perplexity3.0875446796417236
INFO:root:current mean train loss 1430.9481902218247
INFO:root:current train perplexity3.0902018547058105
INFO:root:current mean train loss 1431.4689064861213
INFO:root:current train perplexity3.0905263423919678
INFO:root:current mean train loss 1431.6432665566538
INFO:root:current train perplexity3.0905277729034424
INFO:root:current mean train loss 1432.5578592559439
INFO:root:current train perplexity3.0925426483154297
INFO:root:current mean train loss 1432.5253186594791
INFO:root:current train perplexity3.0940513610839844
INFO:root:current mean train loss 1432.5298501896987
INFO:root:current train perplexity3.0955708026885986
INFO:root:current mean train loss 1432.88705949138
INFO:root:current train perplexity3.0964648723602295
INFO:root:current mean train loss 1433.4122926244195
INFO:root:current train perplexity3.0980279445648193
INFO:root:current mean train loss 1433.9318611807707
INFO:root:current train perplexity3.098444938659668
INFO:root:current mean train loss 1434.7600888280426
INFO:root:current train perplexity3.0989038944244385

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.82s/it]
INFO:root:final mean train loss: 1434.0330658189828
INFO:root:final train perplexity: 3.098644495010376
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.38s/it]
INFO:root:eval mean loss: 2266.503136168135
INFO:root:eval perplexity: 6.25270938873291
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/142
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 142/200 [7:50:37<3:13:32, 200.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1422.8087815504807
INFO:root:current train perplexity3.0274126529693604
INFO:root:current mean train loss 1433.040489534361
INFO:root:current train perplexity3.0519332885742188
INFO:root:current mean train loss 1427.9559045352846
INFO:root:current train perplexity3.074561834335327
INFO:root:current mean train loss 1425.255094193041
INFO:root:current train perplexity3.0705819129943848
INFO:root:current mean train loss 1426.052150920286
INFO:root:current train perplexity3.074483633041382
INFO:root:current mean train loss 1428.1936568172819
INFO:root:current train perplexity3.077566623687744
INFO:root:current mean train loss 1428.1825359400489
INFO:root:current train perplexity3.0808348655700684
INFO:root:current mean train loss 1430.15346977587
INFO:root:current train perplexity3.0842206478118896
INFO:root:current mean train loss 1430.6919030896852
INFO:root:current train perplexity3.0850377082824707
INFO:root:current mean train loss 1429.8908522941197
INFO:root:current train perplexity3.0817465782165527
INFO:root:current mean train loss 1429.7939307315446
INFO:root:current train perplexity3.0820400714874268
INFO:root:current mean train loss 1430.54263445502
INFO:root:current train perplexity3.0855438709259033
INFO:root:current mean train loss 1430.1250714508838
INFO:root:current train perplexity3.0853610038757324
INFO:root:current mean train loss 1430.870680867854
INFO:root:current train perplexity3.0873734951019287
INFO:root:current mean train loss 1431.1155181690385
INFO:root:current train perplexity3.088351249694824
INFO:root:current mean train loss 1430.9623002016378
INFO:root:current train perplexity3.090888500213623
INFO:root:current mean train loss 1431.262184403213
INFO:root:current train perplexity3.0922739505767822
INFO:root:current mean train loss 1431.2111857737705
INFO:root:current train perplexity3.0935189723968506
INFO:root:current mean train loss 1431.782352403324
INFO:root:current train perplexity3.0944864749908447
INFO:root:current mean train loss 1432.1069630105897
INFO:root:current train perplexity3.09474778175354

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.53s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.53s/it]
INFO:root:final mean train loss: 1432.544320911094
INFO:root:final train perplexity: 3.095008611679077
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.36s/it]
INFO:root:eval mean loss: 2267.2344672020445
INFO:root:eval perplexity: 6.25640869140625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/143
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 143/200 [7:53:58<3:10:11, 200.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1390.3801025390626
INFO:root:current train perplexity3.0341649055480957
INFO:root:current mean train loss 1417.4507502629206
INFO:root:current train perplexity3.070366144180298
INFO:root:current mean train loss 1419.1420144786005
INFO:root:current train perplexity3.0652425289154053
INFO:root:current mean train loss 1420.2831909179688
INFO:root:current train perplexity3.06828236579895
INFO:root:current mean train loss 1419.6371380473292
INFO:root:current train perplexity3.072376251220703
INFO:root:current mean train loss 1419.7174194335937
INFO:root:current train perplexity3.075056552886963
INFO:root:current mean train loss 1421.8862572079613
INFO:root:current train perplexity3.0771806240081787
INFO:root:current mean train loss 1423.9295532226563
INFO:root:current train perplexity3.0762486457824707
INFO:root:current mean train loss 1425.2667002482588
INFO:root:current train perplexity3.0763418674468994
INFO:root:current mean train loss 1425.586081359207
INFO:root:current train perplexity3.078329086303711
INFO:root:current mean train loss 1426.5088905112257
INFO:root:current train perplexity3.079965353012085
INFO:root:current mean train loss 1427.3307725214324
INFO:root:current train perplexity3.081608533859253
INFO:root:current mean train loss 1427.7166781789888
INFO:root:current train perplexity3.082048177719116
INFO:root:current mean train loss 1428.3045631351329
INFO:root:current train perplexity3.0845108032226562
INFO:root:current mean train loss 1428.7697711224323
INFO:root:current train perplexity3.086090087890625
INFO:root:current mean train loss 1428.8994103924122
INFO:root:current train perplexity3.086456298828125
INFO:root:current mean train loss 1429.5625435109519
INFO:root:current train perplexity3.0879249572753906
INFO:root:current mean train loss 1430.4720097712698
INFO:root:current train perplexity3.0885908603668213
INFO:root:current mean train loss 1430.7911040092426
INFO:root:current train perplexity3.0887272357940674
INFO:root:current mean train loss 1431.018933927704
INFO:root:current train perplexity3.0900626182556152

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.70s/it]
INFO:root:final mean train loss: 1431.155020740257
INFO:root:final train perplexity: 3.091618537902832
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.36s/it]
INFO:root:eval mean loss: 2267.4762897897276
INFO:root:eval perplexity: 6.257631301879883
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/144
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/200 [7:57:18<3:06:54, 200.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1420.182549659242
INFO:root:current train perplexity3.0698153972625732
INFO:root:current mean train loss 1425.3196207682292
INFO:root:current train perplexity3.0719265937805176
INFO:root:current mean train loss 1427.7028091986651
INFO:root:current train perplexity3.068681240081787
INFO:root:current mean train loss 1427.4458465136438
INFO:root:current train perplexity3.0703577995300293
INFO:root:current mean train loss 1427.7987882541597
INFO:root:current train perplexity3.0748188495635986
INFO:root:current mean train loss 1427.6703246847148
INFO:root:current train perplexity3.076399803161621
INFO:root:current mean train loss 1427.584762832665
INFO:root:current train perplexity3.0781161785125732
INFO:root:current mean train loss 1427.0788879803067
INFO:root:current train perplexity3.0787527561187744
INFO:root:current mean train loss 1428.0366842186577
INFO:root:current train perplexity3.0795786380767822
INFO:root:current mean train loss 1426.7508080874225
INFO:root:current train perplexity3.0798044204711914
INFO:root:current mean train loss 1426.231468747202
INFO:root:current train perplexity3.0810420513153076
INFO:root:current mean train loss 1425.7600458439476
INFO:root:current train perplexity3.081881046295166
INFO:root:current mean train loss 1426.5302559149773
INFO:root:current train perplexity3.0812947750091553
INFO:root:current mean train loss 1426.8791834683266
INFO:root:current train perplexity3.080634832382202
INFO:root:current mean train loss 1427.209781754816
INFO:root:current train perplexity3.0824766159057617
INFO:root:current mean train loss 1427.5277483101113
INFO:root:current train perplexity3.0824508666992188
INFO:root:current mean train loss 1428.1816676035025
INFO:root:current train perplexity3.084181308746338
INFO:root:current mean train loss 1428.2086899947901
INFO:root:current train perplexity3.0847108364105225
INFO:root:current mean train loss 1428.7954510005668
INFO:root:current train perplexity3.0854434967041016
INFO:root:current mean train loss 1429.3797059453445
INFO:root:current train perplexity3.0861551761627197

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.75s/it]
INFO:root:final mean train loss: 1428.8580053563198
INFO:root:final train perplexity: 3.0860230922698975
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.36s/it]
INFO:root:eval mean loss: 2269.6160174430684
INFO:root:eval perplexity: 6.268467903137207
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/145
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 145/200 [8:00:38<3:03:36, 200.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1423.1551036834717
INFO:root:current train perplexity3.0660433769226074
INFO:root:current mean train loss 1416.674693037824
INFO:root:current train perplexity3.0673000812530518
INFO:root:current mean train loss 1414.4140477035985
INFO:root:current train perplexity3.070103168487549
INFO:root:current mean train loss 1415.8236395867316
INFO:root:current train perplexity3.071863889694214
INFO:root:current mean train loss 1418.0805314162683
INFO:root:current train perplexity3.070122480392456
INFO:root:current mean train loss 1419.1001379134807
INFO:root:current train perplexity3.070934295654297
INFO:root:current mean train loss 1421.0473448971668
INFO:root:current train perplexity3.0709664821624756
INFO:root:current mean train loss 1422.1267172928256
INFO:root:current train perplexity3.0708701610565186
INFO:root:current mean train loss 1423.7700771755642
INFO:root:current train perplexity3.0732173919677734
INFO:root:current mean train loss 1423.6935414674372
INFO:root:current train perplexity3.071475028991699
INFO:root:current mean train loss 1426.128982658673
INFO:root:current train perplexity3.0742318630218506
INFO:root:current mean train loss 1426.2156546156841
INFO:root:current train perplexity3.075228214263916
INFO:root:current mean train loss 1426.4081444076344
INFO:root:current train perplexity3.0760576725006104
INFO:root:current mean train loss 1426.1809592149125
INFO:root:current train perplexity3.07773494720459
INFO:root:current mean train loss 1426.4176394770054
INFO:root:current train perplexity3.078392505645752
INFO:root:current mean train loss 1427.1427525669108
INFO:root:current train perplexity3.080003261566162
INFO:root:current mean train loss 1426.9730469630315
INFO:root:current train perplexity3.0817770957946777
INFO:root:current mean train loss 1427.3902827325592
INFO:root:current train perplexity3.082073211669922
INFO:root:current mean train loss 1427.183221710598
INFO:root:current train perplexity3.082266092300415
INFO:root:current mean train loss 1427.7512837893607
INFO:root:current train perplexity3.0820207595825195

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.75s/it]
INFO:root:final mean train loss: 1427.3817925696053
INFO:root:final train perplexity: 3.082432508468628
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.37s/it]
INFO:root:eval mean loss: 2272.6473012071974
INFO:root:eval perplexity: 6.283854961395264
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/146
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/200 [8:03:59<3:00:18, 200.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1423.153160566165
INFO:root:current train perplexity3.0736427307128906
INFO:root:current mean train loss 1420.7971710710895
INFO:root:current train perplexity3.0653021335601807
INFO:root:current mean train loss 1421.7041719375557
INFO:root:current train perplexity3.0596067905426025
INFO:root:current mean train loss 1421.8750076894685
INFO:root:current train perplexity3.0609962940216064
INFO:root:current mean train loss 1423.419620886662
INFO:root:current train perplexity3.0628509521484375
INFO:root:current mean train loss 1422.1168815888554
INFO:root:current train perplexity3.061532974243164
INFO:root:current mean train loss 1422.3203888611647
INFO:root:current train perplexity3.063610792160034
INFO:root:current mean train loss 1421.8031767040452
INFO:root:current train perplexity3.0628161430358887
INFO:root:current mean train loss 1421.8626995801114
INFO:root:current train perplexity3.0633368492126465
INFO:root:current mean train loss 1423.111694211503
INFO:root:current train perplexity3.0674548149108887
INFO:root:current mean train loss 1423.8249779347466
INFO:root:current train perplexity3.0677711963653564
INFO:root:current mean train loss 1424.3854038842546
INFO:root:current train perplexity3.069610118865967
INFO:root:current mean train loss 1424.0200419251012
INFO:root:current train perplexity3.0709426403045654
INFO:root:current mean train loss 1424.039501192948
INFO:root:current train perplexity3.0724077224731445
INFO:root:current mean train loss 1423.9646695875622
INFO:root:current train perplexity3.0741539001464844
INFO:root:current mean train loss 1424.3772351761395
INFO:root:current train perplexity3.0743601322174072
INFO:root:current mean train loss 1425.0395661035795
INFO:root:current train perplexity3.0754404067993164
INFO:root:current mean train loss 1424.6617393943447
INFO:root:current train perplexity3.075998067855835
INFO:root:current mean train loss 1425.4841483814293
INFO:root:current train perplexity3.0769901275634766
INFO:root:current mean train loss 1425.4307301813517
INFO:root:current train perplexity3.076805591583252

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.56s/it]
INFO:root:final mean train loss: 1425.0666461738745
INFO:root:final train perplexity: 3.0768096446990967
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.42s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.42s/it]
INFO:root:eval mean loss: 2273.353368015154
INFO:root:eval perplexity: 6.287445545196533
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/147
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 147/200 [8:07:19<2:56:57, 200.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1419.1781977439414
INFO:root:current train perplexity3.0492942333221436
INFO:root:current mean train loss 1417.3704038677793
INFO:root:current train perplexity3.0612823963165283
INFO:root:current mean train loss 1415.3782332247536
INFO:root:current train perplexity3.061020612716675
INFO:root:current mean train loss 1418.501037290947
INFO:root:current train perplexity3.0621302127838135
INFO:root:current mean train loss 1419.9992790988172
INFO:root:current train perplexity3.0665762424468994
INFO:root:current mean train loss 1422.398953951322
INFO:root:current train perplexity3.0688180923461914
INFO:root:current mean train loss 1422.3793838632141
INFO:root:current train perplexity3.0665791034698486
INFO:root:current mean train loss 1422.9849954476033
INFO:root:current train perplexity3.0645925998687744
INFO:root:current mean train loss 1422.7174847099457
INFO:root:current train perplexity3.064166784286499
INFO:root:current mean train loss 1422.8986443345675
INFO:root:current train perplexity3.062910795211792
INFO:root:current mean train loss 1423.0976357937727
INFO:root:current train perplexity3.064438581466675
INFO:root:current mean train loss 1423.3842398463585
INFO:root:current train perplexity3.065394639968872
INFO:root:current mean train loss 1422.8540418063546
INFO:root:current train perplexity3.0658645629882812
INFO:root:current mean train loss 1422.6904814669674
INFO:root:current train perplexity3.0657100677490234
INFO:root:current mean train loss 1422.9023992439138
INFO:root:current train perplexity3.0675265789031982
INFO:root:current mean train loss 1423.2002335836055
INFO:root:current train perplexity3.069312334060669
INFO:root:current mean train loss 1423.3049849834824
INFO:root:current train perplexity3.07066011428833
INFO:root:current mean train loss 1423.2673543520577
INFO:root:current train perplexity3.0700788497924805
INFO:root:current mean train loss 1423.074423401072
INFO:root:current train perplexity3.0713164806365967

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.50s/it]
INFO:root:final mean train loss: 1423.1724139745945
INFO:root:final train perplexity: 3.0722169876098633
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.36s/it]
INFO:root:eval mean loss: 2276.028673537234
INFO:root:eval perplexity: 6.3010640144348145
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/148
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/200 [8:10:39<2:53:34, 200.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1414.0871256510416
INFO:root:current train perplexity2.9854817390441895
INFO:root:current mean train loss 1408.9025634765626
INFO:root:current train perplexity3.0322463512420654
INFO:root:current mean train loss 1410.6225608648256
INFO:root:current train perplexity3.0512125492095947
INFO:root:current mean train loss 1412.880908203125
INFO:root:current train perplexity3.0514330863952637
INFO:root:current mean train loss 1414.7085346267884
INFO:root:current train perplexity3.0509488582611084
INFO:root:current mean train loss 1415.2903360607554
INFO:root:current train perplexity3.0517868995666504
INFO:root:current mean train loss 1416.603411021659
INFO:root:current train perplexity3.054755449295044
INFO:root:current mean train loss 1417.2184522508742
INFO:root:current train perplexity3.0532569885253906
INFO:root:current mean train loss 1417.7477326375574
INFO:root:current train perplexity3.055622100830078
INFO:root:current mean train loss 1417.9122406506146
INFO:root:current train perplexity3.0578465461730957
INFO:root:current mean train loss 1418.4719002559268
INFO:root:current train perplexity3.0603909492492676
INFO:root:current mean train loss 1418.1675067439742
INFO:root:current train perplexity3.06111741065979
INFO:root:current mean train loss 1419.4774711451903
INFO:root:current train perplexity3.0629403591156006
INFO:root:current mean train loss 1419.6786569109433
INFO:root:current train perplexity3.0625088214874268
INFO:root:current mean train loss 1420.331564190868
INFO:root:current train perplexity3.0637905597686768
INFO:root:current mean train loss 1421.0138622724578
INFO:root:current train perplexity3.064065456390381
INFO:root:current mean train loss 1421.6831898975668
INFO:root:current train perplexity3.065444231033325
INFO:root:current mean train loss 1422.0597418515397
INFO:root:current train perplexity3.0661773681640625
INFO:root:current mean train loss 1422.2482006903194
INFO:root:current train perplexity3.0677666664123535
INFO:root:current mean train loss 1422.1914580741065
INFO:root:current train perplexity3.068274736404419

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.89s/it]
INFO:root:final mean train loss: 1421.604576122382
INFO:root:final train perplexity: 3.06842041015625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.37s/it]
INFO:root:eval mean loss: 2277.399962513159
INFO:root:eval perplexity: 6.308054447174072
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/149
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 149/200 [8:14:00<2:50:18, 200.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1395.595790863037
INFO:root:current train perplexity3.0343074798583984
INFO:root:current mean train loss 1411.3912224047112
INFO:root:current train perplexity3.043975353240967
INFO:root:current mean train loss 1414.111883755388
INFO:root:current train perplexity3.040897846221924
INFO:root:current mean train loss 1415.8771454225105
INFO:root:current train perplexity3.0525689125061035
INFO:root:current mean train loss 1414.3320083618164
INFO:root:current train perplexity3.0561742782592773
INFO:root:current mean train loss 1412.4285273731202
INFO:root:current train perplexity3.052473783493042
INFO:root:current mean train loss 1414.1827780808076
INFO:root:current train perplexity3.0542385578155518
INFO:root:current mean train loss 1413.25333158566
INFO:root:current train perplexity3.053095817565918
INFO:root:current mean train loss 1414.4954533210168
INFO:root:current train perplexity3.0554440021514893
INFO:root:current mean train loss 1414.88909807328
INFO:root:current train perplexity3.056309700012207
INFO:root:current mean train loss 1416.3542331429414
INFO:root:current train perplexity3.0588033199310303
INFO:root:current mean train loss 1417.26728761491
INFO:root:current train perplexity3.06119704246521
INFO:root:current mean train loss 1418.1394341208718
INFO:root:current train perplexity3.06322979927063
INFO:root:current mean train loss 1418.8638472456832
INFO:root:current train perplexity3.065152406692505
INFO:root:current mean train loss 1418.9456370263126
INFO:root:current train perplexity3.0652332305908203
INFO:root:current mean train loss 1419.746605855678
INFO:root:current train perplexity3.0641605854034424
INFO:root:current mean train loss 1420.2148715748506
INFO:root:current train perplexity3.0640599727630615
INFO:root:current mean train loss 1419.8887460898031
INFO:root:current train perplexity3.064145565032959
INFO:root:current mean train loss 1420.1493600237318
INFO:root:current train perplexity3.0638327598571777
INFO:root:current mean train loss 1420.3340806033054
INFO:root:current train perplexity3.0642058849334717

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.71s/it]
INFO:root:final mean train loss: 1420.0483648672407
INFO:root:final train perplexity: 3.064656972885132
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.36s/it]
INFO:root:eval mean loss: 2276.0911990767677
INFO:root:eval perplexity: 6.301382541656494
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/150
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/200 [8:17:20<2:46:58, 200.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1426.7800118582588
INFO:root:current train perplexity3.0366592407226562
INFO:root:current mean train loss 1421.8837710386954
INFO:root:current train perplexity3.0508320331573486
INFO:root:current mean train loss 1418.9695118364082
INFO:root:current train perplexity3.0542993545532227
INFO:root:current mean train loss 1418.0642212263833
INFO:root:current train perplexity3.0551631450653076
INFO:root:current mean train loss 1418.9908634856974
INFO:root:current train perplexity3.0614941120147705
INFO:root:current mean train loss 1418.5043153745446
INFO:root:current train perplexity3.0621886253356934
INFO:root:current mean train loss 1418.5617393646476
INFO:root:current train perplexity3.0581343173980713
INFO:root:current mean train loss 1417.7853979459592
INFO:root:current train perplexity3.0559444427490234
INFO:root:current mean train loss 1418.2424710366977
INFO:root:current train perplexity3.0561769008636475
INFO:root:current mean train loss 1417.099092666417
INFO:root:current train perplexity3.053886890411377
INFO:root:current mean train loss 1416.5370914542868
INFO:root:current train perplexity3.05594801902771
INFO:root:current mean train loss 1417.5386594236156
INFO:root:current train perplexity3.0565035343170166
INFO:root:current mean train loss 1417.2301396781488
INFO:root:current train perplexity3.0561506748199463
INFO:root:current mean train loss 1416.811929101852
INFO:root:current train perplexity3.055664539337158
INFO:root:current mean train loss 1416.7207866113213
INFO:root:current train perplexity3.0563743114471436
INFO:root:current mean train loss 1417.565856184938
INFO:root:current train perplexity3.059241533279419
INFO:root:current mean train loss 1418.0424877233836
INFO:root:current train perplexity3.059569835662842
INFO:root:current mean train loss 1418.1136078177485
INFO:root:current train perplexity3.0600647926330566
INFO:root:current mean train loss 1418.3082305099463
INFO:root:current train perplexity3.0602004528045654
INFO:root:current mean train loss 1418.3223213050965
INFO:root:current train perplexity3.0598907470703125

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.70s/it]
INFO:root:final mean train loss: 1418.1274491626086
INFO:root:final train perplexity: 3.0600173473358154
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.37s/it]
INFO:root:eval mean loss: 2278.1593268644724
INFO:root:eval perplexity: 6.311930179595947
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/151
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 151/200 [8:20:41<2:43:38, 200.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1410.5435384114583
INFO:root:current train perplexity3.030876398086548
INFO:root:current mean train loss 1420.709022613893
INFO:root:current train perplexity3.038144111633301
INFO:root:current mean train loss 1415.9386899560914
INFO:root:current train perplexity3.038017749786377
INFO:root:current mean train loss 1414.6420921784281
INFO:root:current train perplexity3.0335395336151123
INFO:root:current mean train loss 1414.4310934042214
INFO:root:current train perplexity3.0376200675964355
INFO:root:current mean train loss 1411.8875357152717
INFO:root:current train perplexity3.0414481163024902
INFO:root:current mean train loss 1413.4098127668685
INFO:root:current train perplexity3.042414665222168
INFO:root:current mean train loss 1414.1143118357845
INFO:root:current train perplexity3.046712875366211
INFO:root:current mean train loss 1413.88027087205
INFO:root:current train perplexity3.048070192337036
INFO:root:current mean train loss 1413.3007705088235
INFO:root:current train perplexity3.049701690673828
INFO:root:current mean train loss 1415.2175597571968
INFO:root:current train perplexity3.052324056625366
INFO:root:current mean train loss 1414.7378029144445
INFO:root:current train perplexity3.0525667667388916
INFO:root:current mean train loss 1415.1264900099045
INFO:root:current train perplexity3.0534169673919678
INFO:root:current mean train loss 1415.078446976088
INFO:root:current train perplexity3.0545527935028076
INFO:root:current mean train loss 1415.3941782786121
INFO:root:current train perplexity3.0547709465026855
INFO:root:current mean train loss 1415.4661809110094
INFO:root:current train perplexity3.053950548171997
INFO:root:current mean train loss 1415.87578898747
INFO:root:current train perplexity3.053340435028076
INFO:root:current mean train loss 1416.0814357597721
INFO:root:current train perplexity3.0551397800445557
INFO:root:current mean train loss 1416.2955573471413
INFO:root:current train perplexity3.055455446243286
INFO:root:current mean train loss 1416.44063868479
INFO:root:current train perplexity3.0552163124084473

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.83s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.83s/it]
INFO:root:final mean train loss: 1416.045468575174
INFO:root:final train perplexity: 3.054997444152832
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.36s/it]
INFO:root:eval mean loss: 2280.3770682693375
INFO:root:eval perplexity: 6.323261737823486
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/152
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 152/200 [8:24:01<2:40:19, 200.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1413.6027140789722
INFO:root:current train perplexity3.0213980674743652
INFO:root:current mean train loss 1409.658071715975
INFO:root:current train perplexity3.032578468322754
INFO:root:current mean train loss 1407.3577358933303
INFO:root:current train perplexity3.041958808898926
INFO:root:current mean train loss 1409.5559085218465
INFO:root:current train perplexity3.0460855960845947
INFO:root:current mean train loss 1409.6920380839156
INFO:root:current train perplexity3.048999309539795
INFO:root:current mean train loss 1409.5231428980624
INFO:root:current train perplexity3.049818754196167
INFO:root:current mean train loss 1410.806209536283
INFO:root:current train perplexity3.049666404724121
INFO:root:current mean train loss 1411.3912512534423
INFO:root:current train perplexity3.0505478382110596
INFO:root:current mean train loss 1412.5750851312553
INFO:root:current train perplexity3.049511194229126
INFO:root:current mean train loss 1412.8589361429456
INFO:root:current train perplexity3.0491580963134766
INFO:root:current mean train loss 1413.5363594823048
INFO:root:current train perplexity3.048948049545288
INFO:root:current mean train loss 1413.2957277660742
INFO:root:current train perplexity3.049581289291382
INFO:root:current mean train loss 1413.6385176458678
INFO:root:current train perplexity3.050509452819824
INFO:root:current mean train loss 1413.2967287451136
INFO:root:current train perplexity3.0502684116363525
INFO:root:current mean train loss 1414.18944966785
INFO:root:current train perplexity3.0500049591064453
INFO:root:current mean train loss 1414.5386500210982
INFO:root:current train perplexity3.0507190227508545
INFO:root:current mean train loss 1414.902151324249
INFO:root:current train perplexity3.0508198738098145
INFO:root:current mean train loss 1414.7086445909501
INFO:root:current train perplexity3.0522303581237793
INFO:root:current mean train loss 1415.0173125264496
INFO:root:current train perplexity3.0526235103607178
INFO:root:current mean train loss 1415.014061847481
INFO:root:current train perplexity3.0525131225585938

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.32s/it]
INFO:root:final mean train loss: 1415.014061847481
INFO:root:final train perplexity: 3.0525131225585938
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.35s/it]
INFO:root:eval mean loss: 2281.843619705092
INFO:root:eval perplexity: 6.3307647705078125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/153
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 153/200 [8:27:21<2:36:53, 200.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1404.9312548828125
INFO:root:current train perplexity3.034421682357788
INFO:root:current mean train loss 1404.814903564453
INFO:root:current train perplexity3.0309948921203613
INFO:root:current mean train loss 1408.1986564127603
INFO:root:current train perplexity3.03912353515625
INFO:root:current mean train loss 1408.289612426758
INFO:root:current train perplexity3.0371410846710205
INFO:root:current mean train loss 1408.2847741699218
INFO:root:current train perplexity3.038116216659546
INFO:root:current mean train loss 1408.5227724202473
INFO:root:current train perplexity3.038691997528076
INFO:root:current mean train loss 1408.5096102469308
INFO:root:current train perplexity3.0371992588043213
INFO:root:current mean train loss 1409.9355438232421
INFO:root:current train perplexity3.0381526947021484
INFO:root:current mean train loss 1410.8581978352865
INFO:root:current train perplexity3.0398316383361816
INFO:root:current mean train loss 1410.3926927490234
INFO:root:current train perplexity3.0397210121154785
INFO:root:current mean train loss 1410.2493142977628
INFO:root:current train perplexity3.041934013366699
INFO:root:current mean train loss 1411.3147158813476
INFO:root:current train perplexity3.0425875186920166
INFO:root:current mean train loss 1412.0690098219652
INFO:root:current train perplexity3.0433311462402344
INFO:root:current mean train loss 1411.9863804408483
INFO:root:current train perplexity3.046053647994995
INFO:root:current mean train loss 1412.350118326823
INFO:root:current train perplexity3.0464274883270264
INFO:root:current mean train loss 1413.3019008636475
INFO:root:current train perplexity3.047532081604004
INFO:root:current mean train loss 1413.4582489372701
INFO:root:current train perplexity3.0476770401000977
INFO:root:current mean train loss 1413.5472530788845
INFO:root:current train perplexity3.048170804977417
INFO:root:current mean train loss 1413.681904296875
INFO:root:current train perplexity3.0491933822631836

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.45s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.45s/it]
INFO:root:final mean train loss: 1413.602535091983
INFO:root:final train perplexity: 3.049116373062134
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.36s/it]
INFO:root:eval mean loss: 2283.5369409872287
INFO:root:eval perplexity: 6.339441299438477
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/154
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/200 [8:30:41<2:33:30, 200.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1403.2276467715992
INFO:root:current train perplexity3.008478879928589
INFO:root:current mean train loss 1406.2623353615786
INFO:root:current train perplexity3.0174639225006104
INFO:root:current mean train loss 1403.7723214285713
INFO:root:current train perplexity3.0183651447296143
INFO:root:current mean train loss 1403.5012291748817
INFO:root:current train perplexity3.0139458179473877
INFO:root:current mean train loss 1404.1383077132045
INFO:root:current train perplexity3.0181071758270264
INFO:root:current mean train loss 1406.007252204349
INFO:root:current train perplexity3.0206520557403564
INFO:root:current mean train loss 1408.2106992947224
INFO:root:current train perplexity3.0261800289154053
INFO:root:current mean train loss 1408.8647716314722
INFO:root:current train perplexity3.030658006668091
INFO:root:current mean train loss 1409.2311795070093
INFO:root:current train perplexity3.031371593475342
INFO:root:current mean train loss 1409.9383024425777
INFO:root:current train perplexity3.032038927078247
INFO:root:current mean train loss 1409.8803409662687
INFO:root:current train perplexity3.0336735248565674
INFO:root:current mean train loss 1411.0664213312025
INFO:root:current train perplexity3.03639817237854
INFO:root:current mean train loss 1411.5238926808365
INFO:root:current train perplexity3.0388307571411133
INFO:root:current mean train loss 1411.6714734748718
INFO:root:current train perplexity3.0391736030578613
INFO:root:current mean train loss 1411.6815611974573
INFO:root:current train perplexity3.0407912731170654
INFO:root:current mean train loss 1411.5197317768416
INFO:root:current train perplexity3.0421648025512695
INFO:root:current mean train loss 1412.0427832665382
INFO:root:current train perplexity3.0425517559051514
INFO:root:current mean train loss 1411.9375400976448
INFO:root:current train perplexity3.0442724227905273
INFO:root:current mean train loss 1412.1915646659586
INFO:root:current train perplexity3.045358419418335
INFO:root:current mean train loss 1412.0321476657252
INFO:root:current train perplexity3.044631004333496

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.71s/it]
INFO:root:final mean train loss: 1411.8852183254933
INFO:root:final train perplexity: 3.044990062713623
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it]
INFO:root:eval mean loss: 2283.6044103744184
INFO:root:eval perplexity: 6.339787006378174
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/155
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 155/200 [8:34:01<2:30:11, 200.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1397.2621747185203
INFO:root:current train perplexity3.0165648460388184
INFO:root:current mean train loss 1402.5349449043842
INFO:root:current train perplexity3.0288822650909424
INFO:root:current mean train loss 1402.8227090428018
INFO:root:current train perplexity3.0359561443328857
INFO:root:current mean train loss 1404.119009783168
INFO:root:current train perplexity3.0351130962371826
INFO:root:current mean train loss 1406.3296520039783
INFO:root:current train perplexity3.034978151321411
INFO:root:current mean train loss 1406.915225368314
INFO:root:current train perplexity3.0341758728027344
INFO:root:current mean train loss 1407.3615395338377
INFO:root:current train perplexity3.033270835876465
INFO:root:current mean train loss 1406.5336341961852
INFO:root:current train perplexity3.0326364040374756
INFO:root:current mean train loss 1406.7974032395177
INFO:root:current train perplexity3.0307326316833496
INFO:root:current mean train loss 1407.823125345038
INFO:root:current train perplexity3.0314953327178955
INFO:root:current mean train loss 1409.341885653409
INFO:root:current train perplexity3.034564733505249
INFO:root:current mean train loss 1409.4814998889096
INFO:root:current train perplexity3.0364532470703125
INFO:root:current mean train loss 1408.2636942314755
INFO:root:current train perplexity3.0371055603027344
INFO:root:current mean train loss 1408.4476763998373
INFO:root:current train perplexity3.037964105606079
INFO:root:current mean train loss 1409.0838402571205
INFO:root:current train perplexity3.039172410964966
INFO:root:current mean train loss 1409.237101926642
INFO:root:current train perplexity3.039557933807373
INFO:root:current mean train loss 1409.6329696823364
INFO:root:current train perplexity3.04101824760437
INFO:root:current mean train loss 1410.4033861347273
INFO:root:current train perplexity3.0417733192443848
INFO:root:current mean train loss 1410.6089498838135
INFO:root:current train perplexity3.041754961013794
INFO:root:current mean train loss 1410.6739388971812
INFO:root:current train perplexity3.0413601398468018

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.38s/it]
INFO:root:final mean train loss: 1410.3746050105092
INFO:root:final train perplexity: 3.0413644313812256
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2285.9737899455617
INFO:root:eval perplexity: 6.35194730758667
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/156
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 156/200 [8:37:20<2:26:34, 199.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1396.4451114430146
INFO:root:current train perplexity3.044325351715088
INFO:root:current mean train loss 1397.6529856296565
INFO:root:current train perplexity3.0191633701324463
INFO:root:current mean train loss 1400.1477945639317
INFO:root:current train perplexity3.023930072784424
INFO:root:current mean train loss 1400.547669326478
INFO:root:current train perplexity3.0263562202453613
INFO:root:current mean train loss 1403.0853222764517
INFO:root:current train perplexity3.026707172393799
INFO:root:current mean train loss 1404.8360422899416
INFO:root:current train perplexity3.0260422229766846
INFO:root:current mean train loss 1405.2224784886232
INFO:root:current train perplexity3.027120590209961
INFO:root:current mean train loss 1406.093353555936
INFO:root:current train perplexity3.0281896591186523
INFO:root:current mean train loss 1406.8809362606492
INFO:root:current train perplexity3.029189109802246
INFO:root:current mean train loss 1407.3587749172334
INFO:root:current train perplexity3.028197765350342
INFO:root:current mean train loss 1408.1124160723045
INFO:root:current train perplexity3.0281991958618164
INFO:root:current mean train loss 1407.3987838784888
INFO:root:current train perplexity3.0290117263793945
INFO:root:current mean train loss 1407.8534545410546
INFO:root:current train perplexity3.02954363822937
INFO:root:current mean train loss 1408.5493978165769
INFO:root:current train perplexity3.030022382736206
INFO:root:current mean train loss 1408.1750195514408
INFO:root:current train perplexity3.029600143432617
INFO:root:current mean train loss 1408.8818932342037
INFO:root:current train perplexity3.03261661529541
INFO:root:current mean train loss 1409.0910687414823
INFO:root:current train perplexity3.0328431129455566
INFO:root:current mean train loss 1409.2624324883548
INFO:root:current train perplexity3.034196376800537
INFO:root:current mean train loss 1408.9174079915629
INFO:root:current train perplexity3.034956455230713
INFO:root:current mean train loss 1408.948312388879
INFO:root:current train perplexity3.0366592407226562

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.07s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.07s/it]
INFO:root:final mean train loss: 1408.4138664057082
INFO:root:final train perplexity: 3.036665201187134
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2284.9559507978724
INFO:root:eval perplexity: 6.346720218658447
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/157
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 157/200 [8:40:39<2:22:59, 199.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1389.7549941119025
INFO:root:current train perplexity3.008951187133789
INFO:root:current mean train loss 1389.691863287063
INFO:root:current train perplexity3.0175280570983887
INFO:root:current mean train loss 1395.6929398721722
INFO:root:current train perplexity3.0162315368652344
INFO:root:current mean train loss 1397.5304956850798
INFO:root:current train perplexity3.0167176723480225
INFO:root:current mean train loss 1399.4368698250535
INFO:root:current train perplexity3.0140228271484375
INFO:root:current mean train loss 1399.7935535269723
INFO:root:current train perplexity3.0162789821624756
INFO:root:current mean train loss 1401.2881237646777
INFO:root:current train perplexity3.01708984375
INFO:root:current mean train loss 1402.4082091649373
INFO:root:current train perplexity3.0190718173980713
INFO:root:current mean train loss 1402.5311671665736
INFO:root:current train perplexity3.020390272140503
INFO:root:current mean train loss 1401.8997471076398
INFO:root:current train perplexity3.0197503566741943
INFO:root:current mean train loss 1401.6838774377486
INFO:root:current train perplexity3.0207910537719727
INFO:root:current mean train loss 1403.3303719089456
INFO:root:current train perplexity3.024571180343628
INFO:root:current mean train loss 1404.4363749892177
INFO:root:current train perplexity3.0276987552642822
INFO:root:current mean train loss 1404.4137166341145
INFO:root:current train perplexity3.0271689891815186
INFO:root:current mean train loss 1405.367306826225
INFO:root:current train perplexity3.0279853343963623
INFO:root:current mean train loss 1405.3062123668437
INFO:root:current train perplexity3.029154062271118
INFO:root:current mean train loss 1405.5639717230122
INFO:root:current train perplexity3.0287625789642334
INFO:root:current mean train loss 1405.6550108620484
INFO:root:current train perplexity3.0292768478393555
INFO:root:current mean train loss 1406.0612344027077
INFO:root:current train perplexity3.030287027359009
INFO:root:current mean train loss 1406.7674511545072
INFO:root:current train perplexity3.0318870544433594

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.90s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.90s/it]
INFO:root:final mean train loss: 1406.4741050885652
INFO:root:final train perplexity: 3.0320234298706055
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.33s/it]
INFO:root:eval mean loss: 2285.765121135306
INFO:root:eval perplexity: 6.350875377655029
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/158
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/200 [8:43:58<2:19:27, 199.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1404.1146225873163
INFO:root:current train perplexity3.029149055480957
INFO:root:current mean train loss 1403.7483655774915
INFO:root:current train perplexity3.007207155227661
INFO:root:current mean train loss 1404.2607987253289
INFO:root:current train perplexity3.014535903930664
INFO:root:current mean train loss 1404.6568901557428
INFO:root:current train perplexity3.011645793914795
INFO:root:current mean train loss 1404.7298113321522
INFO:root:current train perplexity3.014444351196289
INFO:root:current mean train loss 1404.4087145683093
INFO:root:current train perplexity3.016371965408325
INFO:root:current mean train loss 1405.6983491104015
INFO:root:current train perplexity3.0207560062408447
INFO:root:current mean train loss 1405.9199685260749
INFO:root:current train perplexity3.021383762359619
INFO:root:current mean train loss 1405.674438062765
INFO:root:current train perplexity3.0245707035064697
INFO:root:current mean train loss 1406.052026615046
INFO:root:current train perplexity3.0260324478149414
INFO:root:current mean train loss 1404.8086587791618
INFO:root:current train perplexity3.024790048599243
INFO:root:current mean train loss 1405.3872566834784
INFO:root:current train perplexity3.025001287460327
INFO:root:current mean train loss 1405.1164430135882
INFO:root:current train perplexity3.0252628326416016
INFO:root:current mean train loss 1405.5077437528205
INFO:root:current train perplexity3.0260913372039795
INFO:root:current mean train loss 1406.3130122020991
INFO:root:current train perplexity3.02785587310791
INFO:root:current mean train loss 1405.8280700876135
INFO:root:current train perplexity3.0278446674346924
INFO:root:current mean train loss 1405.270062563752
INFO:root:current train perplexity3.0281102657318115
INFO:root:current mean train loss 1405.5536083574054
INFO:root:current train perplexity3.0283384323120117
INFO:root:current mean train loss 1405.504988628357
INFO:root:current train perplexity3.028723955154419

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.03s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.03s/it]
INFO:root:final mean train loss: 1405.1129017732267
INFO:root:final train perplexity: 3.0287699699401855
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.34s/it]
INFO:root:eval mean loss: 2288.15651232131
INFO:root:eval perplexity: 6.363169193267822
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/159
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 159/200 [8:47:16<2:16:00, 199.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1367.4031982421875
INFO:root:current train perplexity3.159862518310547
INFO:root:current mean train loss 1394.3731581744025
INFO:root:current train perplexity3.0044326782226562
INFO:root:current mean train loss 1398.93098555461
INFO:root:current train perplexity3.0040066242218018
INFO:root:current mean train loss 1398.865524595147
INFO:root:current train perplexity3.0160038471221924
INFO:root:current mean train loss 1402.0392571444536
INFO:root:current train perplexity3.015360116958618
INFO:root:current mean train loss 1400.062255373039
INFO:root:current train perplexity3.0139989852905273
INFO:root:current mean train loss 1400.9227694387848
INFO:root:current train perplexity3.015831708908081
INFO:root:current mean train loss 1400.6569817263176
INFO:root:current train perplexity3.0188350677490234
INFO:root:current mean train loss 1401.8826073244622
INFO:root:current train perplexity3.0198707580566406
INFO:root:current mean train loss 1402.5702832680847
INFO:root:current train perplexity3.0208563804626465
INFO:root:current mean train loss 1402.6049771794303
INFO:root:current train perplexity3.0222256183624268
INFO:root:current mean train loss 1402.5401824009614
INFO:root:current train perplexity3.022414445877075
INFO:root:current mean train loss 1402.1638878236793
INFO:root:current train perplexity3.022578001022339
INFO:root:current mean train loss 1402.250702044931
INFO:root:current train perplexity3.0237038135528564
INFO:root:current mean train loss 1402.694071684006
INFO:root:current train perplexity3.0236592292785645
INFO:root:current mean train loss 1403.2849877734634
INFO:root:current train perplexity3.024320363998413
INFO:root:current mean train loss 1403.4602330430469
INFO:root:current train perplexity3.0247724056243896
INFO:root:current mean train loss 1403.462141635415
INFO:root:current train perplexity3.0244531631469727
INFO:root:current mean train loss 1404.4898967510057
INFO:root:current train perplexity3.0262961387634277
INFO:root:current mean train loss 1404.8232505308968
INFO:root:current train perplexity3.027317762374878

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.13s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.13s/it]
INFO:root:final mean train loss: 1404.563427161882
INFO:root:final train perplexity: 3.0274577140808105
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2290.4653709898603
INFO:root:eval perplexity: 6.375062942504883
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/160
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 160/200 [8:50:35<2:12:37, 198.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1402.1356972142269
INFO:root:current train perplexity3.0285532474517822
INFO:root:current mean train loss 1391.8293364709166
INFO:root:current train perplexity3.003587484359741
INFO:root:current mean train loss 1397.3540657775043
INFO:root:current train perplexity3.012387990951538
INFO:root:current mean train loss 1399.2501239836404
INFO:root:current train perplexity3.018707752227783
INFO:root:current mean train loss 1401.5767053135255
INFO:root:current train perplexity3.015787124633789
INFO:root:current mean train loss 1402.5976404914047
INFO:root:current train perplexity3.0189898014068604
INFO:root:current mean train loss 1401.5917745907589
INFO:root:current train perplexity3.0186429023742676
INFO:root:current mean train loss 1400.659508037965
INFO:root:current train perplexity3.0187318325042725
INFO:root:current mean train loss 1400.5492178855216
INFO:root:current train perplexity3.0208842754364014
INFO:root:current mean train loss 1401.2321250010627
INFO:root:current train perplexity3.0204520225524902
INFO:root:current mean train loss 1401.888475172887
INFO:root:current train perplexity3.019862413406372
INFO:root:current mean train loss 1402.381135042274
INFO:root:current train perplexity3.020707368850708
INFO:root:current mean train loss 1402.424403527802
INFO:root:current train perplexity3.0205774307250977
INFO:root:current mean train loss 1403.0147184960492
INFO:root:current train perplexity3.020085334777832
INFO:root:current mean train loss 1403.413203104354
INFO:root:current train perplexity3.019981861114502
INFO:root:current mean train loss 1403.5907608444084
INFO:root:current train perplexity3.0208182334899902
INFO:root:current mean train loss 1404.2191392829052
INFO:root:current train perplexity3.0222227573394775
INFO:root:current mean train loss 1403.575555345359
INFO:root:current train perplexity3.0209505558013916
INFO:root:current mean train loss 1403.7366277643323
INFO:root:current train perplexity3.022736072540283
INFO:root:current mean train loss 1403.8423959673414
INFO:root:current train perplexity3.023872137069702

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.14s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.14s/it]
INFO:root:final mean train loss: 1403.373465749151
INFO:root:final train perplexity: 3.0246176719665527
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.35s/it]
INFO:root:eval mean loss: 2289.651951133782
INFO:root:eval perplexity: 6.3708696365356445
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/161
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/200 [8:53:54<2:09:16, 198.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1407.991967095269
INFO:root:current train perplexity3.038837432861328
INFO:root:current mean train loss 1400.4626087861902
INFO:root:current train perplexity3.022629499435425
INFO:root:current mean train loss 1399.777361853648
INFO:root:current train perplexity3.014092445373535
INFO:root:current mean train loss 1399.325769696917
INFO:root:current train perplexity3.008880138397217
INFO:root:current mean train loss 1397.941788419671
INFO:root:current train perplexity3.011294364929199
INFO:root:current mean train loss 1398.466656812981
INFO:root:current train perplexity3.014181137084961
INFO:root:current mean train loss 1398.9607269479043
INFO:root:current train perplexity3.013479471206665
INFO:root:current mean train loss 1399.243203204611
INFO:root:current train perplexity3.014629602432251
INFO:root:current mean train loss 1399.8674173309473
INFO:root:current train perplexity3.0143752098083496
INFO:root:current mean train loss 1401.211894108699
INFO:root:current train perplexity3.014758348464966
INFO:root:current mean train loss 1400.7044324248914
INFO:root:current train perplexity3.0139598846435547
INFO:root:current mean train loss 1401.1479218174034
INFO:root:current train perplexity3.014376401901245
INFO:root:current mean train loss 1400.576864890682
INFO:root:current train perplexity3.0130221843719482
INFO:root:current mean train loss 1400.8630870887619
INFO:root:current train perplexity3.015606641769409
INFO:root:current mean train loss 1400.3295583910929
INFO:root:current train perplexity3.0161631107330322
INFO:root:current mean train loss 1400.6639308929443
INFO:root:current train perplexity3.017526626586914
INFO:root:current mean train loss 1400.8594514804831
INFO:root:current train perplexity3.0181949138641357
INFO:root:current mean train loss 1401.0449545724052
INFO:root:current train perplexity3.0179684162139893
INFO:root:current mean train loss 1401.6023310908565
INFO:root:current train perplexity3.0189671516418457
INFO:root:current mean train loss 1401.2052423146145
INFO:root:current train perplexity3.019411325454712

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.31s/it]
INFO:root:final mean train loss: 1401.1095944557055
INFO:root:final train perplexity: 3.0192222595214844
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2291.030690294631
INFO:root:eval perplexity: 6.377978324890137
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/162
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 162/200 [8:57:13<2:05:58, 198.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1394.3961688347583
INFO:root:current train perplexity3.0029146671295166
INFO:root:current mean train loss 1397.2928825827205
INFO:root:current train perplexity3.000732660293579
INFO:root:current mean train loss 1396.336003118824
INFO:root:current train perplexity3.0123701095581055
INFO:root:current mean train loss 1393.453736043179
INFO:root:current train perplexity3.006422519683838
INFO:root:current mean train loss 1397.127685816346
INFO:root:current train perplexity3.0099802017211914
INFO:root:current mean train loss 1396.8229026863416
INFO:root:current train perplexity3.0084950923919678
INFO:root:current mean train loss 1396.1797684440203
INFO:root:current train perplexity3.009678840637207
INFO:root:current mean train loss 1397.072281674085
INFO:root:current train perplexity3.0100579261779785
INFO:root:current mean train loss 1396.72052080662
INFO:root:current train perplexity3.0091423988342285
INFO:root:current mean train loss 1397.4928065617212
INFO:root:current train perplexity3.0107040405273438
INFO:root:current mean train loss 1398.1190024409425
INFO:root:current train perplexity3.0116469860076904
INFO:root:current mean train loss 1397.930527275992
INFO:root:current train perplexity3.012742042541504
INFO:root:current mean train loss 1399.2873571202551
INFO:root:current train perplexity3.01351261138916
INFO:root:current mean train loss 1399.340895286949
INFO:root:current train perplexity3.0133564472198486
INFO:root:current mean train loss 1399.8134753863235
INFO:root:current train perplexity3.0136287212371826
INFO:root:current mean train loss 1400.2141867869145
INFO:root:current train perplexity3.014491081237793
INFO:root:current mean train loss 1399.7554289165296
INFO:root:current train perplexity3.014375686645508
INFO:root:current mean train loss 1399.8225993859855
INFO:root:current train perplexity3.014683485031128
INFO:root:current mean train loss 1400.0932966995033
INFO:root:current train perplexity3.0157129764556885
INFO:root:current mean train loss 1400.5487580580157
INFO:root:current train perplexity3.0162649154663086

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.20s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.21s/it]
INFO:root:final mean train loss: 1400.002298929327
INFO:root:final train perplexity: 3.0165867805480957
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.34s/it]
INFO:root:eval mean loss: 2292.3916868385695
INFO:root:eval perplexity: 6.385002613067627
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/163
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/200 [9:00:32<2:02:38, 198.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1390.3777640206474
INFO:root:current train perplexity2.9810969829559326
INFO:root:current mean train loss 1397.4978745404412
INFO:root:current train perplexity2.9889585971832275
INFO:root:current mean train loss 1396.2041295934607
INFO:root:current train perplexity2.9956533908843994
INFO:root:current mean train loss 1395.1992012642525
INFO:root:current train perplexity2.9953324794769287
INFO:root:current mean train loss 1394.360306630236
INFO:root:current train perplexity3.0004348754882812
INFO:root:current mean train loss 1394.1786104971902
INFO:root:current train perplexity3.001594066619873
INFO:root:current mean train loss 1395.3199991254664
INFO:root:current train perplexity3.003096103668213
INFO:root:current mean train loss 1394.4656104149756
INFO:root:current train perplexity3.0047380924224854
INFO:root:current mean train loss 1395.4951702249462
INFO:root:current train perplexity3.0064282417297363
INFO:root:current mean train loss 1395.3043531280202
INFO:root:current train perplexity3.008039951324463
INFO:root:current mean train loss 1395.4360771393108
INFO:root:current train perplexity3.008747100830078
INFO:root:current mean train loss 1395.1722011468348
INFO:root:current train perplexity3.010103464126587
INFO:root:current mean train loss 1395.249729907419
INFO:root:current train perplexity3.010352611541748
INFO:root:current mean train loss 1396.2951452547616
INFO:root:current train perplexity3.010652542114258
INFO:root:current mean train loss 1396.5888912693983
INFO:root:current train perplexity3.011824369430542
INFO:root:current mean train loss 1397.5134332547523
INFO:root:current train perplexity3.012624979019165
INFO:root:current mean train loss 1398.1222189897549
INFO:root:current train perplexity3.012061595916748
INFO:root:current mean train loss 1398.2801117115775
INFO:root:current train perplexity3.0121145248413086
INFO:root:current mean train loss 1398.3197139637994
INFO:root:current train perplexity3.0122547149658203
INFO:root:current mean train loss 1399.0255054454512
INFO:root:current train perplexity3.0138585567474365

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.12s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.12s/it]
INFO:root:final mean train loss: 1398.8389220975953
INFO:root:final train perplexity: 3.0138204097747803
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.33s/it]
INFO:root:eval mean loss: 2293.91874575784
INFO:root:eval perplexity: 6.392893314361572
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/164
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/200 [9:03:50<1:59:18, 198.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1400.986398280352
INFO:root:current train perplexity2.996448516845703
INFO:root:current mean train loss 1396.6357630765374
INFO:root:current train perplexity3.0123822689056396
INFO:root:current mean train loss 1393.1814334882677
INFO:root:current train perplexity3.011507034301758
INFO:root:current mean train loss 1394.2027780301196
INFO:root:current train perplexity3.0115816593170166
INFO:root:current mean train loss 1394.1964108821549
INFO:root:current train perplexity3.008897542953491
INFO:root:current mean train loss 1395.278737472716
INFO:root:current train perplexity3.0090394020080566
INFO:root:current mean train loss 1396.8259508335607
INFO:root:current train perplexity3.008448839187622
INFO:root:current mean train loss 1396.3780776609156
INFO:root:current train perplexity3.007150411605835
INFO:root:current mean train loss 1396.7438759787644
INFO:root:current train perplexity3.006049633026123
INFO:root:current mean train loss 1397.8651977662742
INFO:root:current train perplexity3.0065460205078125
INFO:root:current mean train loss 1397.9315479773388
INFO:root:current train perplexity3.007290840148926
INFO:root:current mean train loss 1398.2680927331244
INFO:root:current train perplexity3.0070507526397705
INFO:root:current mean train loss 1398.6405437146404
INFO:root:current train perplexity3.007873296737671
INFO:root:current mean train loss 1397.9148860125551
INFO:root:current train perplexity3.008439064025879
INFO:root:current mean train loss 1397.6326899371375
INFO:root:current train perplexity3.0078108310699463
INFO:root:current mean train loss 1397.7088509976074
INFO:root:current train perplexity3.008335828781128
INFO:root:current mean train loss 1398.2860524212035
INFO:root:current train perplexity3.009521007537842
INFO:root:current mean train loss 1398.4706701311775
INFO:root:current train perplexity3.0106425285339355
INFO:root:current mean train loss 1398.3480368868409
INFO:root:current train perplexity3.0112738609313965

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.04s/it]
INFO:root:final mean train loss: 1397.7415655138993
INFO:root:final train perplexity: 3.0112133026123047
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.36s/it]
INFO:root:eval mean loss: 2296.1265821524544
INFO:root:eval perplexity: 6.404317378997803
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/165
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 165/200 [9:07:09<1:55:57, 198.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1349.0972900390625
INFO:root:current train perplexity2.9193174839019775
INFO:root:current mean train loss 1385.1327409010667
INFO:root:current train perplexity2.997450113296509
INFO:root:current mean train loss 1387.6688800886566
INFO:root:current train perplexity3.0059587955474854
INFO:root:current mean train loss 1388.581831681101
INFO:root:current train perplexity3.0084502696990967
INFO:root:current mean train loss 1391.5360992733795
INFO:root:current train perplexity3.0083727836608887
INFO:root:current mean train loss 1392.2097916376024
INFO:root:current train perplexity3.008361339569092
INFO:root:current mean train loss 1393.4132453968982
INFO:root:current train perplexity3.0028131008148193
INFO:root:current mean train loss 1393.8242986852472
INFO:root:current train perplexity3.0024211406707764
INFO:root:current mean train loss 1393.1262179702076
INFO:root:current train perplexity3.000840187072754
INFO:root:current mean train loss 1393.7867847543903
INFO:root:current train perplexity3.0022332668304443
INFO:root:current mean train loss 1395.3181732299317
INFO:root:current train perplexity3.0037639141082764
INFO:root:current mean train loss 1395.4029623943827
INFO:root:current train perplexity3.0049121379852295
INFO:root:current mean train loss 1396.5744764765236
INFO:root:current train perplexity3.0060553550720215
INFO:root:current mean train loss 1396.3764286158275
INFO:root:current train perplexity3.0044572353363037
INFO:root:current mean train loss 1396.767147575009
INFO:root:current train perplexity3.005337715148926
INFO:root:current mean train loss 1396.541275024414
INFO:root:current train perplexity3.005821704864502
INFO:root:current mean train loss 1396.6173080482388
INFO:root:current train perplexity3.0067851543426514
INFO:root:current mean train loss 1396.0963659152178
INFO:root:current train perplexity3.006312131881714
INFO:root:current mean train loss 1396.425614384484
INFO:root:current train perplexity3.0071239471435547
INFO:root:current mean train loss 1396.436629736123
INFO:root:current train perplexity3.00710129737854

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.04s/it]
INFO:root:final mean train loss: 1396.0329192792053
INFO:root:final train perplexity: 3.0071587562561035
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it]
INFO:root:eval mean loss: 2296.2636619189107
INFO:root:eval perplexity: 6.405026435852051
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/166
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/200 [9:10:28<1:52:37, 198.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1412.945056733631
INFO:root:current train perplexity2.9859955310821533
INFO:root:current mean train loss 1400.9352724690082
INFO:root:current train perplexity2.9966845512390137
INFO:root:current mean train loss 1396.803213266226
INFO:root:current train perplexity3.0019593238830566
INFO:root:current mean train loss 1396.2081306433752
INFO:root:current train perplexity3.001152515411377
INFO:root:current mean train loss 1394.7611162271749
INFO:root:current train perplexity2.9991769790649414
INFO:root:current mean train loss 1395.2086469829655
INFO:root:current train perplexity3.000459909439087
INFO:root:current mean train loss 1394.522857616873
INFO:root:current train perplexity3.0002424716949463
INFO:root:current mean train loss 1394.1698345126126
INFO:root:current train perplexity3.000328302383423
INFO:root:current mean train loss 1394.4530762313489
INFO:root:current train perplexity3.00285267829895
INFO:root:current mean train loss 1393.787235023921
INFO:root:current train perplexity3.0039730072021484
INFO:root:current mean train loss 1393.9187130082717
INFO:root:current train perplexity3.004641056060791
INFO:root:current mean train loss 1394.0627667906026
INFO:root:current train perplexity3.003260612487793
INFO:root:current mean train loss 1393.9666318151426
INFO:root:current train perplexity3.002305507659912
INFO:root:current mean train loss 1393.8322796413702
INFO:root:current train perplexity3.0020313262939453
INFO:root:current mean train loss 1394.6855691242688
INFO:root:current train perplexity3.0046730041503906
INFO:root:current mean train loss 1394.5706379405767
INFO:root:current train perplexity3.0053746700286865
INFO:root:current mean train loss 1394.5509455667316
INFO:root:current train perplexity3.0050604343414307
INFO:root:current mean train loss 1395.3321516179956
INFO:root:current train perplexity3.0048649311065674
INFO:root:current mean train loss 1395.3978291594806
INFO:root:current train perplexity3.0050601959228516
INFO:root:current mean train loss 1396.0688253518879
INFO:root:current train perplexity3.005720376968384

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.25s/it]
INFO:root:final mean train loss: 1395.7782136625674
INFO:root:final train perplexity: 3.006554365158081
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2297.210737512467
INFO:root:eval perplexity: 6.409934043884277
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/167
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 167/200 [9:13:46<1:49:19, 198.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1387.7580727025081
INFO:root:current train perplexity2.968231201171875
INFO:root:current mean train loss 1394.763673644135
INFO:root:current train perplexity2.9882044792175293
INFO:root:current mean train loss 1394.0309094340862
INFO:root:current train perplexity2.9957258701324463
INFO:root:current mean train loss 1391.981469148715
INFO:root:current train perplexity2.9908063411712646
INFO:root:current mean train loss 1392.2216816383955
INFO:root:current train perplexity2.9894745349884033
INFO:root:current mean train loss 1391.040960716049
INFO:root:current train perplexity2.9920554161071777
INFO:root:current mean train loss 1390.7247702858665
INFO:root:current train perplexity2.9948511123657227
INFO:root:current mean train loss 1390.3206505917599
INFO:root:current train perplexity2.994291305541992
INFO:root:current mean train loss 1392.3064444093545
INFO:root:current train perplexity2.993546724319458
INFO:root:current mean train loss 1394.1071060278268
INFO:root:current train perplexity2.9950850009918213
INFO:root:current mean train loss 1393.8674250549434
INFO:root:current train perplexity2.9950270652770996
INFO:root:current mean train loss 1394.9597325651844
INFO:root:current train perplexity2.9964869022369385
INFO:root:current mean train loss 1395.1017783416107
INFO:root:current train perplexity2.9974098205566406
INFO:root:current mean train loss 1394.6472916995108
INFO:root:current train perplexity2.9970221519470215
INFO:root:current mean train loss 1394.44660001073
INFO:root:current train perplexity2.998429298400879
INFO:root:current mean train loss 1395.3382002454741
INFO:root:current train perplexity3.0004348754882812
INFO:root:current mean train loss 1395.9200808346927
INFO:root:current train perplexity3.001978874206543
INFO:root:current mean train loss 1395.8960700523455
INFO:root:current train perplexity3.0027880668640137
INFO:root:current mean train loss 1395.8279923033274
INFO:root:current train perplexity3.004035472869873
INFO:root:current mean train loss 1395.538530379257
INFO:root:current train perplexity3.0044994354248047

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.07s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.07s/it]
INFO:root:final mean train loss: 1395.1455488719546
INFO:root:final train perplexity: 3.0050547122955322
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it]
INFO:root:eval mean loss: 2297.75414346465
INFO:root:eval perplexity: 6.412753105163574
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/168
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 168/200 [9:17:05<1:45:59, 198.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1375.7847500887783
INFO:root:current train perplexity2.998380422592163
INFO:root:current mean train loss 1387.5123834425403
INFO:root:current train perplexity2.9957330226898193
INFO:root:current mean train loss 1386.3061389399509
INFO:root:current train perplexity2.9937798976898193
INFO:root:current mean train loss 1385.804920980964
INFO:root:current train perplexity3.0007214546203613
INFO:root:current mean train loss 1385.299575302627
INFO:root:current train perplexity2.9998626708984375
INFO:root:current mean train loss 1386.4189149598817
INFO:root:current train perplexity2.996434450149536
INFO:root:current mean train loss 1388.00325620229
INFO:root:current train perplexity2.994940996170044
INFO:root:current mean train loss 1389.3316517810947
INFO:root:current train perplexity2.9952170848846436
INFO:root:current mean train loss 1390.2089919419316
INFO:root:current train perplexity2.9955995082855225
INFO:root:current mean train loss 1390.9737101450016
INFO:root:current train perplexity2.9949302673339844
INFO:root:current mean train loss 1391.507025464677
INFO:root:current train perplexity2.9972400665283203
INFO:root:current mean train loss 1390.7030535545184
INFO:root:current train perplexity2.996926784515381
INFO:root:current mean train loss 1391.3456769601282
INFO:root:current train perplexity2.996445655822754
INFO:root:current mean train loss 1391.6012089014932
INFO:root:current train perplexity2.9975409507751465
INFO:root:current mean train loss 1391.457679690856
INFO:root:current train perplexity2.9968814849853516
INFO:root:current mean train loss 1391.4412899103195
INFO:root:current train perplexity2.996814489364624
INFO:root:current mean train loss 1391.92804010397
INFO:root:current train perplexity2.997861862182617
INFO:root:current mean train loss 1392.420146957354
INFO:root:current train perplexity2.9989593029022217
INFO:root:current mean train loss 1393.3029775943396
INFO:root:current train perplexity2.999699354171753
INFO:root:current mean train loss 1393.5614154786406
INFO:root:current train perplexity3.0003697872161865

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.17s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.17s/it]
INFO:root:final mean train loss: 1393.2153005749062
INFO:root:final train perplexity: 3.000483512878418
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.41s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.41s/it]
INFO:root:eval mean loss: 2298.3101053440823
INFO:root:eval perplexity: 6.4156365394592285
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/169
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 169/200 [9:20:24<1:42:42, 198.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1395.1392737494575
INFO:root:current train perplexity2.996241569519043
INFO:root:current mean train loss 1387.6542209359102
INFO:root:current train perplexity2.9985105991363525
INFO:root:current mean train loss 1388.1876216215246
INFO:root:current train perplexity2.990562915802002
INFO:root:current mean train loss 1386.503652593141
INFO:root:current train perplexity2.9886298179626465
INFO:root:current mean train loss 1391.456679521981
INFO:root:current train perplexity2.9912681579589844
INFO:root:current mean train loss 1389.4132805670893
INFO:root:current train perplexity2.9892771244049072
INFO:root:current mean train loss 1389.2272778465635
INFO:root:current train perplexity2.9921464920043945
INFO:root:current mean train loss 1390.0005210125385
INFO:root:current train perplexity2.991751194000244
INFO:root:current mean train loss 1391.3312043356239
INFO:root:current train perplexity2.991988182067871
INFO:root:current mean train loss 1391.3036785361207
INFO:root:current train perplexity2.9927353858947754
INFO:root:current mean train loss 1391.76439006293
INFO:root:current train perplexity2.992572784423828
INFO:root:current mean train loss 1391.5999340278704
INFO:root:current train perplexity2.9950153827667236
INFO:root:current mean train loss 1391.7637550785857
INFO:root:current train perplexity2.9976541996002197
INFO:root:current mean train loss 1391.7228074143302
INFO:root:current train perplexity2.9975876808166504
INFO:root:current mean train loss 1391.76460647583
INFO:root:current train perplexity2.997328758239746
INFO:root:current mean train loss 1392.1041449238628
INFO:root:current train perplexity2.9972689151763916
INFO:root:current mean train loss 1392.414841574345
INFO:root:current train perplexity2.997098684310913
INFO:root:current mean train loss 1392.3896263243114
INFO:root:current train perplexity2.997177839279175
INFO:root:current mean train loss 1392.2792113866562
INFO:root:current train perplexity2.9973337650299072
INFO:root:current mean train loss 1392.152229788821
INFO:root:current train perplexity2.9969546794891357

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.87s/it]
INFO:root:final mean train loss: 1391.6205144546516
INFO:root:final train perplexity: 2.9967119693756104
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2299.871921836907
INFO:root:eval perplexity: 6.423745155334473
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/170
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/200 [9:23:42<1:39:20, 198.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1383.372968695137
INFO:root:current train perplexity2.99450945854187
INFO:root:current mean train loss 1385.8950027385085
INFO:root:current train perplexity2.9885849952697754
INFO:root:current mean train loss 1387.8308629230644
INFO:root:current train perplexity2.9913201332092285
INFO:root:current mean train loss 1388.2355743643557
INFO:root:current train perplexity2.992788314819336
INFO:root:current mean train loss 1389.2731916119471
INFO:root:current train perplexity2.997504472732544
INFO:root:current mean train loss 1387.5211873855978
INFO:root:current train perplexity2.9958744049072266
INFO:root:current mean train loss 1389.0933924704054
INFO:root:current train perplexity2.9948582649230957
INFO:root:current mean train loss 1389.8132011693995
INFO:root:current train perplexity2.992030143737793
INFO:root:current mean train loss 1390.3740425238593
INFO:root:current train perplexity2.993459463119507
INFO:root:current mean train loss 1391.1955062819925
INFO:root:current train perplexity2.9930858612060547
INFO:root:current mean train loss 1392.009300210916
INFO:root:current train perplexity2.9934659004211426
INFO:root:current mean train loss 1391.6446147177578
INFO:root:current train perplexity2.993321657180786
INFO:root:current mean train loss 1390.8190765499237
INFO:root:current train perplexity2.991684675216675
INFO:root:current mean train loss 1390.5921437515467
INFO:root:current train perplexity2.9920248985290527
INFO:root:current mean train loss 1391.1236481266264
INFO:root:current train perplexity2.992910861968994
INFO:root:current mean train loss 1391.003605184201
INFO:root:current train perplexity2.992690086364746
INFO:root:current mean train loss 1391.0860382495653
INFO:root:current train perplexity2.994101047515869
INFO:root:current mean train loss 1391.2239541255808
INFO:root:current train perplexity2.9935171604156494
INFO:root:current mean train loss 1391.6518488127192
INFO:root:current train perplexity2.993605613708496

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.15s/it]
INFO:root:final mean train loss: 1390.5921262432335
INFO:root:final train perplexity: 2.9942824840545654
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2300.2326902565383
INFO:root:eval perplexity: 6.425620079040527
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/171
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/200 [9:27:01<1:36:02, 198.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1360.9137369791667
INFO:root:current train perplexity2.9500064849853516
INFO:root:current mean train loss 1381.3916234430278
INFO:root:current train perplexity2.9855847358703613
INFO:root:current mean train loss 1385.4107488243326
INFO:root:current train perplexity2.9821527004241943
INFO:root:current mean train loss 1382.8443220549939
INFO:root:current train perplexity2.9800126552581787
INFO:root:current mean train loss 1384.272972670682
INFO:root:current train perplexity2.980839729309082
INFO:root:current mean train loss 1384.37629018188
INFO:root:current train perplexity2.9837472438812256
INFO:root:current mean train loss 1384.525255461337
INFO:root:current train perplexity2.9857304096221924
INFO:root:current mean train loss 1384.986666152565
INFO:root:current train perplexity2.9845728874206543
INFO:root:current mean train loss 1385.0496247625233
INFO:root:current train perplexity2.984856367111206
INFO:root:current mean train loss 1385.9259921109704
INFO:root:current train perplexity2.986412763595581
INFO:root:current mean train loss 1387.106375540701
INFO:root:current train perplexity2.985668420791626
INFO:root:current mean train loss 1387.3820382475205
INFO:root:current train perplexity2.9881463050842285
INFO:root:current mean train loss 1387.5793034947333
INFO:root:current train perplexity2.9894139766693115
INFO:root:current mean train loss 1388.264841076791
INFO:root:current train perplexity2.98922061920166
INFO:root:current mean train loss 1387.6943780456804
INFO:root:current train perplexity2.9898788928985596
INFO:root:current mean train loss 1387.7575597674406
INFO:root:current train perplexity2.990687370300293
INFO:root:current mean train loss 1387.9859463322355
INFO:root:current train perplexity2.991999864578247
INFO:root:current mean train loss 1388.9711963434431
INFO:root:current train perplexity2.991703510284424
INFO:root:current mean train loss 1389.4918640069127
INFO:root:current train perplexity2.9920690059661865
INFO:root:current mean train loss 1389.6919706170481
INFO:root:current train perplexity2.9924426078796387

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.23s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.23s/it]
INFO:root:final mean train loss: 1389.7517408100691
INFO:root:final train perplexity: 2.992298126220703
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.33s/it]
INFO:root:eval mean loss: 2301.2925531914893
INFO:root:eval perplexity: 6.431129455566406
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/172
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 172/200 [9:30:20<1:32:44, 198.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1371.6502101732337
INFO:root:current train perplexity2.976637840270996
INFO:root:current mean train loss 1383.3648175495427
INFO:root:current train perplexity2.982666015625
INFO:root:current mean train loss 1385.9318404261842
INFO:root:current train perplexity2.9915177822113037
INFO:root:current mean train loss 1386.9288753355988
INFO:root:current train perplexity2.985567092895508
INFO:root:current mean train loss 1385.3024498328532
INFO:root:current train perplexity2.9819610118865967
INFO:root:current mean train loss 1387.4642735439322
INFO:root:current train perplexity2.9839277267456055
INFO:root:current mean train loss 1387.542265327172
INFO:root:current train perplexity2.9863579273223877
INFO:root:current mean train loss 1387.4851519952672
INFO:root:current train perplexity2.9842824935913086
INFO:root:current mean train loss 1388.1274064018833
INFO:root:current train perplexity2.9864189624786377
INFO:root:current mean train loss 1387.9447811039918
INFO:root:current train perplexity2.986435651779175
INFO:root:current mean train loss 1388.7215028466367
INFO:root:current train perplexity2.986530303955078
INFO:root:current mean train loss 1388.3189443994183
INFO:root:current train perplexity2.986609697341919
INFO:root:current mean train loss 1388.2719996055423
INFO:root:current train perplexity2.9870505332946777
INFO:root:current mean train loss 1388.404171113975
INFO:root:current train perplexity2.989098310470581
INFO:root:current mean train loss 1388.8318687755293
INFO:root:current train perplexity2.989114284515381
INFO:root:current mean train loss 1388.842744663211
INFO:root:current train perplexity2.9879250526428223
INFO:root:current mean train loss 1388.3264904762593
INFO:root:current train perplexity2.987401247024536
INFO:root:current mean train loss 1388.3396278066962
INFO:root:current train perplexity2.9889259338378906
INFO:root:current mean train loss 1388.3161571542444
INFO:root:current train perplexity2.98964524269104
INFO:root:current mean train loss 1388.5735739292852
INFO:root:current train perplexity2.9891257286071777

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.25s/it]
INFO:root:final mean train loss: 1388.4562694339877
INFO:root:final train perplexity: 2.9892430305480957
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2303.180944997368
INFO:root:eval perplexity: 6.440959930419922
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/173
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 173/200 [9:33:39<1:29:27, 198.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1400.8866485595704
INFO:root:current train perplexity2.9823570251464844
INFO:root:current mean train loss 1387.0957903180804
INFO:root:current train perplexity2.984736442565918
INFO:root:current mean train loss 1385.250894165039
INFO:root:current train perplexity2.9796061515808105
INFO:root:current mean train loss 1386.2568276798024
INFO:root:current train perplexity2.9862053394317627
INFO:root:current mean train loss 1386.4136066783558
INFO:root:current train perplexity2.982229709625244
INFO:root:current mean train loss 1385.536828161169
INFO:root:current train perplexity2.9825422763824463
INFO:root:current mean train loss 1385.780930709839
INFO:root:current train perplexity2.9861366748809814
INFO:root:current mean train loss 1386.2800149453653
INFO:root:current train perplexity2.98832631111145
INFO:root:current mean train loss 1386.1543378557478
INFO:root:current train perplexity2.9848618507385254
INFO:root:current mean train loss 1385.2510908410904
INFO:root:current train perplexity2.9855453968048096
INFO:root:current mean train loss 1385.77396674523
INFO:root:current train perplexity2.9859471321105957
INFO:root:current mean train loss 1386.842158481531
INFO:root:current train perplexity2.986670970916748
INFO:root:current mean train loss 1386.409348912393
INFO:root:current train perplexity2.987403392791748
INFO:root:current mean train loss 1385.6231987341127
INFO:root:current train perplexity2.9872705936431885
INFO:root:current mean train loss 1386.3511169433593
INFO:root:current train perplexity2.9869027137756348
INFO:root:current mean train loss 1386.6230627282873
INFO:root:current train perplexity2.986330509185791
INFO:root:current mean train loss 1386.5122284679878
INFO:root:current train perplexity2.986034393310547
INFO:root:current mean train loss 1386.7532944953305
INFO:root:current train perplexity2.986083984375
INFO:root:current mean train loss 1387.0205726955248
INFO:root:current train perplexity2.9857287406921387
INFO:root:current mean train loss 1387.0588051707475
INFO:root:current train perplexity2.985473871231079

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.34s/it]
INFO:root:final mean train loss: 1386.6837839962434
INFO:root:final train perplexity: 2.985067367553711
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it]
INFO:root:eval mean loss: 2304.901075863669
INFO:root:eval perplexity: 6.44992733001709
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/174
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 174/200 [9:36:58<1:26:09, 198.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1387.277009662829
INFO:root:current train perplexity2.9412713050842285
INFO:root:current mean train loss 1387.3260412519905
INFO:root:current train perplexity2.977196216583252
INFO:root:current mean train loss 1386.7838438753952
INFO:root:current train perplexity2.9815831184387207
INFO:root:current mean train loss 1383.949220801602
INFO:root:current train perplexity2.9790923595428467
INFO:root:current mean train loss 1383.8775600041029
INFO:root:current train perplexity2.980684757232666
INFO:root:current mean train loss 1383.0926916920305
INFO:root:current train perplexity2.982017993927002
INFO:root:current mean train loss 1384.9949181961686
INFO:root:current train perplexity2.982667922973633
INFO:root:current mean train loss 1385.270755500743
INFO:root:current train perplexity2.9827332496643066
INFO:root:current mean train loss 1384.3505494730894
INFO:root:current train perplexity2.981368064880371
INFO:root:current mean train loss 1384.8612028658079
INFO:root:current train perplexity2.982405185699463
INFO:root:current mean train loss 1385.0627034890167
INFO:root:current train perplexity2.9841601848602295
INFO:root:current mean train loss 1386.2295179309367
INFO:root:current train perplexity2.9854798316955566
INFO:root:current mean train loss 1386.3809859124838
INFO:root:current train perplexity2.9853098392486572
INFO:root:current mean train loss 1386.3775705830876
INFO:root:current train perplexity2.9840292930603027
INFO:root:current mean train loss 1386.7811655477865
INFO:root:current train perplexity2.9834301471710205
INFO:root:current mean train loss 1386.8593564189698
INFO:root:current train perplexity2.983898401260376
INFO:root:current mean train loss 1387.4999311190452
INFO:root:current train perplexity2.984651565551758
INFO:root:current mean train loss 1387.2334384559974
INFO:root:current train perplexity2.984442710876465
INFO:root:current mean train loss 1387.347812108218
INFO:root:current train perplexity2.985560655593872
INFO:root:current mean train loss 1387.3612732339038
INFO:root:current train perplexity2.9856603145599365

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.14s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.14s/it]
INFO:root:final mean train loss: 1387.1119537738255
INFO:root:final train perplexity: 2.9860754013061523
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it]
INFO:root:eval mean loss: 2303.9560226548647
INFO:root:eval perplexity: 6.444997787475586
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/175
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/200 [9:40:17<1:22:50, 198.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1372.4508056640625
INFO:root:current train perplexity2.9682912826538086
INFO:root:current mean train loss 1374.1596300848598
INFO:root:current train perplexity2.9598355293273926
INFO:root:current mean train loss 1373.7253729827212
INFO:root:current train perplexity2.9649627208709717
INFO:root:current mean train loss 1377.4253474760822
INFO:root:current train perplexity2.9720072746276855
INFO:root:current mean train loss 1382.4411667449565
INFO:root:current train perplexity2.9769668579101562
INFO:root:current mean train loss 1381.2400519075295
INFO:root:current train perplexity2.9745397567749023
INFO:root:current mean train loss 1381.4850112507534
INFO:root:current train perplexity2.9764370918273926
INFO:root:current mean train loss 1382.1406792534722
INFO:root:current train perplexity2.9771347045898438
INFO:root:current mean train loss 1383.0836479134637
INFO:root:current train perplexity2.9781010150909424
INFO:root:current mean train loss 1384.1903315550003
INFO:root:current train perplexity2.9783365726470947
INFO:root:current mean train loss 1384.1514917583218
INFO:root:current train perplexity2.9805750846862793
INFO:root:current mean train loss 1384.9938466788556
INFO:root:current train perplexity2.980797529220581
INFO:root:current mean train loss 1384.3974360251914
INFO:root:current train perplexity2.9812209606170654
INFO:root:current mean train loss 1385.1440842807554
INFO:root:current train perplexity2.981987237930298
INFO:root:current mean train loss 1385.7203933943563
INFO:root:current train perplexity2.98149037361145
INFO:root:current mean train loss 1386.5140438249484
INFO:root:current train perplexity2.98191237449646
INFO:root:current mean train loss 1386.797020332218
INFO:root:current train perplexity2.9822235107421875
INFO:root:current mean train loss 1387.0387775545862
INFO:root:current train perplexity2.9825518131256104
INFO:root:current mean train loss 1386.8018997501792
INFO:root:current train perplexity2.983748197555542
INFO:root:current mean train loss 1386.6492709050667
INFO:root:current train perplexity2.983985185623169

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.36s/it]
INFO:root:final mean train loss: 1386.1914890768307
INFO:root:final train perplexity: 2.9839086532592773
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2304.313169655225
INFO:root:eval perplexity: 6.446860313415527
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/176
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 176/200 [9:43:36<1:19:32, 198.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1389.5256857400411
INFO:root:current train perplexity2.977808713912964
INFO:root:current mean train loss 1382.6335334178666
INFO:root:current train perplexity2.9679501056671143
INFO:root:current mean train loss 1383.9007769712468
INFO:root:current train perplexity2.9736721515655518
INFO:root:current mean train loss 1385.954887058424
INFO:root:current train perplexity2.971308946609497
INFO:root:current mean train loss 1382.8492899038156
INFO:root:current train perplexity2.970224618911743
INFO:root:current mean train loss 1381.169846071608
INFO:root:current train perplexity2.971216917037964
INFO:root:current mean train loss 1381.48589990729
INFO:root:current train perplexity2.9721109867095947
INFO:root:current mean train loss 1380.2730353315621
INFO:root:current train perplexity2.9714832305908203
INFO:root:current mean train loss 1381.4443481308308
INFO:root:current train perplexity2.9724342823028564
INFO:root:current mean train loss 1382.4304436954071
INFO:root:current train perplexity2.9713971614837646
INFO:root:current mean train loss 1383.236270726219
INFO:root:current train perplexity2.9723591804504395
INFO:root:current mean train loss 1384.1761801565124
INFO:root:current train perplexity2.974172353744507
INFO:root:current mean train loss 1384.393064515153
INFO:root:current train perplexity2.9740097522735596
INFO:root:current mean train loss 1384.5352744589943
INFO:root:current train perplexity2.975931167602539
INFO:root:current mean train loss 1384.2316230553897
INFO:root:current train perplexity2.9755938053131104
INFO:root:current mean train loss 1384.2380007414754
INFO:root:current train perplexity2.9760913848876953
INFO:root:current mean train loss 1383.9611996876847
INFO:root:current train perplexity2.976109266281128
INFO:root:current mean train loss 1384.323292624145
INFO:root:current train perplexity2.976797342300415
INFO:root:current mean train loss 1384.0890942860508
INFO:root:current train perplexity2.9775021076202393

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.34s/it]
INFO:root:final mean train loss: 1383.6634820042627
INFO:root:final train perplexity: 2.9779648780822754
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2305.642533539035
INFO:root:eval perplexity: 6.453793525695801
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/177
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 177/200 [9:46:55<1:16:14, 198.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1377.3966827392578
INFO:root:current train perplexity2.917590856552124
INFO:root:current mean train loss 1375.6641167534722
INFO:root:current train perplexity2.964245319366455
INFO:root:current mean train loss 1379.2137545072114
INFO:root:current train perplexity2.965960741043091
INFO:root:current mean train loss 1381.3455965116427
INFO:root:current train perplexity2.9713144302368164
INFO:root:current mean train loss 1381.122823079427
INFO:root:current train perplexity2.9704041481018066
INFO:root:current mean train loss 1382.105524018055
INFO:root:current train perplexity2.9751012325286865
INFO:root:current mean train loss 1381.9065461409718
INFO:root:current train perplexity2.9763412475585938
INFO:root:current mean train loss 1383.4698213911327
INFO:root:current train perplexity2.977503538131714
INFO:root:current mean train loss 1383.0094862834062
INFO:root:current train perplexity2.977553129196167
INFO:root:current mean train loss 1383.2739464848053
INFO:root:current train perplexity2.9782521724700928
INFO:root:current mean train loss 1383.6628668648857
INFO:root:current train perplexity2.9788949489593506
INFO:root:current mean train loss 1383.859430085881
INFO:root:current train perplexity2.977804183959961
INFO:root:current mean train loss 1384.2640927548441
INFO:root:current train perplexity2.977748394012451
INFO:root:current mean train loss 1383.91917284111
INFO:root:current train perplexity2.977185010910034
INFO:root:current mean train loss 1384.3524228876288
INFO:root:current train perplexity2.977712869644165
INFO:root:current mean train loss 1384.9514294530732
INFO:root:current train perplexity2.9787492752075195
INFO:root:current mean train loss 1385.3360773342758
INFO:root:current train perplexity2.978071451187134
INFO:root:current mean train loss 1385.1279189670392
INFO:root:current train perplexity2.9792184829711914
INFO:root:current mean train loss 1384.9469809743155
INFO:root:current train perplexity2.9795727729797363
INFO:root:current mean train loss 1384.722915617425
INFO:root:current train perplexity2.979016065597534

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.79s/it]
INFO:root:final mean train loss: 1384.0642897489993
INFO:root:final train perplexity: 2.9789066314697266
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.34s/it]
INFO:root:eval mean loss: 2306.2334603384033
INFO:root:eval perplexity: 6.45688009262085
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/178
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/200 [9:50:14<1:12:58, 199.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1375.403203125
INFO:root:current train perplexity2.9708826541900635
INFO:root:current mean train loss 1375.2806015625
INFO:root:current train perplexity2.9564547538757324
INFO:root:current mean train loss 1375.647874891493
INFO:root:current train perplexity2.9688539505004883
INFO:root:current mean train loss 1377.782442533053
INFO:root:current train perplexity2.970325469970703
INFO:root:current mean train loss 1379.0866687729779
INFO:root:current train perplexity2.973456382751465
INFO:root:current mean train loss 1379.0956596447172
INFO:root:current train perplexity2.975961208343506
INFO:root:current mean train loss 1379.8361333984376
INFO:root:current train perplexity2.974841594696045
INFO:root:current mean train loss 1380.2073612607758
INFO:root:current train perplexity2.9731369018554688
INFO:root:current mean train loss 1380.6991968513257
INFO:root:current train perplexity2.971345901489258
INFO:root:current mean train loss 1381.432564136402
INFO:root:current train perplexity2.9742393493652344
INFO:root:current mean train loss 1381.9420928210748
INFO:root:current train perplexity2.9756040573120117
INFO:root:current mean train loss 1382.4653895399306
INFO:root:current train perplexity2.9746525287628174
INFO:root:current mean train loss 1382.715708605708
INFO:root:current train perplexity2.975076675415039
INFO:root:current mean train loss 1382.5816561947229
INFO:root:current train perplexity2.9759058952331543
INFO:root:current mean train loss 1382.5510721628289
INFO:root:current train perplexity2.9763922691345215
INFO:root:current mean train loss 1383.2353714939804
INFO:root:current train perplexity2.975825786590576
INFO:root:current mean train loss 1383.0479966195915
INFO:root:current train perplexity2.9751052856445312
INFO:root:current mean train loss 1383.2382673092166
INFO:root:current train perplexity2.9762258529663086
INFO:root:current mean train loss 1383.6045239592252
INFO:root:current train perplexity2.976500988006592
INFO:root:current mean train loss 1383.3077409065543
INFO:root:current train perplexity2.9760894775390625

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.06s/it]
INFO:root:final mean train loss: 1383.1108479694592
INFO:root:final train perplexity: 2.9766674041748047
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.34s/it]
INFO:root:eval mean loss: 2306.667803825216
INFO:root:eval perplexity: 6.45914888381958
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/179
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/200 [9:53:33<1:09:37, 198.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1382.7974562872023
INFO:root:current train perplexity2.9575605392456055
INFO:root:current mean train loss 1379.339677837533
INFO:root:current train perplexity2.9719455242156982
INFO:root:current mean train loss 1378.328225380133
INFO:root:current train perplexity2.959872245788574
INFO:root:current mean train loss 1377.417082134046
INFO:root:current train perplexity2.96431040763855
INFO:root:current mean train loss 1375.5639350166148
INFO:root:current train perplexity2.961665391921997
INFO:root:current mean train loss 1377.065535091386
INFO:root:current train perplexity2.964367151260376
INFO:root:current mean train loss 1377.5895853488246
INFO:root:current train perplexity2.9669384956359863
INFO:root:current mean train loss 1379.6301631464792
INFO:root:current train perplexity2.969799757003784
INFO:root:current mean train loss 1381.0991348665286
INFO:root:current train perplexity2.9715378284454346
INFO:root:current mean train loss 1381.8713658812699
INFO:root:current train perplexity2.9760568141937256
INFO:root:current mean train loss 1381.7856694842026
INFO:root:current train perplexity2.9741032123565674
INFO:root:current mean train loss 1382.0718532368514
INFO:root:current train perplexity2.97479510307312
INFO:root:current mean train loss 1382.2988716653772
INFO:root:current train perplexity2.974687337875366
INFO:root:current mean train loss 1381.9140023744585
INFO:root:current train perplexity2.974370002746582
INFO:root:current mean train loss 1381.8758608411981
INFO:root:current train perplexity2.974247694015503
INFO:root:current mean train loss 1382.1153254923345
INFO:root:current train perplexity2.9737608432769775
INFO:root:current mean train loss 1381.4411163887646
INFO:root:current train perplexity2.9730007648468018
INFO:root:current mean train loss 1381.452178779891
INFO:root:current train perplexity2.9732859134674072
INFO:root:current mean train loss 1381.6202387939188
INFO:root:current train perplexity2.9736154079437256
INFO:root:current mean train loss 1382.161566972978
INFO:root:current train perplexity2.9743926525115967

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.14s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.15s/it]
INFO:root:final mean train loss: 1382.1258373789515
INFO:root:final train perplexity: 2.974356174468994
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it]
INFO:root:eval mean loss: 2307.123478882702
INFO:root:eval perplexity: 6.461528778076172
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/180
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/200 [9:56:51<1:06:17, 198.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1363.0482819120762
INFO:root:current train perplexity2.960813045501709
INFO:root:current mean train loss 1371.8921635465802
INFO:root:current train perplexity2.9607865810394287
INFO:root:current mean train loss 1377.4168729450712
INFO:root:current train perplexity2.965578556060791
INFO:root:current mean train loss 1381.3505359532774
INFO:root:current train perplexity2.9663984775543213
INFO:root:current mean train loss 1377.8679880046636
INFO:root:current train perplexity2.96230411529541
INFO:root:current mean train loss 1379.0469461894845
INFO:root:current train perplexity2.963325262069702
INFO:root:current mean train loss 1380.58961053828
INFO:root:current train perplexity2.9656472206115723
INFO:root:current mean train loss 1380.1938164551425
INFO:root:current train perplexity2.966613531112671
INFO:root:current mean train loss 1380.3175962579126
INFO:root:current train perplexity2.9665021896362305
INFO:root:current mean train loss 1380.2248211841763
INFO:root:current train perplexity2.966771125793457
INFO:root:current mean train loss 1380.3127110583023
INFO:root:current train perplexity2.967930793762207
INFO:root:current mean train loss 1379.8691120822436
INFO:root:current train perplexity2.967395544052124
INFO:root:current mean train loss 1380.4445943309731
INFO:root:current train perplexity2.9667131900787354
INFO:root:current mean train loss 1381.2275458890958
INFO:root:current train perplexity2.9675564765930176
INFO:root:current mean train loss 1380.9347217499678
INFO:root:current train perplexity2.966916561126709
INFO:root:current mean train loss 1380.9376678760423
INFO:root:current train perplexity2.9674599170684814
INFO:root:current mean train loss 1380.6225217298402
INFO:root:current train perplexity2.967637538909912
INFO:root:current mean train loss 1380.1844450498995
INFO:root:current train perplexity2.9696245193481445
INFO:root:current mean train loss 1380.8018664872536
INFO:root:current train perplexity2.9701988697052
INFO:root:current mean train loss 1381.0678172556948
INFO:root:current train perplexity2.971069574356079

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.90s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.90s/it]
INFO:root:final mean train loss: 1380.8152475361865
INFO:root:final train perplexity: 2.971283435821533
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it]
INFO:root:eval mean loss: 2307.857613637938
INFO:root:eval perplexity: 6.465365886688232
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/181
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 181/200 [10:00:10<1:02:56, 198.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1385.3618790475946
INFO:root:current train perplexity2.955284833908081
INFO:root:current mean train loss 1383.2656867287376
INFO:root:current train perplexity2.96005916595459
INFO:root:current mean train loss 1379.63183947577
INFO:root:current train perplexity2.95859694480896
INFO:root:current mean train loss 1378.9128073834358
INFO:root:current train perplexity2.9621822834014893
INFO:root:current mean train loss 1379.509132962267
INFO:root:current train perplexity2.9612348079681396
INFO:root:current mean train loss 1380.9059641096328
INFO:root:current train perplexity2.965137481689453
INFO:root:current mean train loss 1380.5962905432345
INFO:root:current train perplexity2.9640328884124756
INFO:root:current mean train loss 1380.7550062985765
INFO:root:current train perplexity2.9643778800964355
INFO:root:current mean train loss 1381.975592347585
INFO:root:current train perplexity2.964905261993408
INFO:root:current mean train loss 1381.7314044139425
INFO:root:current train perplexity2.9678843021392822
INFO:root:current mean train loss 1381.3345992644922
INFO:root:current train perplexity2.968764543533325
INFO:root:current mean train loss 1381.4581935130009
INFO:root:current train perplexity2.967852830886841
INFO:root:current mean train loss 1380.2979009263568
INFO:root:current train perplexity2.9672434329986572
INFO:root:current mean train loss 1380.7427616562954
INFO:root:current train perplexity2.966707944869995
INFO:root:current mean train loss 1380.3190913833578
INFO:root:current train perplexity2.967411518096924
INFO:root:current mean train loss 1380.7044012389208
INFO:root:current train perplexity2.966981887817383
INFO:root:current mean train loss 1380.6199713732008
INFO:root:current train perplexity2.967127799987793
INFO:root:current mean train loss 1380.3285322447082
INFO:root:current train perplexity2.967892646789551
INFO:root:current mean train loss 1380.5590897094467
INFO:root:current train perplexity2.968994140625
INFO:root:current mean train loss 1380.3621672348454
INFO:root:current train perplexity2.969276189804077

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.25s/it]
INFO:root:final mean train loss: 1379.9140157463933
INFO:root:final train perplexity: 2.9691717624664307
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.34s/it]
INFO:root:eval mean loss: 2308.2845450326904
INFO:root:eval perplexity: 6.467599868774414
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/182
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/200 [10:03:29<59:38, 198.81s/it]  
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1383.0559738323252
INFO:root:current train perplexity2.9669876098632812
INFO:root:current mean train loss 1382.5006976349985
INFO:root:current train perplexity2.9670205116271973
INFO:root:current mean train loss 1384.0501192372813
INFO:root:current train perplexity2.9681642055511475
INFO:root:current mean train loss 1379.5587972005208
INFO:root:current train perplexity2.964481830596924
INFO:root:current mean train loss 1377.4290954713647
INFO:root:current train perplexity2.965193271636963
INFO:root:current mean train loss 1377.1726216256718
INFO:root:current train perplexity2.963942050933838
INFO:root:current mean train loss 1377.1276915358947
INFO:root:current train perplexity2.963601589202881
INFO:root:current mean train loss 1375.8868192694376
INFO:root:current train perplexity2.9616360664367676
INFO:root:current mean train loss 1376.2111797268687
INFO:root:current train perplexity2.9644291400909424
INFO:root:current mean train loss 1375.973982919499
INFO:root:current train perplexity2.9632627964019775
INFO:root:current mean train loss 1376.6145943155664
INFO:root:current train perplexity2.963531732559204
INFO:root:current mean train loss 1376.8309524676827
INFO:root:current train perplexity2.9647722244262695
INFO:root:current mean train loss 1376.8700149429924
INFO:root:current train perplexity2.9649317264556885
INFO:root:current mean train loss 1377.3415449351949
INFO:root:current train perplexity2.9668819904327393
INFO:root:current mean train loss 1377.5731884700215
INFO:root:current train perplexity2.9669923782348633
INFO:root:current mean train loss 1378.8483738058105
INFO:root:current train perplexity2.966752052307129
INFO:root:current mean train loss 1379.1165586900795
INFO:root:current train perplexity2.9664576053619385
INFO:root:current mean train loss 1379.1926801929378
INFO:root:current train perplexity2.9673943519592285
INFO:root:current mean train loss 1379.5492259981263
INFO:root:current train perplexity2.9679811000823975

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.44s/it]
INFO:root:final mean train loss: 1379.7931958389474
INFO:root:final train perplexity: 2.9688894748687744
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2309.209044111536
INFO:root:eval perplexity: 6.472436428070068
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/183
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 183/200 [10:06:48<56:21, 198.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1392.7739990234375
INFO:root:current train perplexity2.921769618988037
INFO:root:current mean train loss 1380.6958906693892
INFO:root:current train perplexity2.95038104057312
INFO:root:current mean train loss 1381.2738816034225
INFO:root:current train perplexity2.952488660812378
INFO:root:current mean train loss 1381.468398358745
INFO:root:current train perplexity2.954402446746826
INFO:root:current mean train loss 1380.2436943240282
INFO:root:current train perplexity2.9534406661987305
INFO:root:current mean train loss 1379.2428801891851
INFO:root:current train perplexity2.959057092666626
INFO:root:current mean train loss 1378.7530353483608
INFO:root:current train perplexity2.963085174560547
INFO:root:current mean train loss 1378.9771118164062
INFO:root:current train perplexity2.9618914127349854
INFO:root:current mean train loss 1377.2847633644387
INFO:root:current train perplexity2.96104097366333
INFO:root:current mean train loss 1377.941295850146
INFO:root:current train perplexity2.9598305225372314
INFO:root:current mean train loss 1378.877502924853
INFO:root:current train perplexity2.961045503616333
INFO:root:current mean train loss 1379.4482384484093
INFO:root:current train perplexity2.9618566036224365
INFO:root:current mean train loss 1380.1315844323024
INFO:root:current train perplexity2.9632935523986816
INFO:root:current mean train loss 1379.7351259653806
INFO:root:current train perplexity2.9642527103424072
INFO:root:current mean train loss 1379.951852958084
INFO:root:current train perplexity2.964507579803467
INFO:root:current mean train loss 1379.5999539204781
INFO:root:current train perplexity2.9648308753967285
INFO:root:current mean train loss 1379.758667522928
INFO:root:current train perplexity2.9650425910949707
INFO:root:current mean train loss 1379.2986936334978
INFO:root:current train perplexity2.9648892879486084
INFO:root:current mean train loss 1379.0339870052444
INFO:root:current train perplexity2.9657249450683594
INFO:root:current mean train loss 1379.0392971817735
INFO:root:current train perplexity2.966254949569702

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.32s/it]
INFO:root:final mean train loss: 1378.921671980388
INFO:root:final train perplexity: 2.9668490886688232
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it]
INFO:root:eval mean loss: 2309.3847504744294
INFO:root:eval perplexity: 6.473357200622559
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/184
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 184/200 [10:10:07<53:02, 198.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1376.3823151765046
INFO:root:current train perplexity2.956629753112793
INFO:root:current mean train loss 1370.6178862804502
INFO:root:current train perplexity2.953667163848877
INFO:root:current mean train loss 1373.2146324124105
INFO:root:current train perplexity2.9491446018218994
INFO:root:current mean train loss 1371.7510291983228
INFO:root:current train perplexity2.9572579860687256
INFO:root:current mean train loss 1373.5323806512552
INFO:root:current train perplexity2.9567906856536865
INFO:root:current mean train loss 1375.277861448574
INFO:root:current train perplexity2.9586331844329834
INFO:root:current mean train loss 1375.0864783474133
INFO:root:current train perplexity2.961252450942993
INFO:root:current mean train loss 1375.7002623084486
INFO:root:current train perplexity2.9606125354766846
INFO:root:current mean train loss 1376.194503765729
INFO:root:current train perplexity2.9623024463653564
INFO:root:current mean train loss 1376.6209726014697
INFO:root:current train perplexity2.961028575897217
INFO:root:current mean train loss 1376.4979250424096
INFO:root:current train perplexity2.961311101913452
INFO:root:current mean train loss 1376.702198153803
INFO:root:current train perplexity2.9613029956817627
INFO:root:current mean train loss 1376.1057952657013
INFO:root:current train perplexity2.9607906341552734
INFO:root:current mean train loss 1376.8927274242594
INFO:root:current train perplexity2.961594581604004
INFO:root:current mean train loss 1377.0897814744656
INFO:root:current train perplexity2.9612276554107666
INFO:root:current mean train loss 1376.8541617902495
INFO:root:current train perplexity2.961627244949341
INFO:root:current mean train loss 1376.578454897458
INFO:root:current train perplexity2.961118698120117
INFO:root:current mean train loss 1376.995383664094
INFO:root:current train perplexity2.9616830348968506
INFO:root:current mean train loss 1377.4299839564733
INFO:root:current train perplexity2.9629735946655273
INFO:root:current mean train loss 1377.741588804347
INFO:root:current train perplexity2.9635274410247803

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.31s/it]
INFO:root:final mean train loss: 1377.6645313595739
INFO:root:final train perplexity: 2.963909387588501
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.33s/it]
INFO:root:eval mean loss: 2309.9852450756316
INFO:root:eval perplexity: 6.476500988006592
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/185
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 185/200 [10:13:26<49:43, 198.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1382.5375921075995
INFO:root:current train perplexity2.9488441944122314
INFO:root:current mean train loss 1379.452589246962
INFO:root:current train perplexity2.969740629196167
INFO:root:current mean train loss 1378.9396227226882
INFO:root:current train perplexity2.9645631313323975
INFO:root:current mean train loss 1377.2517086295195
INFO:root:current train perplexity2.958430051803589
INFO:root:current mean train loss 1375.3018581630947
INFO:root:current train perplexity2.9588942527770996
INFO:root:current mean train loss 1374.844523710363
INFO:root:current train perplexity2.959505081176758
INFO:root:current mean train loss 1373.8208929026348
INFO:root:current train perplexity2.9586257934570312
INFO:root:current mean train loss 1374.6097233269804
INFO:root:current train perplexity2.960648536682129
INFO:root:current mean train loss 1375.3941933871445
INFO:root:current train perplexity2.9616029262542725
INFO:root:current mean train loss 1377.0530791201834
INFO:root:current train perplexity2.9630744457244873
INFO:root:current mean train loss 1377.259857879288
INFO:root:current train perplexity2.963693380355835
INFO:root:current mean train loss 1377.5451296292817
INFO:root:current train perplexity2.963438034057617
INFO:root:current mean train loss 1377.897752473592
INFO:root:current train perplexity2.9641947746276855
INFO:root:current mean train loss 1377.926172438122
INFO:root:current train perplexity2.9635956287384033
INFO:root:current mean train loss 1377.5643114422828
INFO:root:current train perplexity2.9631731510162354
INFO:root:current mean train loss 1377.7145315377822
INFO:root:current train perplexity2.9638004302978516
INFO:root:current mean train loss 1377.6285007597467
INFO:root:current train perplexity2.9626269340515137
INFO:root:current mean train loss 1377.1688702084602
INFO:root:current train perplexity2.9624533653259277
INFO:root:current mean train loss 1377.3264013195244
INFO:root:current train perplexity2.9615864753723145
INFO:root:current mean train loss 1377.5192906258037
INFO:root:current train perplexity2.962364912033081

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.55s/it]
INFO:root:final mean train loss: 1377.222161720572
INFO:root:final train perplexity: 2.9628753662109375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2309.9029696850066
INFO:root:eval perplexity: 6.476070404052734
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/186
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 186/200 [10:16:45<46:25, 198.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1370.5839703669315
INFO:root:current train perplexity2.9574360847473145
INFO:root:current mean train loss 1377.9306071974477
INFO:root:current train perplexity2.9439945220947266
INFO:root:current mean train loss 1376.1311184821002
INFO:root:current train perplexity2.953216552734375
INFO:root:current mean train loss 1375.7531001125346
INFO:root:current train perplexity2.957505941390991
INFO:root:current mean train loss 1377.3381464165875
INFO:root:current train perplexity2.95839786529541
INFO:root:current mean train loss 1376.4383982286097
INFO:root:current train perplexity2.959550380706787
INFO:root:current mean train loss 1379.0827525913626
INFO:root:current train perplexity2.9608397483825684
INFO:root:current mean train loss 1378.3535271743594
INFO:root:current train perplexity2.961315155029297
INFO:root:current mean train loss 1376.812232182555
INFO:root:current train perplexity2.961980104446411
INFO:root:current mean train loss 1376.408896296379
INFO:root:current train perplexity2.9597296714782715
INFO:root:current mean train loss 1377.4106118564443
INFO:root:current train perplexity2.9597556591033936
INFO:root:current mean train loss 1377.0831709934862
INFO:root:current train perplexity2.959667444229126
INFO:root:current mean train loss 1376.9357049565388
INFO:root:current train perplexity2.959465980529785
INFO:root:current mean train loss 1377.1308250231045
INFO:root:current train perplexity2.959167242050171
INFO:root:current mean train loss 1377.763049826077
INFO:root:current train perplexity2.9604480266571045
INFO:root:current mean train loss 1377.6582175138133
INFO:root:current train perplexity2.9616687297821045
INFO:root:current mean train loss 1376.991079827664
INFO:root:current train perplexity2.961702585220337
INFO:root:current mean train loss 1377.2837104689054
INFO:root:current train perplexity2.9616665840148926
INFO:root:current mean train loss 1377.5840522647225
INFO:root:current train perplexity2.961787223815918
INFO:root:current mean train loss 1377.229009197416
INFO:root:current train perplexity2.9622340202331543

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.80s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.80s/it]
INFO:root:final mean train loss: 1376.911743071725
INFO:root:final train perplexity: 2.9621503353118896
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it]
INFO:root:eval mean loss: 2310.122994497313
INFO:root:eval perplexity: 6.477222919464111
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/187
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 187/200 [10:20:03<43:04, 198.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1379.093751565004
INFO:root:current train perplexity2.945420742034912
INFO:root:current mean train loss 1376.843558665072
INFO:root:current train perplexity2.9498531818389893
INFO:root:current mean train loss 1378.7620586148269
INFO:root:current train perplexity2.9480233192443848
INFO:root:current mean train loss 1380.3158746951472
INFO:root:current train perplexity2.9497182369232178
INFO:root:current mean train loss 1381.6886747352248
INFO:root:current train perplexity2.952542304992676
INFO:root:current mean train loss 1379.1788458906656
INFO:root:current train perplexity2.9543116092681885
INFO:root:current mean train loss 1378.8626285879309
INFO:root:current train perplexity2.9573817253112793
INFO:root:current mean train loss 1377.8057608714753
INFO:root:current train perplexity2.956695795059204
INFO:root:current mean train loss 1377.585337019727
INFO:root:current train perplexity2.956815719604492
INFO:root:current mean train loss 1376.7993974120095
INFO:root:current train perplexity2.956483840942383
INFO:root:current mean train loss 1377.5072226444734
INFO:root:current train perplexity2.958937883377075
INFO:root:current mean train loss 1376.7490345253807
INFO:root:current train perplexity2.95888614654541
INFO:root:current mean train loss 1375.871700376412
INFO:root:current train perplexity2.9580559730529785
INFO:root:current mean train loss 1376.1572013157372
INFO:root:current train perplexity2.9589180946350098
INFO:root:current mean train loss 1376.4399413236586
INFO:root:current train perplexity2.9597909450531006
INFO:root:current mean train loss 1376.502688099676
INFO:root:current train perplexity2.9590952396392822
INFO:root:current mean train loss 1376.203094591543
INFO:root:current train perplexity2.958303451538086
INFO:root:current mean train loss 1376.5309534748708
INFO:root:current train perplexity2.958246946334839
INFO:root:current mean train loss 1376.4446743283663
INFO:root:current train perplexity2.95904278755188
INFO:root:current mean train loss 1376.4556404630625
INFO:root:current train perplexity2.9603207111358643

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.26s/it]
INFO:root:final mean train loss: 1376.1433584393124
INFO:root:final train perplexity: 2.9603559970855713
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it]
INFO:root:eval mean loss: 2311.187160627216
INFO:root:eval perplexity: 6.482800006866455
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/188
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/200 [10:23:22<39:45, 198.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1374.7771767064144
INFO:root:current train perplexity2.955212354660034
INFO:root:current mean train loss 1369.570294971955
INFO:root:current train perplexity2.9430410861968994
INFO:root:current mean train loss 1371.3644622285487
INFO:root:current train perplexity2.95186710357666
INFO:root:current mean train loss 1371.6996835443038
INFO:root:current train perplexity2.9542224407196045
INFO:root:current mean train loss 1370.2162511343909
INFO:root:current train perplexity2.9515833854675293
INFO:root:current mean train loss 1371.0326955586922
INFO:root:current train perplexity2.9553422927856445
INFO:root:current mean train loss 1371.4224233503821
INFO:root:current train perplexity2.952219247817993
INFO:root:current mean train loss 1371.091612464377
INFO:root:current train perplexity2.9512887001037598
INFO:root:current mean train loss 1371.1042106756286
INFO:root:current train perplexity2.9519095420837402
INFO:root:current mean train loss 1372.0990527589117
INFO:root:current train perplexity2.9523985385894775
INFO:root:current mean train loss 1372.251858032784
INFO:root:current train perplexity2.952793836593628
INFO:root:current mean train loss 1373.60490119966
INFO:root:current train perplexity2.955927610397339
INFO:root:current mean train loss 1373.3976542704813
INFO:root:current train perplexity2.9567997455596924
INFO:root:current mean train loss 1374.104818670895
INFO:root:current train perplexity2.958484411239624
INFO:root:current mean train loss 1373.7276214497545
INFO:root:current train perplexity2.957841396331787
INFO:root:current mean train loss 1373.7192522868095
INFO:root:current train perplexity2.9568095207214355
INFO:root:current mean train loss 1373.9546254926024
INFO:root:current train perplexity2.956312417984009
INFO:root:current mean train loss 1374.8812704697293
INFO:root:current train perplexity2.9562644958496094
INFO:root:current mean train loss 1375.1491757838266
INFO:root:current train perplexity2.9569764137268066

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.15s/it]
INFO:root:final mean train loss: 1375.1759312370482
INFO:root:final train perplexity: 2.9580981731414795
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2311.824412677305
INFO:root:eval perplexity: 6.486141204833984
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/189
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 189/200 [10:26:41<36:26, 198.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1373.3019409179688
INFO:root:current train perplexity2.9488375186920166
INFO:root:current mean train loss 1365.9473233904157
INFO:root:current train perplexity2.943485736846924
INFO:root:current mean train loss 1369.0429595371463
INFO:root:current train perplexity2.9558165073394775
INFO:root:current mean train loss 1371.7682444254558
INFO:root:current train perplexity2.95253849029541
INFO:root:current mean train loss 1372.0011430758875
INFO:root:current train perplexity2.9487404823303223
INFO:root:current mean train loss 1371.4666624069214
INFO:root:current train perplexity2.949542284011841
INFO:root:current mean train loss 1371.7348586936403
INFO:root:current train perplexity2.9500632286071777
INFO:root:current mean train loss 1372.4843734569763
INFO:root:current train perplexity2.950071096420288
INFO:root:current mean train loss 1373.1833874932652
INFO:root:current train perplexity2.9515604972839355
INFO:root:current mean train loss 1372.8382530881647
INFO:root:current train perplexity2.952355146408081
INFO:root:current mean train loss 1373.5618898896832
INFO:root:current train perplexity2.951517343521118
INFO:root:current mean train loss 1373.7553330016651
INFO:root:current train perplexity2.9524524211883545
INFO:root:current mean train loss 1374.9778929858319
INFO:root:current train perplexity2.952206611633301
INFO:root:current mean train loss 1375.0412802347323
INFO:root:current train perplexity2.9540016651153564
INFO:root:current mean train loss 1374.534252998849
INFO:root:current train perplexity2.9543752670288086
INFO:root:current mean train loss 1374.8420029897538
INFO:root:current train perplexity2.9549763202667236
INFO:root:current mean train loss 1375.3026765960617
INFO:root:current train perplexity2.955206871032715
INFO:root:current mean train loss 1374.9825750333127
INFO:root:current train perplexity2.9568424224853516
INFO:root:current mean train loss 1374.597214519846
INFO:root:current train perplexity2.955892562866211
INFO:root:current mean train loss 1374.7430041325142
INFO:root:current train perplexity2.955641984939575

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.28s/it]
INFO:root:final mean train loss: 1374.1995878541825
INFO:root:final train perplexity: 2.9558207988739014
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.36s/it]
INFO:root:eval mean loss: 2311.969378099374
INFO:root:eval perplexity: 6.486902236938477
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/190
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/200 [10:30:00<33:08, 198.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1363.6992818898168
INFO:root:current train perplexity2.9624485969543457
INFO:root:current mean train loss 1374.5589523906856
INFO:root:current train perplexity2.9499895572662354
INFO:root:current mean train loss 1375.954601571029
INFO:root:current train perplexity2.94480299949646
INFO:root:current mean train loss 1374.3626245191394
INFO:root:current train perplexity2.9453980922698975
INFO:root:current mean train loss 1374.5858216897036
INFO:root:current train perplexity2.9526727199554443
INFO:root:current mean train loss 1373.4443200152853
INFO:root:current train perplexity2.9528448581695557
INFO:root:current mean train loss 1373.1647328193312
INFO:root:current train perplexity2.9553894996643066
INFO:root:current mean train loss 1373.2834717131773
INFO:root:current train perplexity2.9520211219787598
INFO:root:current mean train loss 1373.4027102554376
INFO:root:current train perplexity2.953458309173584
INFO:root:current mean train loss 1373.4353965537541
INFO:root:current train perplexity2.9553778171539307
INFO:root:current mean train loss 1373.1942399657967
INFO:root:current train perplexity2.9555656909942627
INFO:root:current mean train loss 1373.1673751574265
INFO:root:current train perplexity2.9555022716522217
INFO:root:current mean train loss 1372.363295453462
INFO:root:current train perplexity2.9547855854034424
INFO:root:current mean train loss 1372.6385263826185
INFO:root:current train perplexity2.954233407974243
INFO:root:current mean train loss 1373.4733695369905
INFO:root:current train perplexity2.9540576934814453
INFO:root:current mean train loss 1373.5854994360336
INFO:root:current train perplexity2.95296311378479
INFO:root:current mean train loss 1373.8128231978255
INFO:root:current train perplexity2.9532907009124756
INFO:root:current mean train loss 1373.9910371867545
INFO:root:current train perplexity2.9545066356658936
INFO:root:current mean train loss 1374.262767593609
INFO:root:current train perplexity2.954846143722534
INFO:root:current mean train loss 1374.2102130516134
INFO:root:current train perplexity2.954690456390381

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.92s/it]
INFO:root:final mean train loss: 1373.8778314710685
INFO:root:final train perplexity: 2.955070972442627
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2311.924893426557
INFO:root:eval perplexity: 6.486668586730957
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/191
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/200 [10:33:18<29:48, 198.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1381.3786116890285
INFO:root:current train perplexity2.963249683380127
INFO:root:current mean train loss 1365.6800478582513
INFO:root:current train perplexity2.9479007720947266
INFO:root:current mean train loss 1367.0636869601117
INFO:root:current train perplexity2.9495601654052734
INFO:root:current mean train loss 1367.7342165908371
INFO:root:current train perplexity2.9548799991607666
INFO:root:current mean train loss 1371.0334292014084
INFO:root:current train perplexity2.95320725440979
INFO:root:current mean train loss 1371.2703155405793
INFO:root:current train perplexity2.9565846920013428
INFO:root:current mean train loss 1372.58494449769
INFO:root:current train perplexity2.9540185928344727
INFO:root:current mean train loss 1373.0755965409267
INFO:root:current train perplexity2.951425552368164
INFO:root:current mean train loss 1371.9549231563053
INFO:root:current train perplexity2.950495958328247
INFO:root:current mean train loss 1372.7532831236374
INFO:root:current train perplexity2.948920488357544
INFO:root:current mean train loss 1372.746540718735
INFO:root:current train perplexity2.949079751968384
INFO:root:current mean train loss 1373.7690858957446
INFO:root:current train perplexity2.9501936435699463
INFO:root:current mean train loss 1374.4311478371415
INFO:root:current train perplexity2.9506638050079346
INFO:root:current mean train loss 1374.0299086776213
INFO:root:current train perplexity2.950756072998047
INFO:root:current mean train loss 1374.2346165236265
INFO:root:current train perplexity2.951032876968384
INFO:root:current mean train loss 1374.1601796218063
INFO:root:current train perplexity2.9508724212646484
INFO:root:current mean train loss 1374.2001293085036
INFO:root:current train perplexity2.950284004211426
INFO:root:current mean train loss 1373.9027411427012
INFO:root:current train perplexity2.9499945640563965
INFO:root:current mean train loss 1373.9315754899742
INFO:root:current train perplexity2.9508583545684814
INFO:root:current mean train loss 1373.6014733623258
INFO:root:current train perplexity2.952704429626465

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.17s/it]
INFO:root:final mean train loss: 1372.8603659671662
INFO:root:final train perplexity: 2.9527008533477783
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.32s/it]
INFO:root:eval mean loss: 2313.3134159602173
INFO:root:eval perplexity: 6.493956565856934
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/192
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/200 [10:36:37<26:29, 198.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1385.6404021732392
INFO:root:current train perplexity2.962178945541382
INFO:root:current mean train loss 1377.7351418711658
INFO:root:current train perplexity2.963181257247925
INFO:root:current mean train loss 1378.5978637231167
INFO:root:current train perplexity2.9567432403564453
INFO:root:current mean train loss 1377.2051547972624
INFO:root:current train perplexity2.958317279815674
INFO:root:current mean train loss 1376.6560527892143
INFO:root:current train perplexity2.956153631210327
INFO:root:current mean train loss 1375.2923518938028
INFO:root:current train perplexity2.9548826217651367
INFO:root:current mean train loss 1375.0631331822092
INFO:root:current train perplexity2.954882860183716
INFO:root:current mean train loss 1377.6021234154857
INFO:root:current train perplexity2.956634044647217
INFO:root:current mean train loss 1377.4253156005577
INFO:root:current train perplexity2.955726385116577
INFO:root:current mean train loss 1376.3076998353129
INFO:root:current train perplexity2.955887794494629
INFO:root:current mean train loss 1375.0291441435647
INFO:root:current train perplexity2.9571645259857178
INFO:root:current mean train loss 1375.2954549748429
INFO:root:current train perplexity2.957726240158081
INFO:root:current mean train loss 1375.3648866244187
INFO:root:current train perplexity2.9566891193389893
INFO:root:current mean train loss 1374.8801014285182
INFO:root:current train perplexity2.9561386108398438
INFO:root:current mean train loss 1374.9348796184797
INFO:root:current train perplexity2.955300807952881
INFO:root:current mean train loss 1374.9274197100679
INFO:root:current train perplexity2.9546291828155518
INFO:root:current mean train loss 1375.1282710879952
INFO:root:current train perplexity2.9548096656799316
INFO:root:current mean train loss 1374.7612168976887
INFO:root:current train perplexity2.9546003341674805
INFO:root:current mean train loss 1374.1521840691887
INFO:root:current train perplexity2.954010248184204
INFO:root:current mean train loss 1373.8828195269716
INFO:root:current train perplexity2.954432725906372

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.92s/it]
INFO:root:final mean train loss: 1373.7370132392425
INFO:root:final train perplexity: 2.954742670059204
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2312.474617166722
INFO:root:eval perplexity: 6.489554405212402
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/193
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193/200 [10:39:56<23:10, 198.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1371.0673431396485
INFO:root:current train perplexity2.9353137016296387
INFO:root:current mean train loss 1369.3271830240885
INFO:root:current train perplexity2.9409685134887695
INFO:root:current mean train loss 1373.062075805664
INFO:root:current train perplexity2.939680337905884
INFO:root:current mean train loss 1373.8948563425165
INFO:root:current train perplexity2.9438605308532715
INFO:root:current mean train loss 1375.0796638488769
INFO:root:current train perplexity2.9502077102661133
INFO:root:current mean train loss 1374.6694167564656
INFO:root:current train perplexity2.950296640396118
INFO:root:current mean train loss 1373.920557538201
INFO:root:current train perplexity2.9494924545288086
INFO:root:current mean train loss 1372.942145714393
INFO:root:current train perplexity2.9485366344451904
INFO:root:current mean train loss 1373.27163030451
INFO:root:current train perplexity2.9484894275665283
INFO:root:current mean train loss 1372.9580132932078
INFO:root:current train perplexity2.9482908248901367
INFO:root:current mean train loss 1373.1816279658565
INFO:root:current train perplexity2.9485466480255127
INFO:root:current mean train loss 1372.1424354682536
INFO:root:current train perplexity2.9497568607330322
INFO:root:current mean train loss 1372.6369668006896
INFO:root:current train perplexity2.948812484741211
INFO:root:current mean train loss 1372.8624954002491
INFO:root:current train perplexity2.9502387046813965
INFO:root:current mean train loss 1372.8997437348237
INFO:root:current train perplexity2.949598550796509
INFO:root:current mean train loss 1372.8554142046578
INFO:root:current train perplexity2.9499030113220215
INFO:root:current mean train loss 1373.2059087844123
INFO:root:current train perplexity2.9501280784606934
INFO:root:current mean train loss 1372.9381958007812
INFO:root:current train perplexity2.9504587650299072
INFO:root:current mean train loss 1372.9715927448678
INFO:root:current train perplexity2.9508183002471924
INFO:root:current mean train loss 1372.349553580236
INFO:root:current train perplexity2.950509548187256

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.37s/it]
INFO:root:final mean train loss: 1371.9392771910852
INFO:root:final train perplexity: 2.950556516647339
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2313.732548274047
INFO:root:eval perplexity: 6.496158599853516
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/194
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/200 [10:43:15<19:52, 198.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1372.5463514819587
INFO:root:current train perplexity2.9392499923706055
INFO:root:current mean train loss 1373.9002443884835
INFO:root:current train perplexity2.938260316848755
INFO:root:current mean train loss 1368.8661833997528
INFO:root:current train perplexity2.944427728652954
INFO:root:current mean train loss 1370.9533162537389
INFO:root:current train perplexity2.9426510334014893
INFO:root:current mean train loss 1370.0718547122578
INFO:root:current train perplexity2.941279172897339
INFO:root:current mean train loss 1369.774422854834
INFO:root:current train perplexity2.942584753036499
INFO:root:current mean train loss 1371.1298493613813
INFO:root:current train perplexity2.9435462951660156
INFO:root:current mean train loss 1371.685996406201
INFO:root:current train perplexity2.945693254470825
INFO:root:current mean train loss 1371.4766826650903
INFO:root:current train perplexity2.9472219944000244
INFO:root:current mean train loss 1372.1744001535858
INFO:root:current train perplexity2.947326898574829
INFO:root:current mean train loss 1371.8945953452599
INFO:root:current train perplexity2.948049306869507
INFO:root:current mean train loss 1371.5583722489819
INFO:root:current train perplexity2.948516845703125
INFO:root:current mean train loss 1371.124015625753
INFO:root:current train perplexity2.9491169452667236
INFO:root:current mean train loss 1371.5524220777224
INFO:root:current train perplexity2.9487035274505615
INFO:root:current mean train loss 1371.5475059004727
INFO:root:current train perplexity2.9484336376190186
INFO:root:current mean train loss 1371.6605789480764
INFO:root:current train perplexity2.94789981842041
INFO:root:current mean train loss 1372.047194310617
INFO:root:current train perplexity2.9491806030273438
INFO:root:current mean train loss 1371.7289368592837
INFO:root:current train perplexity2.94985032081604
INFO:root:current mean train loss 1372.1891239148163
INFO:root:current train perplexity2.9503462314605713

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.34s/it]
INFO:root:final mean train loss: 1371.5584172312804
INFO:root:final train perplexity: 2.9496707916259766
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it]
INFO:root:eval mean loss: 2313.0815096374945
INFO:root:eval perplexity: 6.492738246917725
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/195
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 195/200 [10:46:34<16:34, 198.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1360.563720703125
INFO:root:current train perplexity2.9457509517669678
INFO:root:current mean train loss 1369.7082573070861
INFO:root:current train perplexity2.9321157932281494
INFO:root:current mean train loss 1369.5742626724957
INFO:root:current train perplexity2.939356803894043
INFO:root:current mean train loss 1367.1725386115397
INFO:root:current train perplexity2.9426233768463135
INFO:root:current mean train loss 1367.806613203408
INFO:root:current train perplexity2.945509910583496
INFO:root:current mean train loss 1368.5929686550037
INFO:root:current train perplexity2.944605588912964
INFO:root:current mean train loss 1369.1710650416073
INFO:root:current train perplexity2.946814775466919
INFO:root:current mean train loss 1369.8082907967876
INFO:root:current train perplexity2.948075771331787
INFO:root:current mean train loss 1371.5263994296588
INFO:root:current train perplexity2.9486329555511475
INFO:root:current mean train loss 1371.4181458454425
INFO:root:current train perplexity2.949359893798828
INFO:root:current mean train loss 1371.3232567540758
INFO:root:current train perplexity2.950026512145996
INFO:root:current mean train loss 1371.3477528981289
INFO:root:current train perplexity2.950514078140259
INFO:root:current mean train loss 1371.3546074202661
INFO:root:current train perplexity2.9493138790130615
INFO:root:current mean train loss 1371.3023104732983
INFO:root:current train perplexity2.9493348598480225
INFO:root:current mean train loss 1371.7478601436803
INFO:root:current train perplexity2.9497947692871094
INFO:root:current mean train loss 1371.9667910698067
INFO:root:current train perplexity2.9480185508728027
INFO:root:current mean train loss 1371.7641958546315
INFO:root:current train perplexity2.949631690979004
INFO:root:current mean train loss 1371.2242796284731
INFO:root:current train perplexity2.949613094329834
INFO:root:current mean train loss 1371.0400787656336
INFO:root:current train perplexity2.948232412338257
INFO:root:current mean train loss 1371.0238353446234
INFO:root:current train perplexity2.9486308097839355

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.48s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.48s/it]
INFO:root:final mean train loss: 1370.7167110933658
INFO:root:final train perplexity: 2.9477131366729736
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2313.7386232200242
INFO:root:eval perplexity: 6.496191501617432
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/196
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 196/200 [10:49:53<13:15, 198.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1385.3624661353326
INFO:root:current train perplexity2.962509870529175
INFO:root:current mean train loss 1377.2948091230319
INFO:root:current train perplexity2.947190999984741
INFO:root:current mean train loss 1372.599770550088
INFO:root:current train perplexity2.9549548625946045
INFO:root:current mean train loss 1373.5966033474556
INFO:root:current train perplexity2.9493205547332764
INFO:root:current mean train loss 1373.8787462274327
INFO:root:current train perplexity2.946028232574463
INFO:root:current mean train loss 1375.7900114759886
INFO:root:current train perplexity2.9520022869110107
INFO:root:current mean train loss 1376.2838024496089
INFO:root:current train perplexity2.9523115158081055
INFO:root:current mean train loss 1373.3867588278044
INFO:root:current train perplexity2.952324628829956
INFO:root:current mean train loss 1372.7780799911627
INFO:root:current train perplexity2.953367233276367
INFO:root:current mean train loss 1371.2845759243253
INFO:root:current train perplexity2.9511513710021973
INFO:root:current mean train loss 1371.595036533478
INFO:root:current train perplexity2.9533045291900635
INFO:root:current mean train loss 1371.4924218188758
INFO:root:current train perplexity2.95432186126709
INFO:root:current mean train loss 1372.0256331790085
INFO:root:current train perplexity2.953888416290283
INFO:root:current mean train loss 1372.1701172241853
INFO:root:current train perplexity2.9527690410614014
INFO:root:current mean train loss 1371.76913270877
INFO:root:current train perplexity2.9524550437927246
INFO:root:current mean train loss 1371.6415951041752
INFO:root:current train perplexity2.9513792991638184
INFO:root:current mean train loss 1371.7744691475705
INFO:root:current train perplexity2.9501891136169434
INFO:root:current mean train loss 1372.2207539700003
INFO:root:current train perplexity2.9510974884033203
INFO:root:current mean train loss 1372.0745198256589
INFO:root:current train perplexity2.9501898288726807
INFO:root:current mean train loss 1371.7868682687483
INFO:root:current train perplexity2.949079751968384

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.01s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.01s/it]
INFO:root:final mean train loss: 1371.419725749929
INFO:root:final train perplexity: 2.949347972869873
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2313.544230143229
INFO:root:eval perplexity: 6.4951701164245605
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/197
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 197/200 [10:53:11<09:56, 198.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1376.175765991211
INFO:root:current train perplexity2.928755760192871
INFO:root:current mean train loss 1369.865763071421
INFO:root:current train perplexity2.9491164684295654
INFO:root:current mean train loss 1371.1389376732611
INFO:root:current train perplexity2.9540560245513916
INFO:root:current mean train loss 1372.9648213002874
INFO:root:current train perplexity2.9513487815856934
INFO:root:current mean train loss 1372.3313127245221
INFO:root:current train perplexity2.953258752822876
INFO:root:current mean train loss 1371.6597811288207
INFO:root:current train perplexity2.9508769512176514
INFO:root:current mean train loss 1369.849398954415
INFO:root:current train perplexity2.950124979019165
INFO:root:current mean train loss 1370.4034139867772
INFO:root:current train perplexity2.9491331577301025
INFO:root:current mean train loss 1370.3812346548405
INFO:root:current train perplexity2.9469075202941895
INFO:root:current mean train loss 1369.7677698578009
INFO:root:current train perplexity2.9486963748931885
INFO:root:current mean train loss 1370.0394897460938
INFO:root:current train perplexity2.947040319442749
INFO:root:current mean train loss 1370.6339869482592
INFO:root:current train perplexity2.946652889251709
INFO:root:current mean train loss 1370.3718956189277
INFO:root:current train perplexity2.948394775390625
INFO:root:current mean train loss 1370.8198767415847
INFO:root:current train perplexity2.948322057723999
INFO:root:current mean train loss 1370.0755347151783
INFO:root:current train perplexity2.947298765182495
INFO:root:current mean train loss 1370.0423074569505
INFO:root:current train perplexity2.9480624198913574
INFO:root:current mean train loss 1370.1337392121843
INFO:root:current train perplexity2.947594404220581
INFO:root:current mean train loss 1370.0955003262657
INFO:root:current train perplexity2.947845220565796
INFO:root:current mean train loss 1370.843481154669
INFO:root:current train perplexity2.9483892917633057
INFO:root:current mean train loss 1371.21850366612
INFO:root:current train perplexity2.9478368759155273

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.89s/it]
INFO:root:final mean train loss: 1370.9648470741538
INFO:root:final train perplexity: 2.9482898712158203
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it]
INFO:root:eval mean loss: 2313.648669087295
INFO:root:eval perplexity: 6.495718002319336
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/198
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 198/200 [10:56:30<06:37, 198.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1369.6068284254807
INFO:root:current train perplexity2.9587817192077637
INFO:root:current mean train loss 1370.5571977095171
INFO:root:current train perplexity2.943399667739868
INFO:root:current mean train loss 1368.2696560841687
INFO:root:current train perplexity2.9438068866729736
INFO:root:current mean train loss 1368.5984756260702
INFO:root:current train perplexity2.9449033737182617
INFO:root:current mean train loss 1368.792132108955
INFO:root:current train perplexity2.9463160037994385
INFO:root:current mean train loss 1370.4243334744883
INFO:root:current train perplexity2.9474997520446777
INFO:root:current mean train loss 1370.243532843339
INFO:root:current train perplexity2.94622540473938
INFO:root:current mean train loss 1371.3191170087828
INFO:root:current train perplexity2.949993848800659
INFO:root:current mean train loss 1371.3355626806358
INFO:root:current train perplexity2.950589179992676
INFO:root:current mean train loss 1371.591374499069
INFO:root:current train perplexity2.9494946002960205
INFO:root:current mean train loss 1370.6886635077392
INFO:root:current train perplexity2.949167490005493
INFO:root:current mean train loss 1371.0364663316457
INFO:root:current train perplexity2.9486656188964844
INFO:root:current mean train loss 1370.8086419991355
INFO:root:current train perplexity2.948871612548828
INFO:root:current mean train loss 1371.1431741679544
INFO:root:current train perplexity2.9488108158111572
INFO:root:current mean train loss 1371.757382379213
INFO:root:current train perplexity2.9490303993225098
INFO:root:current mean train loss 1371.9520823713308
INFO:root:current train perplexity2.948063850402832
INFO:root:current mean train loss 1371.8677042276652
INFO:root:current train perplexity2.9487531185150146
INFO:root:current mean train loss 1371.6013707147442
INFO:root:current train perplexity2.9482128620147705
INFO:root:current mean train loss 1371.900218548404
INFO:root:current train perplexity2.9484996795654297
INFO:root:current mean train loss 1371.358490626988
INFO:root:current train perplexity2.947636604309082

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.32s/it]
INFO:root:final mean train loss: 1370.7694950536593
INFO:root:final train perplexity: 2.94783616065979
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.29s/it]
INFO:root:eval mean loss: 2313.9032484555073
INFO:root:eval perplexity: 6.497056484222412
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/199
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/200 [10:59:49<03:18, 198.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1371.2856713271722
INFO:root:current train perplexity2.9082560539245605
INFO:root:current mean train loss 1368.2318752414578
INFO:root:current train perplexity2.9276273250579834
INFO:root:current mean train loss 1366.7759390756594
INFO:root:current train perplexity2.934626817703247
INFO:root:current mean train loss 1369.28126821468
INFO:root:current train perplexity2.939314365386963
INFO:root:current mean train loss 1368.5641626381775
INFO:root:current train perplexity2.9441232681274414
INFO:root:current mean train loss 1369.050308909203
INFO:root:current train perplexity2.9451797008514404
INFO:root:current mean train loss 1369.4409140309979
INFO:root:current train perplexity2.9458811283111572
INFO:root:current mean train loss 1369.6947592810902
INFO:root:current train perplexity2.9481470584869385
INFO:root:current mean train loss 1370.511405685321
INFO:root:current train perplexity2.948408603668213
INFO:root:current mean train loss 1370.2455912976548
INFO:root:current train perplexity2.9487552642822266
INFO:root:current mean train loss 1369.91814644015
INFO:root:current train perplexity2.9479451179504395
INFO:root:current mean train loss 1369.5554922139384
INFO:root:current train perplexity2.947125196456909
INFO:root:current mean train loss 1369.85989860737
INFO:root:current train perplexity2.9478983879089355
INFO:root:current mean train loss 1369.713160469287
INFO:root:current train perplexity2.946492910385132
INFO:root:current mean train loss 1370.1008612134679
INFO:root:current train perplexity2.9463140964508057
INFO:root:current mean train loss 1369.7505264764488
INFO:root:current train perplexity2.945795774459839
INFO:root:current mean train loss 1370.4461182946966
INFO:root:current train perplexity2.945363998413086
INFO:root:current mean train loss 1370.5934150259102
INFO:root:current train perplexity2.946399450302124
INFO:root:current mean train loss 1370.6984034344696
INFO:root:current train perplexity2.9474246501922607
INFO:root:current mean train loss 1370.5141477767684
INFO:root:current train perplexity2.9463820457458496

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.96s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.96s/it]
INFO:root:final mean train loss: 1370.1534400435933
INFO:root:final train perplexity: 2.946403741836548
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2313.749000062334
INFO:root:eval perplexity: 6.49624490737915
INFO:root:evalaution complete
INFO:root:checkpoint. save model: minilm_l12_baseline/200
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [11:03:07<00:00, 198.71s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [11:03:07<00:00, 198.94s/it]
INFO:root:evaluating final model
INFO:root:start evaluating on validation
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.28s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.28s/it]
INFO:root:eval mean loss: 2313.749000062334
INFO:root:eval perplexity: 6.49624490737915
INFO:root:evalaution complete
INFO:root:save model final: minilm_l12_baseline/final
