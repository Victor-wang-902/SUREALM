INFO:root:Output: large_mp_gpt2
INFO:root:Steps per epochs:1983
INFO:root:Total steps:396600
Using pad_token, but it is not set yet.
Some weights of RetrievalGenerationModelGPT2 were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.10.crossattention.c_attn_v.weight', 'h.5.crossattention.c_proj.weight', 'h.7.ln_cross_attn.weight', 'h.6.crossattention.c_attn.weight', 'h.1.crossattention.masked_bias', 'h.9.crossattention.q_attn.weight', 'h.10.ln_cross_attn.weight', 'h.9.crossattention.c_attn_v.bias', 'h.3.crossattention.c_attn.weight', 'h.7.crossattention.masked_bias', 'h.6.crossattention.c_attn_v.bias', 'h.3.crossattention.c_attn_v.bias', 'h.7.crossattention.c_attn.weight', 'h.8.crossattention.bias', 'h.4.crossattention.c_proj.weight', 'h.5.crossattention.c_attn.weight', 'h.10.crossattention.c_attn_v.bias', 'h.8.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.weight', 'h.1.crossattention.c_attn_v.bias', 'h.5.crossattention.bias', 'h.4.crossattention.masked_bias', 'h.2.crossattention.c_attn.weight', 'h.1.crossattention.c_attn_v.weight', 'h.0.crossattention.c_proj.weight', 'h.2.crossattention.bias', 'h.5.crossattention.masked_bias', 'h.9.crossattention.c_attn.weight', 'h.8.crossattention.c_attn.weight', 'h.3.crossattention.bias', 'h.8.crossattention.c_attn_v.weight', 'h.8.crossattention.c_attn_v.bias', 'h.0.crossattention.c_attn.weight', 'h.5.crossattention.c_attn_v.weight', 'h.11.crossattention.bias', 'h.6.crossattention.c_proj.bias', 'h.11.ln_cross_attn.weight', 'h.7.crossattention.bias', 'h.2.crossattention.masked_bias', 'h.1.crossattention.c_proj.weight', 'h.2.crossattention.c_proj.bias', 'h.9.crossattention.bias', 'h.0.crossattention.q_attn.weight', 'h.10.crossattention.c_proj.weight', 'h.9.ln_cross_attn.weight', 'h.1.crossattention.q_attn.weight', 'h.8.crossattention.q_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.2.crossattention.c_attn_v.bias', 'h.1.crossattention.bias', 'h.7.crossattention.q_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.q_attn.weight', 'h.10.crossattention.bias', 'h.7.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.weight', 'h.6.crossattention.q_attn.weight', 'h.7.crossattention.c_attn_v.weight', 'h.3.crossattention.q_attn.weight', 'h.4.crossattention.c_attn.weight', 'h.2.crossattention.c_attn_v.weight', 'h.1.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn_v.bias', 'h.8.crossattention.masked_bias', 'h.4.crossattention.bias', 'h.9.crossattention.c_proj.bias', 'h.11.crossattention.c_attn_v.weight', 'h.3.crossattention.c_proj.bias', 'h.2.crossattention.q_attn.weight', 'h.8.ln_cross_attn.weight', 'h.10.crossattention.masked_bias', 'h.1.crossattention.c_attn.weight', 'h.0.crossattention.masked_bias', 'h.9.crossattention.masked_bias', 'h.0.crossattention.c_proj.bias', 'h.4.crossattention.c_attn_v.weight', 'h.6.crossattention.bias', 'h.11.crossattention.c_proj.weight', 'h.6.crossattention.c_attn_v.weight', 'h.11.crossattention.c_attn_v.bias', 'h.5.crossattention.q_attn.weight', 'h.2.crossattention.c_proj.weight', 'h.4.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.6.crossattention.c_proj.weight', 'h.11.crossattention.c_attn.weight', 'h.3.ln_cross_attn.weight', 'h.0.ln_cross_attn.weight', 'h.5.ln_cross_attn.weight', 'h.4.ln_cross_attn.weight', 'h.6.crossattention.masked_bias', 'h.11.crossattention.c_proj.bias', 'h.9.crossattention.c_attn_v.weight', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn_v.weight', 'h.11.crossattention.masked_bias', 'h.5.crossattention.c_proj.bias', 'h.1.ln_cross_attn.weight', 'h.4.crossattention.c_attn_v.bias', 'h.0.crossattention.c_attn_v.bias', 'h.8.crossattention.c_proj.bias', 'h.0.crossattention.c_attn_v.weight', 'h.5.crossattention.c_attn_v.bias', 'h.3.crossattention.masked_bias', 'h.10.crossattention.c_attn.weight', 'h.0.crossattention.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:root:started training
  0%|          | 0/200 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4696.995504360007
INFO:root:current train perplexity61.5967903137207
INFO:root:current mean train loss 4419.911221144786
INFO:root:current train perplexity48.477577209472656
INFO:root:current mean train loss 4172.248337557483
INFO:root:current train perplexity38.87746047973633
INFO:root:current mean train loss 3968.1459061472037
INFO:root:current train perplexity32.64414596557617
INFO:root:current mean train loss 3809.2608234046215
INFO:root:current train perplexity28.412641525268555
INFO:root:current mean train loss 3685.1954967263146
INFO:root:current train perplexity25.417129516601562
INFO:root:current mean train loss 3580.1089467934103
INFO:root:current train perplexity23.16328239440918
INFO:root:current mean train loss 3494.032020311033
INFO:root:current train perplexity21.41790771484375
INFO:root:current mean train loss 3418.7934953224935
INFO:root:current train perplexity20.03339195251465
INFO:root:current mean train loss 3352.5994561455987
INFO:root:current train perplexity18.8956356048584
INFO:root:current mean train loss 3293.7606206725573
INFO:root:current train perplexity17.974428176879883
INFO:root:current mean train loss 3244.0438642716585
INFO:root:current train perplexity17.20723533630371
INFO:root:current mean train loss 3199.008185007097
INFO:root:current train perplexity16.538591384887695
INFO:root:current mean train loss 3155.4242715569717
INFO:root:current train perplexity15.946303367614746
INFO:root:current mean train loss 3115.5259080598003
INFO:root:current train perplexity15.423399925231934
INFO:root:current mean train loss 3083.0667847519444
INFO:root:current train perplexity14.983540534973145
INFO:root:current mean train loss 3052.4281951383396
INFO:root:current train perplexity14.586482048034668
INFO:root:current mean train loss 3024.716805085399
INFO:root:current train perplexity14.22638988494873
INFO:root:current mean train loss 2998.7317521274563
INFO:root:current train perplexity13.902200698852539

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:28<00:00, 508.48s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:28<00:00, 508.48s/it]
INFO:root:final mean train loss: 2977.487871825064
INFO:root:final train perplexity: 13.650907516479492
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:45<00:00, 45.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:45<00:00, 45.25s/it]
INFO:root:eval mean loss: 2295.643026149019
INFO:root:eval perplexity: 7.942051887512207
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.50s/it]
INFO:root:eval mean loss: 2502.1370715418607
INFO:root:eval perplexity: 9.960272789001465
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/1
  0%|          | 1/200 [09:59<33:07:00, 599.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2494.2684173583984
INFO:root:current train perplexity8.993522644042969
INFO:root:current mean train loss 2452.2347517342405
INFO:root:current train perplexity8.609987258911133
INFO:root:current mean train loss 2447.7914360894097
INFO:root:current train perplexity8.586482048034668
INFO:root:current mean train loss 2445.6871600573577
INFO:root:current train perplexity8.583581924438477
INFO:root:current mean train loss 2443.1874504089355
INFO:root:current train perplexity8.522760391235352
INFO:root:current mean train loss 2439.5779648418575
INFO:root:current train perplexity8.49374008178711
INFO:root:current mean train loss 2429.678825675667
INFO:root:current train perplexity8.439542770385742
INFO:root:current mean train loss 2421.1528652766565
INFO:root:current train perplexity8.39845085144043
INFO:root:current mean train loss 2413.863322239296
INFO:root:current train perplexity8.345736503601074
INFO:root:current mean train loss 2408.0236940342265
INFO:root:current train perplexity8.298027038574219
INFO:root:current mean train loss 2401.5562902735915
INFO:root:current train perplexity8.251916885375977
INFO:root:current mean train loss 2396.215528153177
INFO:root:current train perplexity8.20976448059082
INFO:root:current mean train loss 2391.57955099407
INFO:root:current train perplexity8.179235458374023
INFO:root:current mean train loss 2386.550554269715
INFO:root:current train perplexity8.141242980957031
INFO:root:current mean train loss 2382.4781216551355
INFO:root:current train perplexity8.103605270385742
INFO:root:current mean train loss 2378.728703481226
INFO:root:current train perplexity8.06933307647705
INFO:root:current mean train loss 2374.9663671361336
INFO:root:current train perplexity8.039347648620605
INFO:root:current mean train loss 2370.155240430143
INFO:root:current train perplexity8.00455379486084
INFO:root:current mean train loss 2365.0812603786653
INFO:root:current train perplexity7.9702229499816895
INFO:root:current mean train loss 2361.342499798673
INFO:root:current train perplexity7.94456148147583

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:16<00:00, 496.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:16<00:00, 496.97s/it]
INFO:root:final mean train loss: 2358.1364402597865
INFO:root:final train perplexity: 7.92565393447876
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:44<00:00, 44.23s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:44<00:00, 44.23s/it]
INFO:root:eval mean loss: 2110.436677540448
INFO:root:eval perplexity: 6.719370365142822
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.68s/it]
INFO:root:eval mean loss: 2356.5439669561724
INFO:root:eval perplexity: 8.713335990905762
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/2
  1%|          | 2/200 [19:43<32:29:01, 590.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2265.1514781605115
INFO:root:current train perplexity7.360304355621338
INFO:root:current mean train loss 2261.5282579006107
INFO:root:current train perplexity7.286500930786133
INFO:root:current mean train loss 2251.0298846985647
INFO:root:current train perplexity7.244363784790039
INFO:root:current mean train loss 2260.94683343011
INFO:root:current train perplexity7.229010105133057
INFO:root:current mean train loss 2253.7180649402785
INFO:root:current train perplexity7.2007269859313965
INFO:root:current mean train loss 2250.9297589557927
INFO:root:current train perplexity7.170629024505615
INFO:root:current mean train loss 2250.364980013638
INFO:root:current train perplexity7.1595354080200195
INFO:root:current mean train loss 2242.9485867488597
INFO:root:current train perplexity7.139986515045166
INFO:root:current mean train loss 2238.058256554575
INFO:root:current train perplexity7.128970623016357
INFO:root:current mean train loss 2233.6873335761657
INFO:root:current train perplexity7.118653774261475
INFO:root:current mean train loss 2232.9438592369766
INFO:root:current train perplexity7.104054927825928
INFO:root:current mean train loss 2229.0606506293784
INFO:root:current train perplexity7.08212947845459
INFO:root:current mean train loss 2227.5005806507565
INFO:root:current train perplexity7.072536468505859
INFO:root:current mean train loss 2224.5909607895137
INFO:root:current train perplexity7.053585052490234
INFO:root:current mean train loss 2220.8756526885795
INFO:root:current train perplexity7.029980182647705
INFO:root:current mean train loss 2217.922013394131
INFO:root:current train perplexity7.024350643157959
INFO:root:current mean train loss 2216.5616565711352
INFO:root:current train perplexity7.008979320526123
INFO:root:current mean train loss 2214.955389393731
INFO:root:current train perplexity6.99624490737915
INFO:root:current mean train loss 2213.4587524880235
INFO:root:current train perplexity6.9831156730651855
INFO:root:current mean train loss 2211.7488923997876
INFO:root:current train perplexity6.96822452545166

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:18<00:00, 498.93s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:18<00:00, 498.93s/it]
INFO:root:final mean train loss: 2210.4516326088647
INFO:root:final train perplexity: 6.961944103240967
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:44<00:00, 44.18s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:44<00:00, 44.18s/it]
INFO:root:eval mean loss: 2024.1467423675754
INFO:root:eval perplexity: 6.21586275100708
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.25s/it]
INFO:root:eval mean loss: 2293.842969096299
INFO:root:eval perplexity: 8.225621223449707
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/3
  2%|â–         | 3/200 [29:30<32:13:25, 588.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2150.71619140625
INFO:root:current train perplexity6.507720947265625
INFO:root:current mean train loss 2157.0630777994793
INFO:root:current train perplexity6.564268112182617
INFO:root:current mean train loss 2157.8232172851563
INFO:root:current train perplexity6.588978290557861
INFO:root:current mean train loss 2158.1216584123886
INFO:root:current train perplexity6.5874152183532715
INFO:root:current mean train loss 2157.4526312934026
INFO:root:current train perplexity6.582012176513672
INFO:root:current mean train loss 2153.168543368253
INFO:root:current train perplexity6.5747833251953125
INFO:root:current mean train loss 2150.368623046875
INFO:root:current train perplexity6.564370155334473
INFO:root:current mean train loss 2147.3783541666667
INFO:root:current train perplexity6.549909591674805
INFO:root:current mean train loss 2149.049683335248
INFO:root:current train perplexity6.548953533172607
INFO:root:current mean train loss 2145.70202161287
INFO:root:current train perplexity6.5328049659729
INFO:root:current mean train loss 2142.513558872768
INFO:root:current train perplexity6.524516582489014
INFO:root:current mean train loss 2140.304754904042
INFO:root:current train perplexity6.520002841949463
INFO:root:current mean train loss 2138.122396191406
INFO:root:current train perplexity6.510721683502197
INFO:root:current mean train loss 2135.3937169958044
INFO:root:current train perplexity6.5017313957214355
INFO:root:current mean train loss 2132.6617152983567
INFO:root:current train perplexity6.493592262268066
INFO:root:current mean train loss 2131.431090048513
INFO:root:current train perplexity6.493597507476807
INFO:root:current mean train loss 2129.8261315548057
INFO:root:current train perplexity6.4820427894592285
INFO:root:current mean train loss 2129.4974327566965
INFO:root:current train perplexity6.474381446838379
INFO:root:current mean train loss 2127.1594760214316
INFO:root:current train perplexity6.467867374420166
INFO:root:current mean train loss 2126.5284803185095
INFO:root:current train perplexity6.461480140686035

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:13<00:00, 493.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:13<00:00, 493.67s/it]
INFO:root:final mean train loss: 2124.880674361221
INFO:root:final train perplexity: 6.458130359649658
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.25s/it]
INFO:root:eval mean loss: 1972.930508660932
INFO:root:eval perplexity: 5.935042381286621
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.66s/it]
INFO:root:eval mean loss: 2258.659024285932
INFO:root:eval perplexity: 7.964005470275879
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/4
  2%|â–         | 4/200 [39:08<31:49:34, 584.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2070.0864713298743
INFO:root:current train perplexity6.2184295654296875
INFO:root:current mean train loss 2085.1129033437032
INFO:root:current train perplexity6.192614555358887
INFO:root:current mean train loss 2090.711424866866
INFO:root:current train perplexity6.197025775909424
INFO:root:current mean train loss 2084.0818616155057
INFO:root:current train perplexity6.197702884674072
INFO:root:current mean train loss 2086.64825687776
INFO:root:current train perplexity6.196426868438721
INFO:root:current mean train loss 2079.805028737117
INFO:root:current train perplexity6.183588981628418
INFO:root:current mean train loss 2079.5142150604383
INFO:root:current train perplexity6.171853542327881
INFO:root:current mean train loss 2078.4318381338107
INFO:root:current train perplexity6.171507835388184
INFO:root:current mean train loss 2073.903138544595
INFO:root:current train perplexity6.15798282623291
INFO:root:current mean train loss 2073.755578752141
INFO:root:current train perplexity6.159711837768555
INFO:root:current mean train loss 2073.382987311094
INFO:root:current train perplexity6.157903671264648
INFO:root:current mean train loss 2072.295445511662
INFO:root:current train perplexity6.159366130828857
INFO:root:current mean train loss 2070.212658623964
INFO:root:current train perplexity6.153773307800293
INFO:root:current mean train loss 2069.447199098019
INFO:root:current train perplexity6.150312900543213
INFO:root:current mean train loss 2069.7373828224854
INFO:root:current train perplexity6.146399021148682
INFO:root:current mean train loss 2070.1177797786127
INFO:root:current train perplexity6.147361755371094
INFO:root:current mean train loss 2069.0122779887383
INFO:root:current train perplexity6.142692565917969
INFO:root:current mean train loss 2069.060776369951
INFO:root:current train perplexity6.142155170440674
INFO:root:current mean train loss 2068.912819958908
INFO:root:current train perplexity6.139941692352295
INFO:root:current mean train loss 2066.777285724712
INFO:root:current train perplexity6.131962299346924

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:02<00:00, 482.18s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:02<00:00, 482.18s/it]
INFO:root:final mean train loss: 2065.7618230914927
INFO:root:final train perplexity: 6.131515026092529
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.71s/it]
INFO:root:eval mean loss: 1939.8513854547596
INFO:root:eval perplexity: 5.760447025299072
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.59s/it]
INFO:root:eval mean loss: 2242.474022831477
INFO:root:eval perplexity: 7.846468925476074
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/5
  2%|â–Ž         | 5/200 [48:32<31:15:33, 577.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2017.6725783575148
INFO:root:current train perplexity5.990253448486328
INFO:root:current mean train loss 2028.449138475501
INFO:root:current train perplexity6.0093674659729
INFO:root:current mean train loss 2038.074463750275
INFO:root:current train perplexity6.009058475494385
INFO:root:current mean train loss 2034.3282101949055
INFO:root:current train perplexity5.994359493255615
INFO:root:current mean train loss 2034.5757418545809
INFO:root:current train perplexity5.985190391540527
INFO:root:current mean train loss 2033.3195058744247
INFO:root:current train perplexity5.98075008392334
INFO:root:current mean train loss 2034.8204040527344
INFO:root:current train perplexity5.971357822418213
INFO:root:current mean train loss 2032.1656738592653
INFO:root:current train perplexity5.961940765380859
INFO:root:current mean train loss 2029.3773707048924
INFO:root:current train perplexity5.9427385330200195
INFO:root:current mean train loss 2028.3943285438104
INFO:root:current train perplexity5.939640522003174
INFO:root:current mean train loss 2029.016654714887
INFO:root:current train perplexity5.939664840698242
INFO:root:current mean train loss 2027.1184519174938
INFO:root:current train perplexity5.93040132522583
INFO:root:current mean train loss 2026.4879204580718
INFO:root:current train perplexity5.923512935638428
INFO:root:current mean train loss 2025.974910493531
INFO:root:current train perplexity5.923025131225586
INFO:root:current mean train loss 2025.7919902955746
INFO:root:current train perplexity5.92195987701416
INFO:root:current mean train loss 2023.8871540108112
INFO:root:current train perplexity5.912256240844727
INFO:root:current mean train loss 2022.8455241513648
INFO:root:current train perplexity5.906378269195557
INFO:root:current mean train loss 2021.5564482821478
INFO:root:current train perplexity5.900150299072266
INFO:root:current mean train loss 2021.3787020867544
INFO:root:current train perplexity5.8966240882873535

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:16<00:00, 496.83s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:16<00:00, 496.83s/it]
INFO:root:final mean train loss: 2020.880213165668
INFO:root:final train perplexity: 5.894633769989014
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.38s/it]
INFO:root:eval mean loss: 1913.2636558586823
INFO:root:eval perplexity: 5.623844623565674
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.52s/it]
INFO:root:eval mean loss: 2224.353374941129
INFO:root:eval perplexity: 7.716930866241455
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/6
  3%|â–Ž         | 6/200 [58:13<31:10:17, 578.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1869.3074951171875
INFO:root:current train perplexity5.080881595611572
INFO:root:current mean train loss 1975.1038915048732
INFO:root:current train perplexity5.771860122680664
INFO:root:current mean train loss 1977.0589253439832
INFO:root:current train perplexity5.72015905380249
INFO:root:current mean train loss 1990.81352279511
INFO:root:current train perplexity5.749399185180664
INFO:root:current mean train loss 1989.173719753351
INFO:root:current train perplexity5.747783184051514
INFO:root:current mean train loss 1990.7774268357816
INFO:root:current train perplexity5.744253158569336
INFO:root:current mean train loss 1990.1980463875311
INFO:root:current train perplexity5.745539665222168
INFO:root:current mean train loss 1989.2451957234643
INFO:root:current train perplexity5.735157489776611
INFO:root:current mean train loss 1990.3183653184983
INFO:root:current train perplexity5.734143257141113
INFO:root:current mean train loss 1991.8382882680269
INFO:root:current train perplexity5.734499454498291
INFO:root:current mean train loss 1991.3445918095576
INFO:root:current train perplexity5.727755069732666
INFO:root:current mean train loss 1989.284498445128
INFO:root:current train perplexity5.727624416351318
INFO:root:current mean train loss 1989.2464366852494
INFO:root:current train perplexity5.720849990844727
INFO:root:current mean train loss 1988.5546374896414
INFO:root:current train perplexity5.713545322418213
INFO:root:current mean train loss 1985.6385241882194
INFO:root:current train perplexity5.710510730743408
INFO:root:current mean train loss 1985.0768396427122
INFO:root:current train perplexity5.710626602172852
INFO:root:current mean train loss 1986.2400772313936
INFO:root:current train perplexity5.711943626403809
INFO:root:current mean train loss 1985.044019157504
INFO:root:current train perplexity5.709030628204346
INFO:root:current mean train loss 1985.103472110763
INFO:root:current train perplexity5.705935955047607
INFO:root:current mean train loss 1984.243038460432
INFO:root:current train perplexity5.70227575302124

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:17<00:00, 497.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:17<00:00, 497.68s/it]
INFO:root:final mean train loss: 1983.1457059997774
INFO:root:final train perplexity: 5.702568054199219
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.63s/it]
INFO:root:eval mean loss: 1892.628246117991
INFO:root:eval perplexity: 5.520061016082764
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.66s/it]
INFO:root:eval mean loss: 2215.575406987616
INFO:root:eval perplexity: 7.654951572418213
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/7
  4%|â–Ž         | 7/200 [1:07:54<31:03:47, 579.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1933.8565199110244
INFO:root:current train perplexity5.57506799697876
INFO:root:current mean train loss 1955.239501953125
INFO:root:current train perplexity5.592813968658447
INFO:root:current mean train loss 1949.3596443386252
INFO:root:current train perplexity5.548509120941162
INFO:root:current mean train loss 1959.0531032730198
INFO:root:current train perplexity5.557988166809082
INFO:root:current mean train loss 1959.0874143171538
INFO:root:current train perplexity5.5617475509643555
INFO:root:current mean train loss 1959.3698973195433
INFO:root:current train perplexity5.570232391357422
INFO:root:current mean train loss 1961.2056475889335
INFO:root:current train perplexity5.58042573928833
INFO:root:current mean train loss 1957.7055079213092
INFO:root:current train perplexity5.571704387664795
INFO:root:current mean train loss 1958.3740091113998
INFO:root:current train perplexity5.572318077087402
INFO:root:current mean train loss 1956.0601806640625
INFO:root:current train perplexity5.561148166656494
INFO:root:current mean train loss 1955.0579176867172
INFO:root:current train perplexity5.558779239654541
INFO:root:current mean train loss 1953.1698945784185
INFO:root:current train perplexity5.558471202850342
INFO:root:current mean train loss 1952.3573424992303
INFO:root:current train perplexity5.555041313171387
INFO:root:current mean train loss 1952.2523670341247
INFO:root:current train perplexity5.552559852600098
INFO:root:current mean train loss 1952.5331129066026
INFO:root:current train perplexity5.5537943840026855
INFO:root:current mean train loss 1952.435798896317
INFO:root:current train perplexity5.552426815032959
INFO:root:current mean train loss 1951.7008909171238
INFO:root:current train perplexity5.547665596008301
INFO:root:current mean train loss 1951.3792375025012
INFO:root:current train perplexity5.548297882080078
INFO:root:current mean train loss 1951.945932654789
INFO:root:current train perplexity5.550205707550049
INFO:root:current mean train loss 1952.0886174461516
INFO:root:current train perplexity5.549488544464111

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:15<00:00, 495.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:15<00:00, 495.28s/it]
INFO:root:final mean train loss: 1951.7542387269327
INFO:root:final train perplexity: 5.547567844390869
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.47s/it]
INFO:root:eval mean loss: 1876.9781736549755
INFO:root:eval perplexity: 5.442629337310791
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.26s/it]
INFO:root:eval mean loss: 2211.1798186606547
INFO:root:eval perplexity: 7.6241044998168945
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/8
  4%|â–         | 8/200 [1:17:33<30:53:03, 579.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1945.6485560825893
INFO:root:current train perplexity5.474288463592529
INFO:root:current mean train loss 1948.8832564742477
INFO:root:current train perplexity5.459931373596191
INFO:root:current mean train loss 1935.1590586560837
INFO:root:current train perplexity5.441092014312744
INFO:root:current mean train loss 1931.654595309585
INFO:root:current train perplexity5.420963287353516
INFO:root:current mean train loss 1936.7719751818427
INFO:root:current train perplexity5.4373345375061035
INFO:root:current mean train loss 1937.044378605067
INFO:root:current train perplexity5.442729949951172
INFO:root:current mean train loss 1933.3727737066315
INFO:root:current train perplexity5.434399604797363
INFO:root:current mean train loss 1929.2959889522215
INFO:root:current train perplexity5.425190448760986
INFO:root:current mean train loss 1930.295390566523
INFO:root:current train perplexity5.4293131828308105
INFO:root:current mean train loss 1929.922905482328
INFO:root:current train perplexity5.425452709197998
INFO:root:current mean train loss 1928.582299568803
INFO:root:current train perplexity5.42197847366333
INFO:root:current mean train loss 1925.8165007872728
INFO:root:current train perplexity5.4192728996276855
INFO:root:current mean train loss 1928.4771024758033
INFO:root:current train perplexity5.429311752319336
INFO:root:current mean train loss 1927.4954233233848
INFO:root:current train perplexity5.422842025756836
INFO:root:current mean train loss 1927.7325650417847
INFO:root:current train perplexity5.426253318786621
INFO:root:current mean train loss 1926.8059664946813
INFO:root:current train perplexity5.42342472076416
INFO:root:current mean train loss 1926.442596640864
INFO:root:current train perplexity5.425530910491943
INFO:root:current mean train loss 1926.8455135818174
INFO:root:current train perplexity5.425447463989258
INFO:root:current mean train loss 1927.3644910432988
INFO:root:current train perplexity5.425940990447998
INFO:root:current mean train loss 1927.3103962900718
INFO:root:current train perplexity5.4253411293029785

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:10<00:00, 490.48s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:10<00:00, 490.48s/it]
INFO:root:final mean train loss: 1925.951701738951
INFO:root:final train perplexity: 5.423323154449463
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.16s/it]
INFO:root:eval mean loss: 1862.6068305698693
INFO:root:eval perplexity: 5.372480869293213
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.48s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.48s/it]
INFO:root:eval mean loss: 2200.136679358516
INFO:root:eval perplexity: 7.5471510887146
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/9
  4%|â–         | 9/200 [1:27:07<30:38:50, 577.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1894.06885939378
INFO:root:current train perplexity5.296872138977051
INFO:root:current mean train loss 1910.4296521638569
INFO:root:current train perplexity5.315711975097656
INFO:root:current mean train loss 1916.0965750558037
INFO:root:current train perplexity5.326986789703369
INFO:root:current mean train loss 1914.9744526256216
INFO:root:current train perplexity5.331394672393799
INFO:root:current mean train loss 1916.5096467954922
INFO:root:current train perplexity5.346822738647461
INFO:root:current mean train loss 1910.416215537251
INFO:root:current train perplexity5.332355499267578
INFO:root:current mean train loss 1910.5815542022144
INFO:root:current train perplexity5.334481716156006
INFO:root:current mean train loss 1910.7931528294341
INFO:root:current train perplexity5.33349084854126
INFO:root:current mean train loss 1908.5653195090138
INFO:root:current train perplexity5.333574295043945
INFO:root:current mean train loss 1907.2650327281792
INFO:root:current train perplexity5.329086780548096
INFO:root:current mean train loss 1905.7097021762863
INFO:root:current train perplexity5.328041076660156
INFO:root:current mean train loss 1904.3601358201768
INFO:root:current train perplexity5.325803279876709
INFO:root:current mean train loss 1904.85802735155
INFO:root:current train perplexity5.323114395141602
INFO:root:current mean train loss 1903.400364170413
INFO:root:current train perplexity5.318164348602295
INFO:root:current mean train loss 1904.3510990195366
INFO:root:current train perplexity5.31989049911499
INFO:root:current mean train loss 1905.3636288200457
INFO:root:current train perplexity5.322430610656738
INFO:root:current mean train loss 1906.4156165319262
INFO:root:current train perplexity5.319943904876709
INFO:root:current mean train loss 1905.2833908290079
INFO:root:current train perplexity5.315134525299072
INFO:root:current mean train loss 1903.400194139254
INFO:root:current train perplexity5.313667297363281
INFO:root:current mean train loss 1903.1338472835353
INFO:root:current train perplexity5.313070774078369

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:07<00:00, 487.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:07<00:00, 487.57s/it]
INFO:root:final mean train loss: 1902.2699977087962
INFO:root:final train perplexity: 5.311740875244141
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.65s/it]
INFO:root:eval mean loss: 1851.9048232491134
INFO:root:eval perplexity: 5.3208327293396
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.91s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.91s/it]
INFO:root:eval mean loss: 2196.3618389156695
INFO:root:eval perplexity: 7.521024227142334
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/10
  5%|â–Œ         | 10/200 [1:36:39<30:23:10, 575.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1847.127582937047
INFO:root:current train perplexity5.132491588592529
INFO:root:current mean train loss 1854.595973991078
INFO:root:current train perplexity5.153615474700928
INFO:root:current mean train loss 1869.281356641351
INFO:root:current train perplexity5.187173843383789
INFO:root:current mean train loss 1878.4086553475397
INFO:root:current train perplexity5.201045989990234
INFO:root:current mean train loss 1876.9731198048542
INFO:root:current train perplexity5.199814796447754
INFO:root:current mean train loss 1879.596132838244
INFO:root:current train perplexity5.19988489151001
INFO:root:current mean train loss 1881.142106630699
INFO:root:current train perplexity5.20166540145874
INFO:root:current mean train loss 1881.2072838037934
INFO:root:current train perplexity5.202775955200195
INFO:root:current mean train loss 1880.0200653251761
INFO:root:current train perplexity5.205036163330078
INFO:root:current mean train loss 1879.674568483335
INFO:root:current train perplexity5.209203720092773
INFO:root:current mean train loss 1878.676329824164
INFO:root:current train perplexity5.2099785804748535
INFO:root:current mean train loss 1881.036459447177
INFO:root:current train perplexity5.210029602050781
INFO:root:current mean train loss 1882.5537856803155
INFO:root:current train perplexity5.214283466339111
INFO:root:current mean train loss 1883.0882889362388
INFO:root:current train perplexity5.215880393981934
INFO:root:current mean train loss 1884.1759014921663
INFO:root:current train perplexity5.218837738037109
INFO:root:current mean train loss 1883.410173988707
INFO:root:current train perplexity5.217339992523193
INFO:root:current mean train loss 1883.1240952607754
INFO:root:current train perplexity5.216170310974121
INFO:root:current mean train loss 1883.3249424772116
INFO:root:current train perplexity5.217952728271484
INFO:root:current mean train loss 1883.1474753717102
INFO:root:current train perplexity5.2195539474487305
INFO:root:current mean train loss 1882.1107710900799
INFO:root:current train perplexity5.2176713943481445

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:08<00:00, 488.95s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:08<00:00, 488.95s/it]
INFO:root:final mean train loss: 1881.6364536795181
INFO:root:final train perplexity: 5.216394424438477
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.53s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.53s/it]
INFO:root:eval mean loss: 1843.8634582952404
INFO:root:eval perplexity: 5.282350540161133
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.55s/it]
INFO:root:eval mean loss: 2194.829398946559
INFO:root:eval perplexity: 7.510441303253174
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/11
  6%|â–Œ         | 11/200 [1:46:09<30:08:29, 574.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1845.1947560864826
INFO:root:current train perplexity5.12713623046875
INFO:root:current mean train loss 1862.1438848023774
INFO:root:current train perplexity5.134460926055908
INFO:root:current mean train loss 1867.5604568161332
INFO:root:current train perplexity5.137515544891357
INFO:root:current mean train loss 1867.703417526008
INFO:root:current train perplexity5.129990100860596
INFO:root:current mean train loss 1872.095516251929
INFO:root:current train perplexity5.140214920043945
INFO:root:current mean train loss 1873.5414797421608
INFO:root:current train perplexity5.141412258148193
INFO:root:current mean train loss 1871.2663506599627
INFO:root:current train perplexity5.138927936553955
INFO:root:current mean train loss 1868.6546619987973
INFO:root:current train perplexity5.133883953094482
INFO:root:current mean train loss 1867.0107449430375
INFO:root:current train perplexity5.140282154083252
INFO:root:current mean train loss 1866.5554030845906
INFO:root:current train perplexity5.138853549957275
INFO:root:current mean train loss 1866.318635213441
INFO:root:current train perplexity5.139550685882568
INFO:root:current mean train loss 1866.692722674365
INFO:root:current train perplexity5.142843723297119
INFO:root:current mean train loss 1866.1072877495321
INFO:root:current train perplexity5.1430864334106445
INFO:root:current mean train loss 1865.7050142714816
INFO:root:current train perplexity5.139134407043457
INFO:root:current mean train loss 1865.1273236897239
INFO:root:current train perplexity5.135404586791992
INFO:root:current mean train loss 1865.9776616715844
INFO:root:current train perplexity5.1385817527771
INFO:root:current mean train loss 1865.4258860161578
INFO:root:current train perplexity5.1352314949035645
INFO:root:current mean train loss 1866.2175223936827
INFO:root:current train perplexity5.1372199058532715
INFO:root:current mean train loss 1864.7565987223911
INFO:root:current train perplexity5.134204387664795

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:14<00:00, 494.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:14<00:00, 494.16s/it]
INFO:root:final mean train loss: 1863.1902409186582
INFO:root:final train perplexity: 5.132604598999023
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.08s/it]
INFO:root:eval mean loss: 1835.2035271394338
INFO:root:eval perplexity: 5.2412190437316895
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.70s/it]
INFO:root:eval mean loss: 2191.9991052505816
INFO:root:eval perplexity: 7.490941524505615
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/12
  6%|â–Œ         | 12/200 [1:55:45<30:00:52, 574.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1852.7780354817708
INFO:root:current train perplexity4.7074456214904785
INFO:root:current mean train loss 1860.1893535725121
INFO:root:current train perplexity5.06720495223999
INFO:root:current mean train loss 1852.2793245362532
INFO:root:current train perplexity5.04504919052124
INFO:root:current mean train loss 1845.874664810231
INFO:root:current train perplexity5.0466485023498535
INFO:root:current mean train loss 1846.7567962570758
INFO:root:current train perplexity5.0476484298706055
INFO:root:current mean train loss 1850.2092748683679
INFO:root:current train perplexity5.058649063110352
INFO:root:current mean train loss 1846.4494940661277
INFO:root:current train perplexity5.049383640289307
INFO:root:current mean train loss 1846.8681927134269
INFO:root:current train perplexity5.049687385559082
INFO:root:current mean train loss 1844.6963077606924
INFO:root:current train perplexity5.042019367218018
INFO:root:current mean train loss 1845.3317898130365
INFO:root:current train perplexity5.044808864593506
INFO:root:current mean train loss 1846.984143881831
INFO:root:current train perplexity5.041487693786621
INFO:root:current mean train loss 1844.2577380182952
INFO:root:current train perplexity5.03944206237793
INFO:root:current mean train loss 1843.9076237222698
INFO:root:current train perplexity5.040963649749756
INFO:root:current mean train loss 1845.0367962829168
INFO:root:current train perplexity5.047717094421387
INFO:root:current mean train loss 1845.0882573579772
INFO:root:current train perplexity5.048017501831055
INFO:root:current mean train loss 1845.362250027939
INFO:root:current train perplexity5.051647186279297
INFO:root:current mean train loss 1845.5725272803923
INFO:root:current train perplexity5.052944183349609
INFO:root:current mean train loss 1845.2207920076703
INFO:root:current train perplexity5.049698829650879
INFO:root:current mean train loss 1845.8589714841582
INFO:root:current train perplexity5.054192066192627
INFO:root:current mean train loss 1845.44970292589
INFO:root:current train perplexity5.053852558135986

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:59<00:00, 479.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:59<00:00, 479.68s/it]
INFO:root:final mean train loss: 1845.2216721773748
INFO:root:final train perplexity: 5.052277565002441
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.89s/it]
INFO:root:eval mean loss: 1827.3058978141623
INFO:root:eval perplexity: 5.203988075256348
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.06s/it]
INFO:root:eval mean loss: 2187.0699242125165
INFO:root:eval perplexity: 7.457095623016357
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/13
  6%|â–‹         | 13/200 [2:05:04<29:36:17, 569.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1828.0447326660155
INFO:root:current train perplexity4.948721885681152
INFO:root:current mean train loss 1836.611019897461
INFO:root:current train perplexity4.937885761260986
INFO:root:current mean train loss 1831.87192216353
INFO:root:current train perplexity4.953609943389893
INFO:root:current mean train loss 1832.7928401947022
INFO:root:current train perplexity4.956246376037598
INFO:root:current mean train loss 1833.5920459565662
INFO:root:current train perplexity4.964354515075684
INFO:root:current mean train loss 1830.3712146465596
INFO:root:current train perplexity4.972469806671143
INFO:root:current mean train loss 1830.7463563980596
INFO:root:current train perplexity4.973832607269287
INFO:root:current mean train loss 1829.191727871365
INFO:root:current train perplexity4.968124866485596
INFO:root:current mean train loss 1831.1917091927878
INFO:root:current train perplexity4.972649574279785
INFO:root:current mean train loss 1831.801686162534
INFO:root:current train perplexity4.97664737701416
INFO:root:current mean train loss 1830.0219478831573
INFO:root:current train perplexity4.978296279907227
INFO:root:current mean train loss 1829.533921269008
INFO:root:current train perplexity4.981192588806152
INFO:root:current mean train loss 1829.1137997486553
INFO:root:current train perplexity4.978862285614014
INFO:root:current mean train loss 1829.4094912442295
INFO:root:current train perplexity4.980947971343994
INFO:root:current mean train loss 1829.6361757950044
INFO:root:current train perplexity4.979599952697754
INFO:root:current mean train loss 1829.086811427066
INFO:root:current train perplexity4.977067947387695
INFO:root:current mean train loss 1829.4569748866704
INFO:root:current train perplexity4.977066516876221
INFO:root:current mean train loss 1831.3306802439135
INFO:root:current train perplexity4.982883453369141
INFO:root:current mean train loss 1829.881485085959
INFO:root:current train perplexity4.983032703399658
INFO:root:current mean train loss 1829.2772256215414
INFO:root:current train perplexity4.981174945831299

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.43s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.43s/it]
INFO:root:final mean train loss: 1829.3971279373206
INFO:root:final train perplexity: 4.982578754425049
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.16s/it]
INFO:root:eval mean loss: 1821.511323103668
INFO:root:eval perplexity: 5.176839828491211
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.07s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.07s/it]
INFO:root:eval mean loss: 2186.1985941136136
INFO:root:eval perplexity: 7.45112943649292
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/14
  7%|â–‹         | 14/200 [2:14:14<29:07:59, 563.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1845.6246238914696
INFO:root:current train perplexity5.046528339385986
INFO:root:current mean train loss 1830.9521270529196
INFO:root:current train perplexity4.9780073165893555
INFO:root:current mean train loss 1825.306823988001
INFO:root:current train perplexity4.936899662017822
INFO:root:current mean train loss 1821.6952335346346
INFO:root:current train perplexity4.910215377807617
INFO:root:current mean train loss 1819.622432612718
INFO:root:current train perplexity4.919741630554199
INFO:root:current mean train loss 1820.3767128487982
INFO:root:current train perplexity4.915185928344727
INFO:root:current mean train loss 1822.0299386543982
INFO:root:current train perplexity4.918646335601807
INFO:root:current mean train loss 1817.7415233182455
INFO:root:current train perplexity4.915293216705322
INFO:root:current mean train loss 1819.89191322828
INFO:root:current train perplexity4.921930313110352
INFO:root:current mean train loss 1819.3306377984925
INFO:root:current train perplexity4.921480655670166
INFO:root:current mean train loss 1817.904525594978
INFO:root:current train perplexity4.921469211578369
INFO:root:current mean train loss 1815.8116232839297
INFO:root:current train perplexity4.91743803024292
INFO:root:current mean train loss 1816.0919735167618
INFO:root:current train perplexity4.916582107543945
INFO:root:current mean train loss 1816.180474155058
INFO:root:current train perplexity4.915968418121338
INFO:root:current mean train loss 1817.1037605301572
INFO:root:current train perplexity4.921393871307373
INFO:root:current mean train loss 1817.3299458887798
INFO:root:current train perplexity4.9231343269348145
INFO:root:current mean train loss 1816.0972631940335
INFO:root:current train perplexity4.925755023956299
INFO:root:current mean train loss 1815.8754296003572
INFO:root:current train perplexity4.923332691192627
INFO:root:current mean train loss 1815.9461208088085
INFO:root:current train perplexity4.923440456390381
INFO:root:current mean train loss 1814.9703868261365
INFO:root:current train perplexity4.91876220703125

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.80s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.80s/it]
INFO:root:final mean train loss: 1814.8532308558292
INFO:root:final train perplexity: 4.919369220733643
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.60s/it]
INFO:root:eval mean loss: 1815.3865823948638
INFO:root:eval perplexity: 5.148299217224121
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.01s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.01s/it]
INFO:root:eval mean loss: 2181.6251497742132
INFO:root:eval perplexity: 7.4198899269104
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/15
  8%|â–Š         | 15/200 [2:23:22<28:43:33, 558.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1785.3401285807292
INFO:root:current train perplexity4.820903301239014
INFO:root:current mean train loss 1804.601875602425
INFO:root:current train perplexity4.874256610870361
INFO:root:current mean train loss 1801.404549666277
INFO:root:current train perplexity4.850930213928223
INFO:root:current mean train loss 1802.7030284472105
INFO:root:current train perplexity4.860980033874512
INFO:root:current mean train loss 1803.379099841685
INFO:root:current train perplexity4.864274978637695
INFO:root:current mean train loss 1802.2541541364649
INFO:root:current train perplexity4.864720821380615
INFO:root:current mean train loss 1800.4855845040138
INFO:root:current train perplexity4.858636379241943
INFO:root:current mean train loss 1802.3547390803735
INFO:root:current train perplexity4.86106538772583
INFO:root:current mean train loss 1801.8913122530005
INFO:root:current train perplexity4.862124443054199
INFO:root:current mean train loss 1801.907183057357
INFO:root:current train perplexity4.863545894622803
INFO:root:current mean train loss 1801.5402109337938
INFO:root:current train perplexity4.863833427429199
INFO:root:current mean train loss 1801.0940342313081
INFO:root:current train perplexity4.866166591644287
INFO:root:current mean train loss 1801.4863082666716
INFO:root:current train perplexity4.867812156677246
INFO:root:current mean train loss 1800.2992737627803
INFO:root:current train perplexity4.864320278167725
INFO:root:current mean train loss 1800.6854261479646
INFO:root:current train perplexity4.860199451446533
INFO:root:current mean train loss 1801.2279573536287
INFO:root:current train perplexity4.859431266784668
INFO:root:current mean train loss 1800.782713958113
INFO:root:current train perplexity4.858250617980957
INFO:root:current mean train loss 1800.4039724908914
INFO:root:current train perplexity4.85761833190918
INFO:root:current mean train loss 1799.8937894786188
INFO:root:current train perplexity4.8580641746521
INFO:root:current mean train loss 1800.4046560890722
INFO:root:current train perplexity4.856480598449707

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.55s/it]
INFO:root:final mean train loss: 1800.5034036564214
INFO:root:final train perplexity: 4.857787609100342
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.56s/it]
INFO:root:eval mean loss: 1811.5102945963542
INFO:root:eval perplexity: 5.130316734313965
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.31s/it]
INFO:root:eval mean loss: 2181.1443879688886
INFO:root:eval perplexity: 7.41661262512207
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/16
  8%|â–Š         | 16/200 [2:32:30<28:24:39, 555.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1774.8349970428037
INFO:root:current train perplexity4.710921764373779
INFO:root:current mean train loss 1774.4999036287006
INFO:root:current train perplexity4.752399921417236
INFO:root:current mean train loss 1778.5688287376038
INFO:root:current train perplexity4.756308555603027
INFO:root:current mean train loss 1783.36383632444
INFO:root:current train perplexity4.781514644622803
INFO:root:current mean train loss 1784.760645516106
INFO:root:current train perplexity4.783493995666504
INFO:root:current mean train loss 1785.3384644196037
INFO:root:current train perplexity4.786491870880127
INFO:root:current mean train loss 1784.280528857349
INFO:root:current train perplexity4.780341625213623
INFO:root:current mean train loss 1783.6874027870663
INFO:root:current train perplexity4.7834391593933105
INFO:root:current mean train loss 1783.3079887241229
INFO:root:current train perplexity4.780076026916504
INFO:root:current mean train loss 1783.9188075276777
INFO:root:current train perplexity4.784032344818115
INFO:root:current mean train loss 1783.8259085860907
INFO:root:current train perplexity4.787429332733154
INFO:root:current mean train loss 1785.1787739011795
INFO:root:current train perplexity4.791284084320068
INFO:root:current mean train loss 1786.581393142088
INFO:root:current train perplexity4.797066688537598
INFO:root:current mean train loss 1786.080523045023
INFO:root:current train perplexity4.79789924621582
INFO:root:current mean train loss 1785.6497708131958
INFO:root:current train perplexity4.795938491821289
INFO:root:current mean train loss 1786.5279753142902
INFO:root:current train perplexity4.796205043792725
INFO:root:current mean train loss 1787.5380761484982
INFO:root:current train perplexity4.799071788787842
INFO:root:current mean train loss 1788.3052418825655
INFO:root:current train perplexity4.801906585693359
INFO:root:current mean train loss 1788.3617903741524
INFO:root:current train perplexity4.803266525268555
INFO:root:current mean train loss 1787.8433346141117
INFO:root:current train perplexity4.80150842666626

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:00<00:00, 480.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:00<00:00, 480.36s/it]
INFO:root:final mean train loss: 1787.513786989213
INFO:root:final train perplexity: 4.802709102630615
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.55s/it]
INFO:root:eval mean loss: 1808.1539072888963
INFO:root:eval perplexity: 5.1147966384887695
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.24s/it]
INFO:root:eval mean loss: 2181.2383241044713
INFO:root:eval perplexity: 7.417254447937012
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/17
  8%|â–Š         | 17/200 [2:41:50<28:18:40, 556.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1752.8519703258169
INFO:root:current train perplexity4.719318866729736
INFO:root:current mean train loss 1761.0494650982796
INFO:root:current train perplexity4.716031551361084
INFO:root:current mean train loss 1771.1021664937336
INFO:root:current train perplexity4.749142169952393
INFO:root:current mean train loss 1772.5471068706709
INFO:root:current train perplexity4.751278400421143
INFO:root:current mean train loss 1776.8493642337987
INFO:root:current train perplexity4.760918140411377
INFO:root:current mean train loss 1774.734699275218
INFO:root:current train perplexity4.75852632522583
INFO:root:current mean train loss 1774.7749154734056
INFO:root:current train perplexity4.757875442504883
INFO:root:current mean train loss 1772.742904275807
INFO:root:current train perplexity4.753930568695068
INFO:root:current mean train loss 1772.4470910424584
INFO:root:current train perplexity4.751053810119629
INFO:root:current mean train loss 1770.8829140605233
INFO:root:current train perplexity4.745579242706299
INFO:root:current mean train loss 1771.8700150882496
INFO:root:current train perplexity4.7462544441223145
INFO:root:current mean train loss 1773.1479548701534
INFO:root:current train perplexity4.749492645263672
INFO:root:current mean train loss 1774.2827348412934
INFO:root:current train perplexity4.751681327819824
INFO:root:current mean train loss 1775.2398744082932
INFO:root:current train perplexity4.75253438949585
INFO:root:current mean train loss 1776.230466945197
INFO:root:current train perplexity4.756703853607178
INFO:root:current mean train loss 1775.58787763389
INFO:root:current train perplexity4.754350662231445
INFO:root:current mean train loss 1774.682013633692
INFO:root:current train perplexity4.7527642250061035
INFO:root:current mean train loss 1774.9242880051036
INFO:root:current train perplexity4.751187324523926
INFO:root:current mean train loss 1775.108656737764
INFO:root:current train perplexity4.75042200088501

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:55<00:00, 475.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:55<00:00, 475.64s/it]
INFO:root:final mean train loss: 1775.5442990270817
INFO:root:final train perplexity: 4.752508163452148
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.74s/it]
INFO:root:eval mean loss: 1802.1908149448693
INFO:root:eval perplexity: 5.087339878082275
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.51s/it]
INFO:root:eval mean loss: 2177.558390299479
INFO:root:eval perplexity: 7.3922224044799805
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/18
  9%|â–‰         | 18/200 [2:51:05<28:07:45, 556.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1766.5833740234375
INFO:root:current train perplexity4.529787540435791
INFO:root:current mean train loss 1764.0553478422619
INFO:root:current train perplexity4.663008689880371
INFO:root:current mean train loss 1759.2557021722562
INFO:root:current train perplexity4.644920349121094
INFO:root:current mean train loss 1760.952557873335
INFO:root:current train perplexity4.6706767082214355
INFO:root:current mean train loss 1761.470101815683
INFO:root:current train perplexity4.684171676635742
INFO:root:current mean train loss 1761.7930884030786
INFO:root:current train perplexity4.699818134307861
INFO:root:current mean train loss 1760.7792506698734
INFO:root:current train perplexity4.697565078735352
INFO:root:current mean train loss 1761.324859922152
INFO:root:current train perplexity4.69296407699585
INFO:root:current mean train loss 1758.8698697107918
INFO:root:current train perplexity4.684755802154541
INFO:root:current mean train loss 1761.8459459167818
INFO:root:current train perplexity4.6870503425598145
INFO:root:current mean train loss 1762.8829956662003
INFO:root:current train perplexity4.68924617767334
INFO:root:current mean train loss 1764.8931340144231
INFO:root:current train perplexity4.69100284576416
INFO:root:current mean train loss 1764.0440628241702
INFO:root:current train perplexity4.694558143615723
INFO:root:current mean train loss 1763.6291279409124
INFO:root:current train perplexity4.695679187774658
INFO:root:current mean train loss 1763.9608537449956
INFO:root:current train perplexity4.696730613708496
INFO:root:current mean train loss 1762.8695187590843
INFO:root:current train perplexity4.695797443389893
INFO:root:current mean train loss 1764.545842536142
INFO:root:current train perplexity4.7005391120910645
INFO:root:current mean train loss 1764.390135573222
INFO:root:current train perplexity4.703025817871094
INFO:root:current mean train loss 1763.7882541984072
INFO:root:current train perplexity4.700536727905273
INFO:root:current mean train loss 1764.6557612061188
INFO:root:current train perplexity4.703188896179199

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:56<00:00, 476.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:56<00:00, 476.15s/it]
INFO:root:final mean train loss: 1763.4248236167089
INFO:root:final train perplexity: 4.702214241027832
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.11s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.11s/it]
INFO:root:eval mean loss: 1800.8686081906583
INFO:root:eval perplexity: 5.081271648406982
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.35s/it]
INFO:root:eval mean loss: 2181.1011560318316
INFO:root:eval perplexity: 7.41632080078125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/19
 10%|â–‰         | 19/200 [3:00:21<27:57:57, 556.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1776.995078346946
INFO:root:current train perplexity4.677546977996826
INFO:root:current mean train loss 1760.7859266937755
INFO:root:current train perplexity4.650827407836914
INFO:root:current mean train loss 1767.6660024282094
INFO:root:current train perplexity4.639777183532715
INFO:root:current mean train loss 1761.3403316521496
INFO:root:current train perplexity4.6417083740234375
INFO:root:current mean train loss 1760.360099322423
INFO:root:current train perplexity4.641761779785156
INFO:root:current mean train loss 1762.4615789537686
INFO:root:current train perplexity4.653208255767822
INFO:root:current mean train loss 1761.5222113017485
INFO:root:current train perplexity4.6526408195495605
INFO:root:current mean train loss 1760.5030480382186
INFO:root:current train perplexity4.655884742736816
INFO:root:current mean train loss 1757.5385255094282
INFO:root:current train perplexity4.659738540649414
INFO:root:current mean train loss 1757.9601380850902
INFO:root:current train perplexity4.66061544418335
INFO:root:current mean train loss 1758.2804646650638
INFO:root:current train perplexity4.662017822265625
INFO:root:current mean train loss 1756.2129828849154
INFO:root:current train perplexity4.656916618347168
INFO:root:current mean train loss 1754.6306682780214
INFO:root:current train perplexity4.6545023918151855
INFO:root:current mean train loss 1754.3127203175231
INFO:root:current train perplexity4.658384799957275
INFO:root:current mean train loss 1755.0264756944443
INFO:root:current train perplexity4.658351421356201
INFO:root:current mean train loss 1753.4342219870289
INFO:root:current train perplexity4.657359600067139
INFO:root:current mean train loss 1753.1763113000684
INFO:root:current train perplexity4.6566667556762695
INFO:root:current mean train loss 1753.0038181353668
INFO:root:current train perplexity4.65781831741333
INFO:root:current mean train loss 1752.4506004492616
INFO:root:current train perplexity4.6573166847229
INFO:root:current mean train loss 1752.7553741423321
INFO:root:current train perplexity4.655939102172852

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:54<00:00, 474.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:54<00:00, 474.75s/it]
INFO:root:final mean train loss: 1752.0515137949917
INFO:root:final train perplexity: 4.6554999351501465
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.43s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.43s/it]
INFO:root:eval mean loss: 1796.1680137688386
INFO:root:eval perplexity: 5.059758186340332
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.31s/it]
INFO:root:eval mean loss: 2177.9301489430964
INFO:root:eval perplexity: 7.394748687744141
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/20
 10%|â–ˆ         | 20/200 [3:09:36<27:47:24, 555.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1738.4058681390225
INFO:root:current train perplexity4.638258457183838
INFO:root:current mean train loss 1727.4124729513264
INFO:root:current train perplexity4.567493915557861
INFO:root:current mean train loss 1730.335985510918
INFO:root:current train perplexity4.558316230773926
INFO:root:current mean train loss 1735.4959291891362
INFO:root:current train perplexity4.580037593841553
INFO:root:current mean train loss 1740.7576159084033
INFO:root:current train perplexity4.591649532318115
INFO:root:current mean train loss 1738.3314208984375
INFO:root:current train perplexity4.5879411697387695
INFO:root:current mean train loss 1739.0661473998068
INFO:root:current train perplexity4.58601188659668
INFO:root:current mean train loss 1739.9395416631428
INFO:root:current train perplexity4.593536376953125
INFO:root:current mean train loss 1738.3131906648075
INFO:root:current train perplexity4.588422775268555
INFO:root:current mean train loss 1736.5287981937233
INFO:root:current train perplexity4.587982177734375
INFO:root:current mean train loss 1739.4101017354428
INFO:root:current train perplexity4.597415447235107
INFO:root:current mean train loss 1740.3595873101747
INFO:root:current train perplexity4.602607250213623
INFO:root:current mean train loss 1741.6654570966693
INFO:root:current train perplexity4.605250835418701
INFO:root:current mean train loss 1742.6753759291566
INFO:root:current train perplexity4.6066765785217285
INFO:root:current mean train loss 1742.927702309275
INFO:root:current train perplexity4.6063642501831055
INFO:root:current mean train loss 1742.1504359948628
INFO:root:current train perplexity4.6060099601745605
INFO:root:current mean train loss 1741.3031475818907
INFO:root:current train perplexity4.607537746429443
INFO:root:current mean train loss 1741.7280283966854
INFO:root:current train perplexity4.609975814819336
INFO:root:current mean train loss 1741.5448762728768
INFO:root:current train perplexity4.6079840660095215
INFO:root:current mean train loss 1742.2935220640672
INFO:root:current train perplexity4.612325191497803

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:54<00:00, 474.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:54<00:00, 474.62s/it]
INFO:root:final mean train loss: 1741.831227605053
INFO:root:final train perplexity: 4.613918304443359
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.25s/it]
INFO:root:eval mean loss: 1792.8803420912288
INFO:root:eval perplexity: 5.044764518737793
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.40s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.40s/it]
INFO:root:eval mean loss: 2175.9380068948085
INFO:root:eval perplexity: 7.381225109100342
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/21
 10%|â–ˆ         | 21/200 [3:18:49<27:36:10, 555.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1762.6718096051898
INFO:root:current train perplexity4.601174354553223
INFO:root:current mean train loss 1737.3194008851663
INFO:root:current train perplexity4.5693359375
INFO:root:current mean train loss 1727.1743969917297
INFO:root:current train perplexity4.5481133460998535
INFO:root:current mean train loss 1726.0155711656207
INFO:root:current train perplexity4.533716678619385
INFO:root:current mean train loss 1729.3674011230469
INFO:root:current train perplexity4.5513081550598145
INFO:root:current mean train loss 1727.82557162278
INFO:root:current train perplexity4.551980495452881
INFO:root:current mean train loss 1729.6716544918897
INFO:root:current train perplexity4.555509567260742
INFO:root:current mean train loss 1732.0283949110244
INFO:root:current train perplexity4.55556583404541
INFO:root:current mean train loss 1731.8690027254763
INFO:root:current train perplexity4.561159610748291
INFO:root:current mean train loss 1731.4403247274615
INFO:root:current train perplexity4.559279441833496
INFO:root:current mean train loss 1729.3985391096635
INFO:root:current train perplexity4.560028076171875
INFO:root:current mean train loss 1730.7354633898883
INFO:root:current train perplexity4.566077709197998
INFO:root:current mean train loss 1731.4705930090254
INFO:root:current train perplexity4.5690202713012695
INFO:root:current mean train loss 1731.4959249580857
INFO:root:current train perplexity4.567617416381836
INFO:root:current mean train loss 1732.7197847471132
INFO:root:current train perplexity4.570810794830322
INFO:root:current mean train loss 1733.385422576669
INFO:root:current train perplexity4.570664405822754
INFO:root:current mean train loss 1733.1504257865574
INFO:root:current train perplexity4.57013463973999
INFO:root:current mean train loss 1732.1305636534116
INFO:root:current train perplexity4.568154335021973
INFO:root:current mean train loss 1731.2663670901595
INFO:root:current train perplexity4.567127704620361
INFO:root:current mean train loss 1731.678920878467
INFO:root:current train perplexity4.570019245147705

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:01<00:00, 481.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:01<00:00, 481.51s/it]
INFO:root:final mean train loss: 1731.0212469750202
INFO:root:final train perplexity: 4.570341110229492
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.83s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.83s/it]
INFO:root:eval mean loss: 1790.4691664242575
INFO:root:eval perplexity: 5.033796310424805
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.99s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.01s/it]
INFO:root:eval mean loss: 2174.409332491827
INFO:root:eval perplexity: 7.3708672523498535
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/22
 11%|â–ˆ         | 22/200 [3:28:10<27:31:59, 556.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1686.7560683995077
INFO:root:current train perplexity4.483229160308838
INFO:root:current mean train loss 1692.5940830473266
INFO:root:current train perplexity4.4627909660339355
INFO:root:current mean train loss 1709.688224820427
INFO:root:current train perplexity4.49365758895874
INFO:root:current mean train loss 1713.7460037517803
INFO:root:current train perplexity4.514005661010742
INFO:root:current mean train loss 1715.4354289339158
INFO:root:current train perplexity4.523955345153809
INFO:root:current mean train loss 1718.3322649518025
INFO:root:current train perplexity4.526782512664795
INFO:root:current mean train loss 1719.8842403417534
INFO:root:current train perplexity4.529786109924316
INFO:root:current mean train loss 1720.2035663797199
INFO:root:current train perplexity4.527937412261963
INFO:root:current mean train loss 1722.4571025625537
INFO:root:current train perplexity4.527727127075195
INFO:root:current mean train loss 1720.9998353995375
INFO:root:current train perplexity4.52357292175293
INFO:root:current mean train loss 1721.122595681282
INFO:root:current train perplexity4.52166748046875
INFO:root:current mean train loss 1721.2671850648378
INFO:root:current train perplexity4.522879123687744
INFO:root:current mean train loss 1720.6116894454537
INFO:root:current train perplexity4.521936893463135
INFO:root:current mean train loss 1721.668711396264
INFO:root:current train perplexity4.525042533874512
INFO:root:current mean train loss 1720.9542940739298
INFO:root:current train perplexity4.525506019592285
INFO:root:current mean train loss 1721.2505345329387
INFO:root:current train perplexity4.528070449829102
INFO:root:current mean train loss 1721.7570475357704
INFO:root:current train perplexity4.530667304992676
INFO:root:current mean train loss 1721.6817619379672
INFO:root:current train perplexity4.530697345733643
INFO:root:current mean train loss 1722.6175791938485
INFO:root:current train perplexity4.534848213195801
INFO:root:current mean train loss 1721.8465083064734
INFO:root:current train perplexity4.532311916351318

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:00<00:00, 480.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:00<00:00, 480.44s/it]
INFO:root:final mean train loss: 1721.4380891446929
INFO:root:final train perplexity: 4.532053470611572
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.71s/it]
INFO:root:eval mean loss: 1791.0831878878546
INFO:root:eval perplexity: 5.036587715148926
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:46<00:00, 46.20s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:46<00:00, 46.20s/it]
INFO:root:eval mean loss: 2181.852370241855
INFO:root:eval perplexity: 7.421438217163086
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/23
 12%|â–ˆâ–        | 23/200 [3:37:40<27:33:58, 560.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1689.1072062174478
INFO:root:current train perplexity4.460411548614502
INFO:root:current mean train loss 1703.3053531044409
INFO:root:current train perplexity4.470256805419922
INFO:root:current mean train loss 1701.544834742053
INFO:root:current train perplexity4.469494342803955
INFO:root:current mean train loss 1697.4370198567708
INFO:root:current train perplexity4.455917835235596
INFO:root:current mean train loss 1699.781683474171
INFO:root:current train perplexity4.468465328216553
INFO:root:current mean train loss 1700.9527902376853
INFO:root:current train perplexity4.475323677062988
INFO:root:current mean train loss 1700.817141856318
INFO:root:current train perplexity4.474577903747559
INFO:root:current mean train loss 1700.3822335158723
INFO:root:current train perplexity4.474029541015625
INFO:root:current mean train loss 1700.3341549991221
INFO:root:current train perplexity4.476232051849365
INFO:root:current mean train loss 1703.530211046007
INFO:root:current train perplexity4.479465484619141
INFO:root:current mean train loss 1703.9082452336584
INFO:root:current train perplexity4.4781813621521
INFO:root:current mean train loss 1705.5425656102284
INFO:root:current train perplexity4.4841437339782715
INFO:root:current mean train loss 1707.751131847293
INFO:root:current train perplexity4.488685607910156
INFO:root:current mean train loss 1707.8117160275685
INFO:root:current train perplexity4.487710475921631
INFO:root:current mean train loss 1708.285349842046
INFO:root:current train perplexity4.4918212890625
INFO:root:current mean train loss 1710.1346979105247
INFO:root:current train perplexity4.491984844207764
INFO:root:current mean train loss 1710.7328793136326
INFO:root:current train perplexity4.491458415985107
INFO:root:current mean train loss 1711.6186358403893
INFO:root:current train perplexity4.491070747375488
INFO:root:current mean train loss 1711.5248996956639
INFO:root:current train perplexity4.490466594696045

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:02<00:00, 482.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:02<00:00, 482.66s/it]
INFO:root:final mean train loss: 1711.0823231599454
INFO:root:final train perplexity: 4.491039752960205
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.04s/it]
INFO:root:eval mean loss: 1788.1809320111647
INFO:root:eval perplexity: 5.023409843444824
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.47s/it]
INFO:root:eval mean loss: 2178.6609345564607
INFO:root:eval perplexity: 7.399711608886719
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/24
 12%|â–ˆâ–        | 24/200 [3:47:03<27:27:05, 561.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1780.26806640625
INFO:root:current train perplexity4.637368679046631
INFO:root:current mean train loss 1719.1660783714224
INFO:root:current train perplexity4.497313022613525
INFO:root:current mean train loss 1717.7245091240188
INFO:root:current train perplexity4.479619979858398
INFO:root:current mean train loss 1714.5512420952514
INFO:root:current train perplexity4.468582630157471
INFO:root:current mean train loss 1714.0295950024954
INFO:root:current train perplexity4.473112106323242
INFO:root:current mean train loss 1709.6463119837895
INFO:root:current train perplexity4.456395626068115
INFO:root:current mean train loss 1708.9892642478378
INFO:root:current train perplexity4.459720611572266
INFO:root:current mean train loss 1710.0858392567075
INFO:root:current train perplexity4.460920810699463
INFO:root:current mean train loss 1707.3730914979767
INFO:root:current train perplexity4.460175037384033
INFO:root:current mean train loss 1706.2869003615542
INFO:root:current train perplexity4.460132122039795
INFO:root:current mean train loss 1705.843834976454
INFO:root:current train perplexity4.45672607421875
INFO:root:current mean train loss 1704.8495345007975
INFO:root:current train perplexity4.458174705505371
INFO:root:current mean train loss 1702.9041009759153
INFO:root:current train perplexity4.454005241394043
INFO:root:current mean train loss 1703.2913970597026
INFO:root:current train perplexity4.455938339233398
INFO:root:current mean train loss 1702.7301648322284
INFO:root:current train perplexity4.4548468589782715
INFO:root:current mean train loss 1703.713191143155
INFO:root:current train perplexity4.457042217254639
INFO:root:current mean train loss 1702.7844577070045
INFO:root:current train perplexity4.456272602081299
INFO:root:current mean train loss 1702.5923066812436
INFO:root:current train perplexity4.454443454742432
INFO:root:current mean train loss 1702.4404905537708
INFO:root:current train perplexity4.453902721405029
INFO:root:current mean train loss 1703.2652225584402
INFO:root:current train perplexity4.456911087036133

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:01<00:00, 481.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:01<00:00, 481.29s/it]
INFO:root:final mean train loss: 1702.5813750374275
INFO:root:final train perplexity: 4.4576497077941895
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.31s/it]
INFO:root:eval mean loss: 1786.6169701975289
INFO:root:eval perplexity: 5.016323089599609
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.47s/it]
INFO:root:eval mean loss: 2180.4518934750386
INFO:root:eval perplexity: 7.411898136138916
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/25
 12%|â–ˆâ–Ž        | 25/200 [3:56:26<27:19:20, 562.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1708.5219116210938
INFO:root:current train perplexity4.444512844085693
INFO:root:current mean train loss 1695.3194373346143
INFO:root:current train perplexity4.408494472503662
INFO:root:current mean train loss 1683.6049177987236
INFO:root:current train perplexity4.382662773132324
INFO:root:current mean train loss 1681.9807019645784
INFO:root:current train perplexity4.383670806884766
INFO:root:current mean train loss 1685.3377984964623
INFO:root:current train perplexity4.392710208892822
INFO:root:current mean train loss 1685.9286522319298
INFO:root:current train perplexity4.392443656921387
INFO:root:current mean train loss 1686.5302435067983
INFO:root:current train perplexity4.399234294891357
INFO:root:current mean train loss 1688.7031690060105
INFO:root:current train perplexity4.406787395477295
INFO:root:current mean train loss 1690.9280383656326
INFO:root:current train perplexity4.4104905128479
INFO:root:current mean train loss 1692.2899508125338
INFO:root:current train perplexity4.413822650909424
INFO:root:current mean train loss 1692.4098118543625
INFO:root:current train perplexity4.41412353515625
INFO:root:current mean train loss 1692.567575040661
INFO:root:current train perplexity4.414778232574463
INFO:root:current mean train loss 1692.201299430498
INFO:root:current train perplexity4.414566516876221
INFO:root:current mean train loss 1693.0760249111947
INFO:root:current train perplexity4.415578365325928
INFO:root:current mean train loss 1692.5259702875373
INFO:root:current train perplexity4.417699337005615
INFO:root:current mean train loss 1691.7926709432927
INFO:root:current train perplexity4.416324138641357
INFO:root:current mean train loss 1691.834015869742
INFO:root:current train perplexity4.418459892272949
INFO:root:current mean train loss 1692.5276196119128
INFO:root:current train perplexity4.41788911819458
INFO:root:current mean train loss 1692.1146125793457
INFO:root:current train perplexity4.416651725769043
INFO:root:current mean train loss 1692.5864670212204
INFO:root:current train perplexity4.4192423820495605

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:56<00:00, 476.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:56<00:00, 476.29s/it]
INFO:root:final mean train loss: 1693.3177022082723
INFO:root:final train perplexity: 4.421546459197998
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.10s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.12s/it]
INFO:root:eval mean loss: 1784.3457130810893
INFO:root:eval perplexity: 5.006050109863281
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.97s/it]
INFO:root:eval mean loss: 2179.489425767398
INFO:root:eval perplexity: 7.405346870422363
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/26
 13%|â–ˆâ–Ž        | 26/200 [4:05:42<27:04:30, 560.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1667.5099025819359
INFO:root:current train perplexity4.322051048278809
INFO:root:current mean train loss 1673.918338423925
INFO:root:current train perplexity4.349175930023193
INFO:root:current mean train loss 1678.0436626766727
INFO:root:current train perplexity4.356356143951416
INFO:root:current mean train loss 1679.1560971436263
INFO:root:current train perplexity4.3654327392578125
INFO:root:current mean train loss 1677.270653411104
INFO:root:current train perplexity4.363024711608887
INFO:root:current mean train loss 1678.323361550153
INFO:root:current train perplexity4.364218235015869
INFO:root:current mean train loss 1681.0562645113228
INFO:root:current train perplexity4.367519378662109
INFO:root:current mean train loss 1679.6471433240554
INFO:root:current train perplexity4.3620195388793945
INFO:root:current mean train loss 1680.0665977015365
INFO:root:current train perplexity4.364583492279053
INFO:root:current mean train loss 1679.7807199476122
INFO:root:current train perplexity4.362363815307617
INFO:root:current mean train loss 1681.77559173027
INFO:root:current train perplexity4.369240760803223
INFO:root:current mean train loss 1681.6940348806556
INFO:root:current train perplexity4.372023105621338
INFO:root:current mean train loss 1683.4762344544786
INFO:root:current train perplexity4.377248764038086
INFO:root:current mean train loss 1683.9541660112557
INFO:root:current train perplexity4.378843784332275
INFO:root:current mean train loss 1684.372351048805
INFO:root:current train perplexity4.381569862365723
INFO:root:current mean train loss 1684.640622069045
INFO:root:current train perplexity4.383072853088379
INFO:root:current mean train loss 1685.106174317894
INFO:root:current train perplexity4.384808540344238
INFO:root:current mean train loss 1685.5686451639683
INFO:root:current train perplexity4.385049343109131
INFO:root:current mean train loss 1684.4545443574739
INFO:root:current train perplexity4.384471416473389
INFO:root:current mean train loss 1684.7846446364028
INFO:root:current train perplexity4.3865814208984375

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:55<00:00, 475.12s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:55<00:00, 475.12s/it]
INFO:root:final mean train loss: 1684.419926307205
INFO:root:final train perplexity: 4.387144565582275
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.39s/it]
INFO:root:eval mean loss: 1784.6623167213818
INFO:root:eval perplexity: 5.007480621337891
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.71s/it]
INFO:root:eval mean loss: 2181.1965847150655
INFO:root:eval perplexity: 7.4169697761535645
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/27
 14%|â–ˆâ–Ž        | 27/200 [4:14:57<26:50:17, 558.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1647.7371910358297
INFO:root:current train perplexity4.271998405456543
INFO:root:current mean train loss 1660.608345128313
INFO:root:current train perplexity4.293919563293457
INFO:root:current mean train loss 1661.543102648831
INFO:root:current train perplexity4.29384183883667
INFO:root:current mean train loss 1665.1550657815774
INFO:root:current train perplexity4.312610626220703
INFO:root:current mean train loss 1667.4319615259963
INFO:root:current train perplexity4.313392162322998
INFO:root:current mean train loss 1664.5091679617497
INFO:root:current train perplexity4.322614669799805
INFO:root:current mean train loss 1664.0244198135329
INFO:root:current train perplexity4.324416637420654
INFO:root:current mean train loss 1668.3543511141572
INFO:root:current train perplexity4.329871654510498
INFO:root:current mean train loss 1668.9064424954927
INFO:root:current train perplexity4.33389139175415
INFO:root:current mean train loss 1669.6397609511596
INFO:root:current train perplexity4.340157985687256
INFO:root:current mean train loss 1671.2378776564715
INFO:root:current train perplexity4.340917110443115
INFO:root:current mean train loss 1673.1243577077598
INFO:root:current train perplexity4.341983795166016
INFO:root:current mean train loss 1673.6774246385633
INFO:root:current train perplexity4.342669486999512
INFO:root:current mean train loss 1674.3278466113713
INFO:root:current train perplexity4.343411922454834
INFO:root:current mean train loss 1674.894495164743
INFO:root:current train perplexity4.344672203063965
INFO:root:current mean train loss 1675.334618310108
INFO:root:current train perplexity4.347204685211182
INFO:root:current mean train loss 1675.3488198937207
INFO:root:current train perplexity4.347994327545166
INFO:root:current mean train loss 1676.0989443070516
INFO:root:current train perplexity4.352087497711182
INFO:root:current mean train loss 1676.6675026752978
INFO:root:current train perplexity4.352452754974365
INFO:root:current mean train loss 1676.215335397847
INFO:root:current train perplexity4.353414535522461

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:56<00:00, 476.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:56<00:00, 476.51s/it]
INFO:root:final mean train loss: 1675.5990103809627
INFO:root:final train perplexity: 4.3533034324646
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.85s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.85s/it]
INFO:root:eval mean loss: 1782.4507151935118
INFO:root:eval perplexity: 4.997494220733643
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.34s/it]
INFO:root:eval mean loss: 2183.5359674548426
INFO:root:eval perplexity: 7.432926654815674
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/28
 14%|â–ˆâ–        | 28/200 [4:24:13<26:38:54, 557.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1682.9503727213541
INFO:root:current train perplexity4.318944454193115
INFO:root:current mean train loss 1655.396619001116
INFO:root:current train perplexity4.2713704109191895
INFO:root:current mean train loss 1659.1298006924717
INFO:root:current train perplexity4.291838645935059
INFO:root:current mean train loss 1660.1708577473958
INFO:root:current train perplexity4.29182767868042
INFO:root:current mean train loss 1659.6563288959703
INFO:root:current train perplexity4.2953572273254395
INFO:root:current mean train loss 1662.1733534307066
INFO:root:current train perplexity4.296687602996826
INFO:root:current mean train loss 1663.9357658781828
INFO:root:current train perplexity4.303986072540283
INFO:root:current mean train loss 1665.4604170866935
INFO:root:current train perplexity4.310187816619873
INFO:root:current mean train loss 1667.7465859375
INFO:root:current train perplexity4.3109588623046875
INFO:root:current mean train loss 1668.281471103766
INFO:root:current train perplexity4.313752174377441
INFO:root:current mean train loss 1665.4614300962935
INFO:root:current train perplexity4.3108439445495605
INFO:root:current mean train loss 1665.2547130568485
INFO:root:current train perplexity4.3123779296875
INFO:root:current mean train loss 1665.0590246821384
INFO:root:current train perplexity4.3150553703308105
INFO:root:current mean train loss 1666.4479368785512
INFO:root:current train perplexity4.31687068939209
INFO:root:current mean train loss 1667.085946520789
INFO:root:current train perplexity4.318821907043457
INFO:root:current mean train loss 1666.8886205667163
INFO:root:current train perplexity4.317623615264893
INFO:root:current mean train loss 1667.680973282999
INFO:root:current train perplexity4.320107460021973
INFO:root:current mean train loss 1667.876689521897
INFO:root:current train perplexity4.32063627243042
INFO:root:current mean train loss 1667.2740097005208
INFO:root:current train perplexity4.321306228637695
INFO:root:current mean train loss 1668.1609236550632
INFO:root:current train perplexity4.323693752288818

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:07<00:00, 487.20s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:07<00:00, 487.20s/it]
INFO:root:final mean train loss: 1667.915619373562
INFO:root:final train perplexity: 4.324039459228516
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.55s/it]
INFO:root:eval mean loss: 1781.2084008615914
INFO:root:eval perplexity: 4.991893291473389
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.94s/it]
INFO:root:eval mean loss: 2182.7591994265294
INFO:root:eval perplexity: 7.4276227951049805
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/29
 14%|â–ˆâ–        | 29/200 [4:33:41<26:38:28, 560.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1658.7681367293649
INFO:root:current train perplexity4.28604793548584
INFO:root:current mean train loss 1654.9549573262532
INFO:root:current train perplexity4.279631614685059
INFO:root:current mean train loss 1651.7565253270816
INFO:root:current train perplexity4.275470733642578
INFO:root:current mean train loss 1652.4805466009645
INFO:root:current train perplexity4.281550884246826
INFO:root:current mean train loss 1654.9805751893578
INFO:root:current train perplexity4.289975643157959
INFO:root:current mean train loss 1654.8342518162083
INFO:root:current train perplexity4.290940284729004
INFO:root:current mean train loss 1653.387849311608
INFO:root:current train perplexity4.29261589050293
INFO:root:current mean train loss 1653.02268904869
INFO:root:current train perplexity4.293726444244385
INFO:root:current mean train loss 1654.863631586323
INFO:root:current train perplexity4.296221733093262
INFO:root:current mean train loss 1654.979863566737
INFO:root:current train perplexity4.2944865226745605
INFO:root:current mean train loss 1657.1492547674493
INFO:root:current train perplexity4.297629356384277
INFO:root:current mean train loss 1657.3646098911363
INFO:root:current train perplexity4.297435283660889
INFO:root:current mean train loss 1657.3371666119933
INFO:root:current train perplexity4.295320510864258
INFO:root:current mean train loss 1657.5759653551825
INFO:root:current train perplexity4.296131610870361
INFO:root:current mean train loss 1657.8674011230469
INFO:root:current train perplexity4.295036315917969
INFO:root:current mean train loss 1658.4600488097224
INFO:root:current train perplexity4.2955498695373535
INFO:root:current mean train loss 1659.1971537272136
INFO:root:current train perplexity4.293734550476074
INFO:root:current mean train loss 1659.6655742781502
INFO:root:current train perplexity4.291946887969971
INFO:root:current mean train loss 1660.6266260126934
INFO:root:current train perplexity4.294515132904053

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:59<00:00, 479.43s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:59<00:00, 479.45s/it]
INFO:root:final mean train loss: 1660.2873234135661
INFO:root:final train perplexity: 4.295180320739746
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.37s/it]
INFO:root:eval mean loss: 1782.7517747811391
INFO:root:eval perplexity: 4.998852252960205
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.81s/it]
INFO:root:eval mean loss: 2189.434149559508
INFO:root:eval perplexity: 7.473308086395264
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/30
 15%|â–ˆâ–Œ        | 30/200 [4:43:01<26:28:22, 560.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1661.7614339192708
INFO:root:current train perplexity4.1739277839660645
INFO:root:current mean train loss 1641.6739300369122
INFO:root:current train perplexity4.212819576263428
INFO:root:current mean train loss 1644.5524511017868
INFO:root:current train perplexity4.226191520690918
INFO:root:current mean train loss 1645.5209095778976
INFO:root:current train perplexity4.227825164794922
INFO:root:current mean train loss 1643.8554221901743
INFO:root:current train perplexity4.228191375732422
INFO:root:current mean train loss 1642.8499830204753
INFO:root:current train perplexity4.231433868408203
INFO:root:current mean train loss 1646.5997214231193
INFO:root:current train perplexity4.243176460266113
INFO:root:current mean train loss 1646.6585204389435
INFO:root:current train perplexity4.241967678070068
INFO:root:current mean train loss 1647.2160578139485
INFO:root:current train perplexity4.2489237785339355
INFO:root:current mean train loss 1647.9940305065663
INFO:root:current train perplexity4.2496232986450195
INFO:root:current mean train loss 1646.0600566580463
INFO:root:current train perplexity4.246979713439941
INFO:root:current mean train loss 1644.8801253020388
INFO:root:current train perplexity4.244581699371338
INFO:root:current mean train loss 1646.363806081666
INFO:root:current train perplexity4.248038291931152
INFO:root:current mean train loss 1647.7843209682665
INFO:root:current train perplexity4.251551628112793
INFO:root:current mean train loss 1648.256167626364
INFO:root:current train perplexity4.25458288192749
INFO:root:current mean train loss 1649.7955562523298
INFO:root:current train perplexity4.259119510650635
INFO:root:current mean train loss 1649.7294071403774
INFO:root:current train perplexity4.260843276977539
INFO:root:current mean train loss 1651.339286326525
INFO:root:current train perplexity4.264562606811523
INFO:root:current mean train loss 1652.5452107544957
INFO:root:current train perplexity4.26597261428833
INFO:root:current mean train loss 1653.150206208666
INFO:root:current train perplexity4.2655744552612305

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:56<00:00, 476.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:56<00:00, 476.66s/it]
INFO:root:final mean train loss: 1652.2587642236845
INFO:root:final train perplexity: 4.2650146484375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.72s/it]
INFO:root:eval mean loss: 1781.2547949391899
INFO:root:eval perplexity: 4.992102146148682
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.88s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.88s/it]
INFO:root:eval mean loss: 2188.6116540787066
INFO:root:eval perplexity: 7.467665672302246
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/31
 16%|â–ˆâ–Œ        | 31/200 [4:52:16<26:14:42, 559.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1623.69142972506
INFO:root:current train perplexity4.211938381195068
INFO:root:current mean train loss 1618.5769294859872
INFO:root:current train perplexity4.169416427612305
INFO:root:current mean train loss 1631.5103089999309
INFO:root:current train perplexity4.188531398773193
INFO:root:current mean train loss 1633.8525360669096
INFO:root:current train perplexity4.210244655609131
INFO:root:current mean train loss 1635.8973259724362
INFO:root:current train perplexity4.218235969543457
INFO:root:current mean train loss 1638.7005882118137
INFO:root:current train perplexity4.225372314453125
INFO:root:current mean train loss 1635.88048828905
INFO:root:current train perplexity4.22634220123291
INFO:root:current mean train loss 1640.9922448360558
INFO:root:current train perplexity4.233818054199219
INFO:root:current mean train loss 1639.2276827094054
INFO:root:current train perplexity4.233379364013672
INFO:root:current mean train loss 1640.3628062831147
INFO:root:current train perplexity4.2362189292907715
INFO:root:current mean train loss 1640.9720512523986
INFO:root:current train perplexity4.233030796051025
INFO:root:current mean train loss 1641.371009948622
INFO:root:current train perplexity4.233251571655273
INFO:root:current mean train loss 1642.159527080094
INFO:root:current train perplexity4.233279228210449
INFO:root:current mean train loss 1642.9164155295532
INFO:root:current train perplexity4.2351789474487305
INFO:root:current mean train loss 1642.4181224079318
INFO:root:current train perplexity4.233534336090088
INFO:root:current mean train loss 1642.2985595063176
INFO:root:current train perplexity4.23374605178833
INFO:root:current mean train loss 1643.4001493371868
INFO:root:current train perplexity4.2345871925354
INFO:root:current mean train loss 1643.951444729731
INFO:root:current train perplexity4.2343645095825195
INFO:root:current mean train loss 1644.1875236653289
INFO:root:current train perplexity4.2351813316345215
INFO:root:current mean train loss 1644.460743873414
INFO:root:current train perplexity4.234230041503906

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:53<00:00, 473.93s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:53<00:00, 473.93s/it]
INFO:root:final mean train loss: 1644.1059598321574
INFO:root:final train perplexity: 4.2345991134643555
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.35s/it]
INFO:root:eval mean loss: 1781.262160713791
INFO:root:eval perplexity: 4.992135524749756
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.84s/it]
INFO:root:eval mean loss: 2190.2586661264404
INFO:root:eval perplexity: 7.478971004486084
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/32
 16%|â–ˆâ–Œ        | 32/200 [5:01:32<26:02:18, 557.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1599.2505734465842
INFO:root:current train perplexity4.137121677398682
INFO:root:current mean train loss 1620.5711652849104
INFO:root:current train perplexity4.174872875213623
INFO:root:current mean train loss 1616.3359420211227
INFO:root:current train perplexity4.169219017028809
INFO:root:current mean train loss 1624.305063675861
INFO:root:current train perplexity4.174257278442383
INFO:root:current mean train loss 1628.3707779653992
INFO:root:current train perplexity4.178714275360107
INFO:root:current mean train loss 1629.2498170069348
INFO:root:current train perplexity4.17785120010376
INFO:root:current mean train loss 1632.1614732045223
INFO:root:current train perplexity4.184930801391602
INFO:root:current mean train loss 1632.4059013685019
INFO:root:current train perplexity4.1871113777160645
INFO:root:current mean train loss 1632.0898664843287
INFO:root:current train perplexity4.186430931091309
INFO:root:current mean train loss 1633.4990224019089
INFO:root:current train perplexity4.188514709472656
INFO:root:current mean train loss 1635.47527005277
INFO:root:current train perplexity4.19492244720459
INFO:root:current mean train loss 1633.4217687358173
INFO:root:current train perplexity4.193084716796875
INFO:root:current mean train loss 1632.63922251719
INFO:root:current train perplexity4.190174102783203
INFO:root:current mean train loss 1634.168566012862
INFO:root:current train perplexity4.190415859222412
INFO:root:current mean train loss 1634.4264658758068
INFO:root:current train perplexity4.193068504333496
INFO:root:current mean train loss 1635.0416872094995
INFO:root:current train perplexity4.192619323730469
INFO:root:current mean train loss 1635.9550397876408
INFO:root:current train perplexity4.196809768676758
INFO:root:current mean train loss 1635.786178475061
INFO:root:current train perplexity4.199839115142822
INFO:root:current mean train loss 1637.364252579969
INFO:root:current train perplexity4.204154968261719
INFO:root:current mean train loss 1637.615430328322
INFO:root:current train perplexity4.2060346603393555

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:00<00:00, 480.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:00<00:00, 480.54s/it]
INFO:root:final mean train loss: 1636.9203341316227
INFO:root:final train perplexity: 4.207971572875977
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.94s/it]
INFO:root:eval mean loss: 1779.2715298267121
INFO:root:eval perplexity: 4.98317289352417
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.53s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.53s/it]
INFO:root:eval mean loss: 2188.4752214580562
INFO:root:eval perplexity: 7.466729164123535
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/33
 16%|â–ˆâ–‹        | 33/200 [5:10:54<25:56:42, 559.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1603.4629821777344
INFO:root:current train perplexity4.114462852478027
INFO:root:current mean train loss 1607.5678985595703
INFO:root:current train perplexity4.1033501625061035
INFO:root:current mean train loss 1611.56906644381
INFO:root:current train perplexity4.115339756011963
INFO:root:current mean train loss 1621.3020718044704
INFO:root:current train perplexity4.140401363372803
INFO:root:current mean train loss 1629.1879158351733
INFO:root:current train perplexity4.161728858947754
INFO:root:current mean train loss 1626.1223264421735
INFO:root:current train perplexity4.149991035461426
INFO:root:current mean train loss 1627.7742263331559
INFO:root:current train perplexity4.152760982513428
INFO:root:current mean train loss 1632.03066133198
INFO:root:current train perplexity4.16619873046875
INFO:root:current mean train loss 1632.1631885617278
INFO:root:current train perplexity4.169668674468994
INFO:root:current mean train loss 1633.1713644663494
INFO:root:current train perplexity4.175034046173096
INFO:root:current mean train loss 1633.476417743035
INFO:root:current train perplexity4.177848815917969
INFO:root:current mean train loss 1632.9282803239494
INFO:root:current train perplexity4.176044940948486
INFO:root:current mean train loss 1632.378847249349
INFO:root:current train perplexity4.178483009338379
INFO:root:current mean train loss 1632.225865352855
INFO:root:current train perplexity4.177535533905029
INFO:root:current mean train loss 1630.9076007163687
INFO:root:current train perplexity4.176609992980957
INFO:root:current mean train loss 1631.5587278708433
INFO:root:current train perplexity4.180020332336426
INFO:root:current mean train loss 1630.5535271702042
INFO:root:current train perplexity4.179596424102783
INFO:root:current mean train loss 1630.285490694913
INFO:root:current train perplexity4.181244850158691
INFO:root:current mean train loss 1630.2041278798092
INFO:root:current train perplexity4.180330276489258
INFO:root:current mean train loss 1629.396294543208
INFO:root:current train perplexity4.178885459899902

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:00<00:00, 480.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:00<00:00, 480.32s/it]
INFO:root:final mean train loss: 1628.8320070575478
INFO:root:final train perplexity: 4.178199291229248
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.44s/it]
INFO:root:eval mean loss: 1781.6756994369182
INFO:root:eval perplexity: 4.993999004364014
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.28s/it]
INFO:root:eval mean loss: 2195.8083712530474
INFO:root:eval perplexity: 7.517200469970703
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/34
 17%|â–ˆâ–‹        | 34/200 [5:20:16<25:49:08, 559.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1599.2097722833807
INFO:root:current train perplexity4.097458839416504
INFO:root:current mean train loss 1618.3762372550318
INFO:root:current train perplexity4.131804943084717
INFO:root:current mean train loss 1616.0928487949852
INFO:root:current train perplexity4.130442142486572
INFO:root:current mean train loss 1613.7288834549072
INFO:root:current train perplexity4.131216049194336
INFO:root:current mean train loss 1613.4876030815972
INFO:root:current train perplexity4.126319885253906
INFO:root:current mean train loss 1613.5732438799826
INFO:root:current train perplexity4.121255397796631
INFO:root:current mean train loss 1615.8223400933
INFO:root:current train perplexity4.120934009552002
INFO:root:current mean train loss 1617.7115621480855
INFO:root:current train perplexity4.12848424911499
INFO:root:current mean train loss 1617.5125831247328
INFO:root:current train perplexity4.1330485343933105
INFO:root:current mean train loss 1618.9554057282337
INFO:root:current train perplexity4.135679721832275
INFO:root:current mean train loss 1619.2989255999014
INFO:root:current train perplexity4.1395063400268555
INFO:root:current mean train loss 1618.5111345341375
INFO:root:current train perplexity4.141735553741455
INFO:root:current mean train loss 1620.931059811203
INFO:root:current train perplexity4.1458940505981445
INFO:root:current mean train loss 1621.3795091550076
INFO:root:current train perplexity4.148408889770508
INFO:root:current mean train loss 1620.593400235909
INFO:root:current train perplexity4.1486616134643555
INFO:root:current mean train loss 1621.0011301373256
INFO:root:current train perplexity4.14970588684082
INFO:root:current mean train loss 1623.1850374843937
INFO:root:current train perplexity4.155946254730225
INFO:root:current mean train loss 1622.6238695615855
INFO:root:current train perplexity4.153992652893066
INFO:root:current mean train loss 1622.7720842559604
INFO:root:current train perplexity4.154474258422852
INFO:root:current mean train loss 1623.083578708863
INFO:root:current train perplexity4.155176162719727

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:57<00:00, 477.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:57<00:00, 477.50s/it]
INFO:root:final mean train loss: 1622.5187027970169
INFO:root:final train perplexity: 4.155107021331787
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.83s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.83s/it]
INFO:root:eval mean loss: 1781.9756067154256
INFO:root:eval perplexity: 4.995351314544678
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.08s/it]
INFO:root:eval mean loss: 2197.704134460882
INFO:root:eval perplexity: 7.530304431915283
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/35
 18%|â–ˆâ–Š        | 35/200 [5:29:32<25:37:19, 559.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1605.698827865276
INFO:root:current train perplexity4.123453617095947
INFO:root:current mean train loss 1601.6255512040916
INFO:root:current train perplexity4.099693775177002
INFO:root:current mean train loss 1611.7742982202647
INFO:root:current train perplexity4.10407018661499
INFO:root:current mean train loss 1610.6990238712524
INFO:root:current train perplexity4.10882043838501
INFO:root:current mean train loss 1615.4353763719319
INFO:root:current train perplexity4.121537685394287
INFO:root:current mean train loss 1615.879975084504
INFO:root:current train perplexity4.1242499351501465
INFO:root:current mean train loss 1616.9567542172304
INFO:root:current train perplexity4.12790584564209
INFO:root:current mean train loss 1616.9289855188326
INFO:root:current train perplexity4.1242828369140625
INFO:root:current mean train loss 1616.2658756947358
INFO:root:current train perplexity4.124709606170654
INFO:root:current mean train loss 1615.897779376454
INFO:root:current train perplexity4.124331474304199
INFO:root:current mean train loss 1616.3474705781537
INFO:root:current train perplexity4.127076148986816
INFO:root:current mean train loss 1617.094828901179
INFO:root:current train perplexity4.129637718200684
INFO:root:current mean train loss 1616.9309449940229
INFO:root:current train perplexity4.1321516036987305
INFO:root:current mean train loss 1615.835797478171
INFO:root:current train perplexity4.129445552825928
INFO:root:current mean train loss 1616.1290275032422
INFO:root:current train perplexity4.129432201385498
INFO:root:current mean train loss 1615.3762715529917
INFO:root:current train perplexity4.125901222229004
INFO:root:current mean train loss 1615.815278865089
INFO:root:current train perplexity4.127884387969971
INFO:root:current mean train loss 1615.4419892480141
INFO:root:current train perplexity4.1283159255981445
INFO:root:current mean train loss 1615.0217721489944
INFO:root:current train perplexity4.127936363220215

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:57<00:00, 477.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:57<00:00, 477.16s/it]
INFO:root:final mean train loss: 1615.2657158909815
INFO:root:final train perplexity: 4.128735065460205
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.51s/it]
INFO:root:eval mean loss: 1781.1145577937998
INFO:root:eval perplexity: 4.9914703369140625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.84s/it]
INFO:root:eval mean loss: 2199.701332903923
INFO:root:eval perplexity: 7.544132232666016
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/36
 18%|â–ˆâ–Š        | 36/200 [5:38:49<25:26:10, 558.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1567.1865678267045
INFO:root:current train perplexity4.022153854370117
INFO:root:current mean train loss 1601.8869815860783
INFO:root:current train perplexity4.057469844818115
INFO:root:current mean train loss 1598.273863299763
INFO:root:current train perplexity4.056301593780518
INFO:root:current mean train loss 1598.1242063467141
INFO:root:current train perplexity4.064209938049316
INFO:root:current mean train loss 1602.5194427415981
INFO:root:current train perplexity4.084918022155762
INFO:root:current mean train loss 1601.8248190683862
INFO:root:current train perplexity4.07673978805542
INFO:root:current mean train loss 1602.5049359560403
INFO:root:current train perplexity4.080654144287109
INFO:root:current mean train loss 1602.5031491050238
INFO:root:current train perplexity4.084407806396484
INFO:root:current mean train loss 1606.3773741245857
INFO:root:current train perplexity4.0901265144348145
INFO:root:current mean train loss 1608.0852915055143
INFO:root:current train perplexity4.090914249420166
INFO:root:current mean train loss 1607.4333648228858
INFO:root:current train perplexity4.090837001800537
INFO:root:current mean train loss 1607.7399692483896
INFO:root:current train perplexity4.091429710388184
INFO:root:current mean train loss 1609.0311878056293
INFO:root:current train perplexity4.096038818359375
INFO:root:current mean train loss 1610.7551242528664
INFO:root:current train perplexity4.102077007293701
INFO:root:current mean train loss 1610.2617644290397
INFO:root:current train perplexity4.104963779449463
INFO:root:current mean train loss 1610.3729819216371
INFO:root:current train perplexity4.10580587387085
INFO:root:current mean train loss 1610.0090296417936
INFO:root:current train perplexity4.1047563552856445
INFO:root:current mean train loss 1608.7104898850728
INFO:root:current train perplexity4.103365898132324
INFO:root:current mean train loss 1609.1104254382938
INFO:root:current train perplexity4.104002475738525
INFO:root:current mean train loss 1609.6453253777472
INFO:root:current train perplexity4.106375217437744

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:02<00:00, 482.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:02<00:00, 482.59s/it]
INFO:root:final mean train loss: 1608.4861691196459
INFO:root:final train perplexity: 4.104236602783203
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.01s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.01s/it]
INFO:root:eval mean loss: 1784.619617218667
INFO:root:eval perplexity: 5.007287979125977
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.45s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.45s/it]
INFO:root:eval mean loss: 2205.509230160544
INFO:root:eval perplexity: 7.5844902992248535
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/37
 18%|â–ˆâ–Š        | 37/200 [5:48:13<25:20:57, 559.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1559.6661028180804
INFO:root:current train perplexity3.9702975749969482
INFO:root:current mean train loss 1577.3163528442383
INFO:root:current train perplexity4.009413719177246
INFO:root:current mean train loss 1592.408875582511
INFO:root:current train perplexity4.038864612579346
INFO:root:current mean train loss 1593.0556789491234
INFO:root:current train perplexity4.039442539215088
INFO:root:current mean train loss 1593.009601058247
INFO:root:current train perplexity4.051204204559326
INFO:root:current mean train loss 1592.9157573815548
INFO:root:current train perplexity4.050527095794678
INFO:root:current mean train loss 1593.0962112329569
INFO:root:current train perplexity4.054464817047119
INFO:root:current mean train loss 1596.0220960679944
INFO:root:current train perplexity4.0592780113220215
INFO:root:current mean train loss 1597.3939782478958
INFO:root:current train perplexity4.062363147735596
INFO:root:current mean train loss 1597.164753880994
INFO:root:current train perplexity4.063409328460693
INFO:root:current mean train loss 1597.337765704797
INFO:root:current train perplexity4.0640482902526855
INFO:root:current mean train loss 1600.087666829427
INFO:root:current train perplexity4.069633483886719
INFO:root:current mean train loss 1601.0023507481678
INFO:root:current train perplexity4.07305383682251
INFO:root:current mean train loss 1601.4346828230891
INFO:root:current train perplexity4.07447624206543
INFO:root:current mean train loss 1601.0602864925268
INFO:root:current train perplexity4.073608875274658
INFO:root:current mean train loss 1599.5507802114437
INFO:root:current train perplexity4.073039531707764
INFO:root:current mean train loss 1600.785618437596
INFO:root:current train perplexity4.0778326988220215
INFO:root:current mean train loss 1600.9342914863869
INFO:root:current train perplexity4.07805061340332
INFO:root:current mean train loss 1601.03325815013
INFO:root:current train perplexity4.077384948730469
INFO:root:current mean train loss 1601.945194735072
INFO:root:current train perplexity4.079061508178711

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:03<00:00, 483.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:03<00:00, 483.15s/it]
INFO:root:final mean train loss: 1601.3504602968
INFO:root:final train perplexity: 4.078606605529785
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.83s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.83s/it]
INFO:root:eval mean loss: 1783.7587488052693
INFO:root:eval perplexity: 5.0033979415893555
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.04s/it]
INFO:root:eval mean loss: 2204.176527956699
INFO:root:eval perplexity: 7.575212001800537
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/38
 19%|â–ˆâ–‰        | 38/200 [5:57:37<25:15:15, 561.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1578.0379177517361
INFO:root:current train perplexity4.023015022277832
INFO:root:current mean train loss 1575.7819664264548
INFO:root:current train perplexity4.001970291137695
INFO:root:current mean train loss 1590.5740418726084
INFO:root:current train perplexity4.026776313781738
INFO:root:current mean train loss 1590.078172766644
INFO:root:current train perplexity4.033888339996338
INFO:root:current mean train loss 1588.3388696563377
INFO:root:current train perplexity4.039116859436035
INFO:root:current mean train loss 1587.496094645929
INFO:root:current train perplexity4.034915924072266
INFO:root:current mean train loss 1588.0591090949006
INFO:root:current train perplexity4.039318561553955
INFO:root:current mean train loss 1588.2515217006608
INFO:root:current train perplexity4.0427703857421875
INFO:root:current mean train loss 1587.5782953205899
INFO:root:current train perplexity4.041257858276367
INFO:root:current mean train loss 1588.7905263103505
INFO:root:current train perplexity4.0392022132873535
INFO:root:current mean train loss 1590.5921433444228
INFO:root:current train perplexity4.038383483886719
INFO:root:current mean train loss 1591.5434167320552
INFO:root:current train perplexity4.042110919952393
INFO:root:current mean train loss 1591.1316587639622
INFO:root:current train perplexity4.045968055725098
INFO:root:current mean train loss 1591.3422002062036
INFO:root:current train perplexity4.0453996658325195
INFO:root:current mean train loss 1592.5357536764707
INFO:root:current train perplexity4.048506736755371
INFO:root:current mean train loss 1592.611498707398
INFO:root:current train perplexity4.047328948974609
INFO:root:current mean train loss 1594.0180716749383
INFO:root:current train perplexity4.049575328826904
INFO:root:current mean train loss 1594.1149672193992
INFO:root:current train perplexity4.050778388977051
INFO:root:current mean train loss 1594.2589014068851
INFO:root:current train perplexity4.052674293518066
INFO:root:current mean train loss 1594.2092916532777
INFO:root:current train perplexity4.052848815917969

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:58<00:00, 478.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:58<00:00, 478.81s/it]
INFO:root:final mean train loss: 1594.2438595801125
INFO:root:final train perplexity: 4.053241729736328
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.98s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.98s/it]
INFO:root:eval mean loss: 1784.9728410869625
INFO:root:eval perplexity: 5.008884906768799
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.47s/it]
INFO:root:eval mean loss: 2208.231609371537
INFO:root:eval perplexity: 7.603482723236084
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/39
 20%|â–ˆâ–‰        | 39/200 [6:06:58<25:05:22, 561.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1580.10254890688
INFO:root:current train perplexity3.9988858699798584
INFO:root:current mean train loss 1572.8802136079764
INFO:root:current train perplexity3.9987666606903076
INFO:root:current mean train loss 1585.6733174797232
INFO:root:current train perplexity4.0191192626953125
INFO:root:current mean train loss 1587.0402238540228
INFO:root:current train perplexity4.028428554534912
INFO:root:current mean train loss 1584.0461822113434
INFO:root:current train perplexity4.022402763366699
INFO:root:current mean train loss 1584.9868802650967
INFO:root:current train perplexity4.016988277435303
INFO:root:current mean train loss 1585.5135885278985
INFO:root:current train perplexity4.019617557525635
INFO:root:current mean train loss 1587.1126827530347
INFO:root:current train perplexity4.0245537757873535
INFO:root:current mean train loss 1587.8837829731456
INFO:root:current train perplexity4.025010585784912
INFO:root:current mean train loss 1588.3657018459264
INFO:root:current train perplexity4.025832653045654
INFO:root:current mean train loss 1589.1181864765404
INFO:root:current train perplexity4.027214527130127
INFO:root:current mean train loss 1589.7668706004263
INFO:root:current train perplexity4.026570796966553
INFO:root:current mean train loss 1588.297151157482
INFO:root:current train perplexity4.0239176750183105
INFO:root:current mean train loss 1588.2895623429756
INFO:root:current train perplexity4.02503776550293
INFO:root:current mean train loss 1588.153955462204
INFO:root:current train perplexity4.025091171264648
INFO:root:current mean train loss 1587.593046024728
INFO:root:current train perplexity4.026166915893555
INFO:root:current mean train loss 1588.0192304076413
INFO:root:current train perplexity4.025684833526611
INFO:root:current mean train loss 1588.104813921059
INFO:root:current train perplexity4.027410507202148
INFO:root:current mean train loss 1588.663728806181
INFO:root:current train perplexity4.0291056632995605
INFO:root:current mean train loss 1588.4781363484328
INFO:root:current train perplexity4.030492782592773

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:00<00:00, 480.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:00<00:00, 480.39s/it]
INFO:root:final mean train loss: 1588.008884755037
INFO:root:final train perplexity: 4.0311174392700195
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.83s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.83s/it]
INFO:root:eval mean loss: 1784.2995505907857
INFO:root:eval perplexity: 5.005841255187988
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.98s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.98s/it]
INFO:root:eval mean loss: 2209.85972519462
INFO:root:eval perplexity: 7.614861488342285
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/40
 20%|â–ˆâ–ˆ        | 40/200 [6:16:18<24:55:35, 560.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1573.2959556096716
INFO:root:current train perplexity3.9326133728027344
INFO:root:current mean train loss 1558.9560089963775
INFO:root:current train perplexity3.929971218109131
INFO:root:current mean train loss 1558.4552885759688
INFO:root:current train perplexity3.939822196960449
INFO:root:current mean train loss 1563.2823434794484
INFO:root:current train perplexity3.9514012336730957
INFO:root:current mean train loss 1569.5585822820165
INFO:root:current train perplexity3.9697248935699463
INFO:root:current mean train loss 1571.2682673268162
INFO:root:current train perplexity3.97312593460083
INFO:root:current mean train loss 1575.5900107651994
INFO:root:current train perplexity3.984724283218384
INFO:root:current mean train loss 1577.7825147361903
INFO:root:current train perplexity3.993009090423584
INFO:root:current mean train loss 1578.5183555420756
INFO:root:current train perplexity3.9959800243377686
INFO:root:current mean train loss 1579.2813566089042
INFO:root:current train perplexity3.9943935871124268
INFO:root:current mean train loss 1580.5605448386093
INFO:root:current train perplexity3.9962313175201416
INFO:root:current mean train loss 1580.907249754824
INFO:root:current train perplexity3.9999265670776367
INFO:root:current mean train loss 1580.7259661784108
INFO:root:current train perplexity3.997835159301758
INFO:root:current mean train loss 1580.2117550789749
INFO:root:current train perplexity4.000141143798828
INFO:root:current mean train loss 1581.9085621553309
INFO:root:current train perplexity4.003169536590576
INFO:root:current mean train loss 1581.4955888473964
INFO:root:current train perplexity4.005302429199219
INFO:root:current mean train loss 1582.2951817924313
INFO:root:current train perplexity4.007694244384766
INFO:root:current mean train loss 1582.7072882220743
INFO:root:current train perplexity4.010020732879639
INFO:root:current mean train loss 1582.9508181374526
INFO:root:current train perplexity4.011476993560791
INFO:root:current mean train loss 1582.6127868004673
INFO:root:current train perplexity4.010990142822266

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:59<00:00, 479.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:59<00:00, 479.89s/it]
INFO:root:final mean train loss: 1582.3575330186961
INFO:root:final train perplexity: 4.011168003082275
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.92s/it]
INFO:root:eval mean loss: 1784.1517559078568
INFO:root:eval perplexity: 5.005173683166504
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.26s/it]
INFO:root:eval mean loss: 2209.3443603515625
INFO:root:eval perplexity: 7.611260414123535
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/41
 20%|â–ˆâ–ˆ        | 41/200 [6:25:38<24:45:50, 560.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1570.6377080281575
INFO:root:current train perplexity3.956866502761841
INFO:root:current mean train loss 1575.9873196348851
INFO:root:current train perplexity3.9831042289733887
INFO:root:current mean train loss 1569.3485610549515
INFO:root:current train perplexity3.97117280960083
INFO:root:current mean train loss 1567.5583030623618
INFO:root:current train perplexity3.965745210647583
INFO:root:current mean train loss 1565.735831968246
INFO:root:current train perplexity3.963104248046875
INFO:root:current mean train loss 1567.3597938486394
INFO:root:current train perplexity3.9628217220306396
INFO:root:current mean train loss 1571.2357086532418
INFO:root:current train perplexity3.96700382232666
INFO:root:current mean train loss 1571.1845508364577
INFO:root:current train perplexity3.9654836654663086
INFO:root:current mean train loss 1571.0028547559466
INFO:root:current train perplexity3.9675965309143066
INFO:root:current mean train loss 1571.7990991063864
INFO:root:current train perplexity3.970980167388916
INFO:root:current mean train loss 1572.604213742444
INFO:root:current train perplexity3.9741718769073486
INFO:root:current mean train loss 1573.1748723569124
INFO:root:current train perplexity3.974287509918213
INFO:root:current mean train loss 1572.6276322353033
INFO:root:current train perplexity3.9748106002807617
INFO:root:current mean train loss 1573.6168597639462
INFO:root:current train perplexity3.97879958152771
INFO:root:current mean train loss 1572.8350441672585
INFO:root:current train perplexity3.977389097213745
INFO:root:current mean train loss 1573.5793971011512
INFO:root:current train perplexity3.9782772064208984
INFO:root:current mean train loss 1574.5382569510982
INFO:root:current train perplexity3.9811575412750244
INFO:root:current mean train loss 1575.2251948503183
INFO:root:current train perplexity3.9849014282226562
INFO:root:current mean train loss 1575.4278732492953
INFO:root:current train perplexity3.986748218536377

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:58<00:00, 478.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:58<00:00, 478.67s/it]
INFO:root:final mean train loss: 1575.8358855447082
INFO:root:final train perplexity: 3.988269329071045
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.02s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.02s/it]
INFO:root:eval mean loss: 1786.314844875471
INFO:root:eval perplexity: 5.014955997467041
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.01s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.01s/it]
INFO:root:eval mean loss: 2215.093830081588
INFO:root:eval perplexity: 7.651566982269287
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/42
 21%|â–ˆâ–ˆ        | 42/200 [6:34:58<24:35:57, 560.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1578.5576171875
INFO:root:current train perplexity4.067263603210449
INFO:root:current mean train loss 1567.315219035191
INFO:root:current train perplexity3.9517791271209717
INFO:root:current mean train loss 1566.784413512324
INFO:root:current train perplexity3.946521043777466
INFO:root:current mean train loss 1565.3606904733676
INFO:root:current train perplexity3.9453699588775635
INFO:root:current mean train loss 1564.404859048691
INFO:root:current train perplexity3.9452505111694336
INFO:root:current mean train loss 1560.2334964744762
INFO:root:current train perplexity3.943756103515625
INFO:root:current mean train loss 1562.3981814112203
INFO:root:current train perplexity3.950644016265869
INFO:root:current mean train loss 1564.197243710554
INFO:root:current train perplexity3.9521117210388184
INFO:root:current mean train loss 1565.622121663111
INFO:root:current train perplexity3.954540252685547
INFO:root:current mean train loss 1563.4443621431749
INFO:root:current train perplexity3.953778028488159
INFO:root:current mean train loss 1563.3625184611765
INFO:root:current train perplexity3.9524905681610107
INFO:root:current mean train loss 1563.2788910707266
INFO:root:current train perplexity3.949174404144287
INFO:root:current mean train loss 1565.13591226073
INFO:root:current train perplexity3.950150489807129
INFO:root:current mean train loss 1565.9266878986577
INFO:root:current train perplexity3.9522526264190674
INFO:root:current mean train loss 1566.339358233258
INFO:root:current train perplexity3.9526569843292236
INFO:root:current mean train loss 1567.1317726836169
INFO:root:current train perplexity3.9561140537261963
INFO:root:current mean train loss 1568.5991114825104
INFO:root:current train perplexity3.959745168685913
INFO:root:current mean train loss 1569.2640418627773
INFO:root:current train perplexity3.9623725414276123
INFO:root:current mean train loss 1569.875131563922
INFO:root:current train perplexity3.9638164043426514
INFO:root:current mean train loss 1569.067575585325
INFO:root:current train perplexity3.9642558097839355

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:02<00:00, 482.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:02<00:00, 482.66s/it]
INFO:root:final mean train loss: 1569.2481584188256
INFO:root:final train perplexity: 3.965271472930908
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.76s/it]
INFO:root:eval mean loss: 1788.876981261774
INFO:root:eval perplexity: 5.026566982269287
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.82s/it]
INFO:root:eval mean loss: 2220.882401270224
INFO:root:eval perplexity: 7.692365646362305
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/43
 22%|â–ˆâ–ˆâ–       | 43/200 [6:44:21<24:28:12, 561.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1545.676407877604
INFO:root:current train perplexity3.93342924118042
INFO:root:current mean train loss 1564.340196814904
INFO:root:current train perplexity3.9363327026367188
INFO:root:current mean train loss 1559.0817287279212
INFO:root:current train perplexity3.929750442504883
INFO:root:current mean train loss 1562.1340195164537
INFO:root:current train perplexity3.9375369548797607
INFO:root:current mean train loss 1557.496552223383
INFO:root:current train perplexity3.9298713207244873
INFO:root:current mean train loss 1560.9745983195755
INFO:root:current train perplexity3.9303736686706543
INFO:root:current mean train loss 1562.1874476841517
INFO:root:current train perplexity3.929635763168335
INFO:root:current mean train loss 1561.8067548359911
INFO:root:current train perplexity3.9274771213531494
INFO:root:current mean train loss 1562.8267557534828
INFO:root:current train perplexity3.931018352508545
INFO:root:current mean train loss 1563.759970125588
INFO:root:current train perplexity3.9361155033111572
INFO:root:current mean train loss 1564.8008898096177
INFO:root:current train perplexity3.9414169788360596
INFO:root:current mean train loss 1563.1807027361035
INFO:root:current train perplexity3.9390671253204346
INFO:root:current mean train loss 1562.9661546660632
INFO:root:current train perplexity3.9399795532226562
INFO:root:current mean train loss 1563.2851824079241
INFO:root:current train perplexity3.9421966075897217
INFO:root:current mean train loss 1564.0288818359375
INFO:root:current train perplexity3.944188356399536
INFO:root:current mean train loss 1564.1872633591197
INFO:root:current train perplexity3.9456093311309814
INFO:root:current mean train loss 1563.3105590820312
INFO:root:current train perplexity3.9449594020843506
INFO:root:current mean train loss 1563.5365272477873
INFO:root:current train perplexity3.9439945220947266
INFO:root:current mean train loss 1562.664526700713
INFO:root:current train perplexity3.9426891803741455
INFO:root:current mean train loss 1563.0458061574036
INFO:root:current train perplexity3.942949056625366

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:04<00:00, 484.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:04<00:00, 484.82s/it]
INFO:root:final mean train loss: 1563.3816835804535
INFO:root:final train perplexity: 3.9449033737182617
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.48s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.48s/it]
INFO:root:eval mean loss: 1788.3628990227448
INFO:root:eval perplexity: 5.024235725402832
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.02s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.03s/it]
INFO:root:eval mean loss: 2221.26582585328
INFO:root:eval perplexity: 7.695073127746582
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/44
 22%|â–ˆâ–ˆâ–       | 44/200 [6:53:50<24:24:52, 563.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1540.3114455202792
INFO:root:current train perplexity3.8909614086151123
INFO:root:current mean train loss 1545.5000124561543
INFO:root:current train perplexity3.896649122238159
INFO:root:current mean train loss 1547.015575084609
INFO:root:current train perplexity3.9097983837127686
INFO:root:current mean train loss 1552.9407635339742
INFO:root:current train perplexity3.9232263565063477
INFO:root:current mean train loss 1552.383389534833
INFO:root:current train perplexity3.9205446243286133
INFO:root:current mean train loss 1553.4238790062273
INFO:root:current train perplexity3.9179704189300537
INFO:root:current mean train loss 1555.152864105366
INFO:root:current train perplexity3.918365001678467
INFO:root:current mean train loss 1555.2918925049155
INFO:root:current train perplexity3.9200100898742676
INFO:root:current mean train loss 1556.54187054955
INFO:root:current train perplexity3.9171857833862305
INFO:root:current mean train loss 1555.73759523289
INFO:root:current train perplexity3.918452024459839
INFO:root:current mean train loss 1556.5399529020788
INFO:root:current train perplexity3.9205358028411865
INFO:root:current mean train loss 1556.3719008827375
INFO:root:current train perplexity3.919145107269287
INFO:root:current mean train loss 1555.8724664781412
INFO:root:current train perplexity3.916419506072998
INFO:root:current mean train loss 1556.4192944861556
INFO:root:current train perplexity3.91549015045166
INFO:root:current mean train loss 1556.5557773592725
INFO:root:current train perplexity3.9151012897491455
INFO:root:current mean train loss 1556.6796009381817
INFO:root:current train perplexity3.9158434867858887
INFO:root:current mean train loss 1556.841775974072
INFO:root:current train perplexity3.919348955154419
INFO:root:current mean train loss 1557.0698026974815
INFO:root:current train perplexity3.918942928314209
INFO:root:current mean train loss 1557.3495437465103
INFO:root:current train perplexity3.9207091331481934
INFO:root:current mean train loss 1557.3773571419972
INFO:root:current train perplexity3.921680212020874

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:16<00:00, 496.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:16<00:00, 496.58s/it]
INFO:root:final mean train loss: 1556.8806588485033
INFO:root:final train perplexity: 3.9224538803100586
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.47s/it]
INFO:root:eval mean loss: 1790.9653350613642
INFO:root:eval perplexity: 5.036051273345947
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.96s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.96s/it]
INFO:root:eval mean loss: 2225.753388100482
INFO:root:eval perplexity: 7.726861476898193
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/45
 22%|â–ˆâ–ˆâ–Ž       | 45/200 [7:03:24<24:23:54, 566.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1543.1313133239746
INFO:root:current train perplexity3.836346387863159
INFO:root:current mean train loss 1541.3736066120427
INFO:root:current train perplexity3.844998359680176
INFO:root:current mean train loss 1544.040104259144
INFO:root:current train perplexity3.860250473022461
INFO:root:current mean train loss 1546.272004850618
INFO:root:current train perplexity3.8656005859375
INFO:root:current mean train loss 1548.0965570910223
INFO:root:current train perplexity3.875483989715576
INFO:root:current mean train loss 1549.2660749719498
INFO:root:current train perplexity3.8758857250213623
INFO:root:current mean train loss 1551.3985062564711
INFO:root:current train perplexity3.8794827461242676
INFO:root:current mean train loss 1550.9887906219324
INFO:root:current train perplexity3.885633707046509
INFO:root:current mean train loss 1550.6202319109882
INFO:root:current train perplexity3.8911821842193604
INFO:root:current mean train loss 1550.0814921905392
INFO:root:current train perplexity3.892528533935547
INFO:root:current mean train loss 1550.7589920158673
INFO:root:current train perplexity3.893588066101074
INFO:root:current mean train loss 1551.2145497954589
INFO:root:current train perplexity3.896313190460205
INFO:root:current mean train loss 1550.6808360618882
INFO:root:current train perplexity3.899045705795288
INFO:root:current mean train loss 1550.704051624645
INFO:root:current train perplexity3.8977882862091064
INFO:root:current mean train loss 1551.4706903676517
INFO:root:current train perplexity3.8986470699310303
INFO:root:current mean train loss 1552.1644049837157
INFO:root:current train perplexity3.900257110595703
INFO:root:current mean train loss 1551.6843614578247
INFO:root:current train perplexity3.902907371520996
INFO:root:current mean train loss 1551.4830683494101
INFO:root:current train perplexity3.9021337032318115
INFO:root:current mean train loss 1551.3258744268458
INFO:root:current train perplexity3.9024858474731445
INFO:root:current mean train loss 1551.5680358638103
INFO:root:current train perplexity3.9020183086395264

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.16s/it]
INFO:root:final mean train loss: 1551.0346781566657
INFO:root:final train perplexity: 3.9023756980895996
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.09s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.09s/it]
INFO:root:eval mean loss: 1791.578536229776
INFO:root:eval perplexity: 5.038839817047119
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.33s/it]
INFO:root:eval mean loss: 2227.8635496315383
INFO:root:eval perplexity: 7.741855144500732
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/46
 23%|â–ˆâ–ˆâ–Ž       | 46/200 [7:12:29<23:57:38, 560.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1555.265436619888
INFO:root:current train perplexity3.8631246089935303
INFO:root:current mean train loss 1541.0773561593578
INFO:root:current train perplexity3.854668378829956
INFO:root:current mean train loss 1542.846566305466
INFO:root:current train perplexity3.8596913814544678
INFO:root:current mean train loss 1544.9534588510908
INFO:root:current train perplexity3.8575000762939453
INFO:root:current mean train loss 1540.7081770867171
INFO:root:current train perplexity3.860793113708496
INFO:root:current mean train loss 1541.1961940955787
INFO:root:current train perplexity3.8683784008026123
INFO:root:current mean train loss 1542.5641801248737
INFO:root:current train perplexity3.8722453117370605
INFO:root:current mean train loss 1542.201922583977
INFO:root:current train perplexity3.875804901123047
INFO:root:current mean train loss 1541.7940262308457
INFO:root:current train perplexity3.8731625080108643
INFO:root:current mean train loss 1541.8286448876306
INFO:root:current train perplexity3.8767151832580566
INFO:root:current mean train loss 1541.9320582161338
INFO:root:current train perplexity3.8771746158599854
INFO:root:current mean train loss 1543.2294990093803
INFO:root:current train perplexity3.877235174179077
INFO:root:current mean train loss 1544.9363532442305
INFO:root:current train perplexity3.880500316619873
INFO:root:current mean train loss 1545.7051659873393
INFO:root:current train perplexity3.8830907344818115
INFO:root:current mean train loss 1545.6309814453125
INFO:root:current train perplexity3.88279128074646
INFO:root:current mean train loss 1546.0574748107408
INFO:root:current train perplexity3.8843295574188232
INFO:root:current mean train loss 1546.5118265872482
INFO:root:current train perplexity3.885934829711914
INFO:root:current mean train loss 1546.0677558774696
INFO:root:current train perplexity3.884599447250366
INFO:root:current mean train loss 1545.9131172825084
INFO:root:current train perplexity3.884532928466797
INFO:root:current mean train loss 1546.4683208744793
INFO:root:current train perplexity3.885136842727661

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.52s/it]
INFO:root:final mean train loss: 1545.8743145164071
INFO:root:final train perplexity: 3.884737968444824
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.89s/it]
INFO:root:eval mean loss: 1795.3605294734873
INFO:root:eval perplexity: 5.0560712814331055
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.79s/it]
INFO:root:eval mean loss: 2234.8244481729275
INFO:root:eval perplexity: 7.791522026062012
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/47
 24%|â–ˆâ–ˆâ–Ž       | 47/200 [7:21:40<23:41:44, 557.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1517.2784286810427
INFO:root:current train perplexity3.8352301120758057
INFO:root:current mean train loss 1522.4716766049164
INFO:root:current train perplexity3.827672004699707
INFO:root:current mean train loss 1525.191411575215
INFO:root:current train perplexity3.828538656234741
INFO:root:current mean train loss 1529.0156498434555
INFO:root:current train perplexity3.8319380283355713
INFO:root:current mean train loss 1528.1679275696536
INFO:root:current train perplexity3.8334200382232666
INFO:root:current mean train loss 1531.5280992386731
INFO:root:current train perplexity3.832671642303467
INFO:root:current mean train loss 1532.5100945852548
INFO:root:current train perplexity3.8364484310150146
INFO:root:current mean train loss 1533.4642569558662
INFO:root:current train perplexity3.841944456100464
INFO:root:current mean train loss 1533.9998780656233
INFO:root:current train perplexity3.846431255340576
INFO:root:current mean train loss 1535.3452353926602
INFO:root:current train perplexity3.849226713180542
INFO:root:current mean train loss 1535.5046331131175
INFO:root:current train perplexity3.848781108856201
INFO:root:current mean train loss 1535.0028325814835
INFO:root:current train perplexity3.8513011932373047
INFO:root:current mean train loss 1535.935605559033
INFO:root:current train perplexity3.852569103240967
INFO:root:current mean train loss 1536.3131237985067
INFO:root:current train perplexity3.853863477706909
INFO:root:current mean train loss 1536.6878515429428
INFO:root:current train perplexity3.856828451156616
INFO:root:current mean train loss 1538.1915099104594
INFO:root:current train perplexity3.857717275619507
INFO:root:current mean train loss 1539.184800146887
INFO:root:current train perplexity3.8621437549591064
INFO:root:current mean train loss 1539.7126943484297
INFO:root:current train perplexity3.861835241317749
INFO:root:current mean train loss 1540.4698047055083
INFO:root:current train perplexity3.8637733459472656

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:16<00:00, 496.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:16<00:00, 496.26s/it]
INFO:root:final mean train loss: 1539.8190200197776
INFO:root:final train perplexity: 3.8641419410705566
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.49s/it]
INFO:root:eval mean loss: 1795.3440486826796
INFO:root:eval perplexity: 5.055995941162109
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.97s/it]
INFO:root:eval mean loss: 2234.274610154172
INFO:root:eval perplexity: 7.787588596343994
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/48
 24%|â–ˆâ–ˆâ–       | 48/200 [7:31:22<23:51:07, 564.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1515.8342447916666
INFO:root:current train perplexity3.8161187171936035
INFO:root:current mean train loss 1516.1774180536686
INFO:root:current train perplexity3.8108725547790527
INFO:root:current mean train loss 1527.955674850109
INFO:root:current train perplexity3.822012186050415
INFO:root:current mean train loss 1528.300013563368
INFO:root:current train perplexity3.8300156593322754
INFO:root:current mean train loss 1526.6898658108998
INFO:root:current train perplexity3.830265998840332
INFO:root:current mean train loss 1525.7916783601336
INFO:root:current train perplexity3.8284454345703125
INFO:root:current mean train loss 1529.339592066819
INFO:root:current train perplexity3.8330729007720947
INFO:root:current mean train loss 1530.151091291521
INFO:root:current train perplexity3.8259799480438232
INFO:root:current mean train loss 1528.6341467360046
INFO:root:current train perplexity3.827277421951294
INFO:root:current mean train loss 1530.705394440531
INFO:root:current train perplexity3.8297271728515625
INFO:root:current mean train loss 1529.8046549078279
INFO:root:current train perplexity3.827483892440796
INFO:root:current mean train loss 1532.529182906215
INFO:root:current train perplexity3.83156156539917
INFO:root:current mean train loss 1533.3425919897763
INFO:root:current train perplexity3.833592414855957
INFO:root:current mean train loss 1531.769290636882
INFO:root:current train perplexity3.8341805934906006
INFO:root:current mean train loss 1532.918583069926
INFO:root:current train perplexity3.836617946624756
INFO:root:current mean train loss 1534.152406839805
INFO:root:current train perplexity3.8393406867980957
INFO:root:current mean train loss 1533.6223804391207
INFO:root:current train perplexity3.841484785079956
INFO:root:current mean train loss 1533.13342185507
INFO:root:current train perplexity3.839895009994507
INFO:root:current mean train loss 1532.7732078194947
INFO:root:current train perplexity3.8399651050567627
INFO:root:current mean train loss 1534.0716715919752
INFO:root:current train perplexity3.842424154281616

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:13<00:00, 493.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:13<00:00, 493.26s/it]
INFO:root:final mean train loss: 1533.940900363047
INFO:root:final train perplexity: 3.844254493713379
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.54s/it]
INFO:root:eval mean loss: 1796.4694122963763
INFO:root:eval perplexity: 5.061134338378906
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.66s/it]
INFO:root:eval mean loss: 2239.647990774601
INFO:root:eval perplexity: 7.826124668121338
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/49
 24%|â–ˆâ–ˆâ–       | 49/200 [7:40:57<23:49:15, 567.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1545.1508903503418
INFO:root:current train perplexity3.7936160564422607
INFO:root:current mean train loss 1524.3236990263968
INFO:root:current train perplexity3.7944066524505615
INFO:root:current mean train loss 1519.5444235966124
INFO:root:current train perplexity3.8089869022369385
INFO:root:current mean train loss 1521.0183112822383
INFO:root:current train perplexity3.8011057376861572
INFO:root:current mean train loss 1521.7543193675854
INFO:root:current train perplexity3.801179885864258
INFO:root:current mean train loss 1523.5121439309944
INFO:root:current train perplexity3.8085038661956787
INFO:root:current mean train loss 1529.804205978973
INFO:root:current train perplexity3.815873622894287
INFO:root:current mean train loss 1528.165303548177
INFO:root:current train perplexity3.814085006713867
INFO:root:current mean train loss 1527.3312775538518
INFO:root:current train perplexity3.813324213027954
INFO:root:current mean train loss 1527.811907330296
INFO:root:current train perplexity3.816312551498413
INFO:root:current mean train loss 1527.5418597080911
INFO:root:current train perplexity3.8204939365386963
INFO:root:current mean train loss 1527.3800517914574
INFO:root:current train perplexity3.819124221801758
INFO:root:current mean train loss 1526.4818044885412
INFO:root:current train perplexity3.8180735111236572
INFO:root:current mean train loss 1527.5239230319187
INFO:root:current train perplexity3.8203089237213135
INFO:root:current mean train loss 1527.737325316701
INFO:root:current train perplexity3.821035146713257
INFO:root:current mean train loss 1528.9868237368432
INFO:root:current train perplexity3.822864294052124
INFO:root:current mean train loss 1529.1271456550148
INFO:root:current train perplexity3.824336051940918
INFO:root:current mean train loss 1529.2206374382038
INFO:root:current train perplexity3.82499361038208
INFO:root:current mean train loss 1529.3512843102867
INFO:root:current train perplexity3.825140953063965
INFO:root:current mean train loss 1529.1452045954038
INFO:root:current train perplexity3.8259575366973877

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:07<00:00, 487.07s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:07<00:00, 487.07s/it]
INFO:root:final mean train loss: 1528.5686051161435
INFO:root:final train perplexity: 3.826166868209839
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.14s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.14s/it]
INFO:root:eval mean loss: 1798.5348601645612
INFO:root:eval perplexity: 5.0705790519714355
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.37s/it]
INFO:root:eval mean loss: 2243.1522342330177
INFO:root:eval perplexity: 7.851358413696289
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/50
 25%|â–ˆâ–ˆâ–Œ       | 50/200 [7:50:22<23:37:32, 567.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1514.0389902543047
INFO:root:current train perplexity3.762173891067505
INFO:root:current mean train loss 1522.7081560992553
INFO:root:current train perplexity3.809013605117798
INFO:root:current mean train loss 1519.1864959839359
INFO:root:current train perplexity3.7900187969207764
INFO:root:current mean train loss 1515.6446753559276
INFO:root:current train perplexity3.7866201400756836
INFO:root:current mean train loss 1520.088974304879
INFO:root:current train perplexity3.7959413528442383
INFO:root:current mean train loss 1520.5130515176743
INFO:root:current train perplexity3.79716420173645
INFO:root:current mean train loss 1522.4252788620113
INFO:root:current train perplexity3.800752639770508
INFO:root:current mean train loss 1521.217001412039
INFO:root:current train perplexity3.801933526992798
INFO:root:current mean train loss 1522.915355956456
INFO:root:current train perplexity3.804164171218872
INFO:root:current mean train loss 1523.8059370163494
INFO:root:current train perplexity3.801325798034668
INFO:root:current mean train loss 1522.596351994459
INFO:root:current train perplexity3.8018720149993896
INFO:root:current mean train loss 1521.315365412009
INFO:root:current train perplexity3.8006575107574463
INFO:root:current mean train loss 1520.972635237096
INFO:root:current train perplexity3.8006036281585693
INFO:root:current mean train loss 1522.4035297051635
INFO:root:current train perplexity3.8012125492095947
INFO:root:current mean train loss 1522.7173777409798
INFO:root:current train perplexity3.802198648452759
INFO:root:current mean train loss 1523.3861384234788
INFO:root:current train perplexity3.8031632900238037
INFO:root:current mean train loss 1522.738880867666
INFO:root:current train perplexity3.802788019180298
INFO:root:current mean train loss 1522.4475197462166
INFO:root:current train perplexity3.803906202316284
INFO:root:current mean train loss 1522.4862439499602
INFO:root:current train perplexity3.8068652153015137
INFO:root:current mean train loss 1523.5506242308757
INFO:root:current train perplexity3.8074443340301514

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:22<00:00, 502.23s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:22<00:00, 502.23s/it]
INFO:root:final mean train loss: 1523.2040009452908
INFO:root:final train perplexity: 3.8081905841827393
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.38s/it]
INFO:root:eval mean loss: 1798.6370451365801
INFO:root:eval perplexity: 5.071046829223633
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.35s/it]
INFO:root:eval mean loss: 2243.8948494119845
INFO:root:eval perplexity: 7.856716156005859
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/51
 26%|â–ˆâ–ˆâ–Œ       | 51/200 [8:00:11<23:44:02, 573.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1527.2227579752605
INFO:root:current train perplexity3.7873599529266357
INFO:root:current mean train loss 1511.0272378576808
INFO:root:current train perplexity3.7860476970672607
INFO:root:current mean train loss 1503.5381584454299
INFO:root:current train perplexity3.776244878768921
INFO:root:current mean train loss 1503.4125362875684
INFO:root:current train perplexity3.7822842597961426
INFO:root:current mean train loss 1506.8962242552138
INFO:root:current train perplexity3.7813217639923096
INFO:root:current mean train loss 1508.7062850251214
INFO:root:current train perplexity3.786999225616455
INFO:root:current mean train loss 1509.8956993492516
INFO:root:current train perplexity3.78902006149292
INFO:root:current mean train loss 1510.8019570134015
INFO:root:current train perplexity3.78310227394104
INFO:root:current mean train loss 1511.0528689906448
INFO:root:current train perplexity3.7838189601898193
INFO:root:current mean train loss 1513.4052182152159
INFO:root:current train perplexity3.7855067253112793
INFO:root:current mean train loss 1513.3149304130511
INFO:root:current train perplexity3.7870864868164062
INFO:root:current mean train loss 1514.4533516571412
INFO:root:current train perplexity3.7854692935943604
INFO:root:current mean train loss 1513.3747450601056
INFO:root:current train perplexity3.7825510501861572
INFO:root:current mean train loss 1512.948870411729
INFO:root:current train perplexity3.7824063301086426
INFO:root:current mean train loss 1513.474610207676
INFO:root:current train perplexity3.784454107284546
INFO:root:current mean train loss 1514.822904194604
INFO:root:current train perplexity3.7869179248809814
INFO:root:current mean train loss 1515.693357616484
INFO:root:current train perplexity3.7878801822662354
INFO:root:current mean train loss 1516.0930729452373
INFO:root:current train perplexity3.7888245582580566
INFO:root:current mean train loss 1516.9208083566746
INFO:root:current train perplexity3.7885055541992188
INFO:root:current mean train loss 1517.8193076241416
INFO:root:current train perplexity3.788648843765259

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:59<00:00, 479.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:59<00:00, 479.46s/it]
INFO:root:final mean train loss: 1517.4384373190182
INFO:root:final train perplexity: 3.7889654636383057
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.07s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.07s/it]
INFO:root:eval mean loss: 1800.4146005616965
INFO:root:eval perplexity: 5.079190254211426
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.44s/it]
INFO:root:eval mean loss: 2247.5773969068596
INFO:root:eval perplexity: 7.883341312408447
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/52
 26%|â–ˆâ–ˆâ–Œ       | 52/200 [8:09:35<23:27:49, 570.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1497.4516954536898
INFO:root:current train perplexity3.7134947776794434
INFO:root:current mean train loss 1502.824403523096
INFO:root:current train perplexity3.730647563934326
INFO:root:current mean train loss 1503.782328359651
INFO:root:current train perplexity3.7411980628967285
INFO:root:current mean train loss 1504.5029959815602
INFO:root:current train perplexity3.746523857116699
INFO:root:current mean train loss 1501.8111741597115
INFO:root:current train perplexity3.748378276824951
INFO:root:current mean train loss 1502.9137754358517
INFO:root:current train perplexity3.747781753540039
INFO:root:current mean train loss 1505.4267547741467
INFO:root:current train perplexity3.7498950958251953
INFO:root:current mean train loss 1507.6889285188677
INFO:root:current train perplexity3.754812002182007
INFO:root:current mean train loss 1507.6273151885882
INFO:root:current train perplexity3.758838176727295
INFO:root:current mean train loss 1508.5927310916438
INFO:root:current train perplexity3.7602977752685547
INFO:root:current mean train loss 1509.3863953707512
INFO:root:current train perplexity3.7622854709625244
INFO:root:current mean train loss 1509.9312107476358
INFO:root:current train perplexity3.7626254558563232
INFO:root:current mean train loss 1510.3383098313889
INFO:root:current train perplexity3.764425039291382
INFO:root:current mean train loss 1510.0577915988792
INFO:root:current train perplexity3.7638604640960693
INFO:root:current mean train loss 1510.8519343082276
INFO:root:current train perplexity3.7638843059539795
INFO:root:current mean train loss 1511.3934718678438
INFO:root:current train perplexity3.7666354179382324
INFO:root:current mean train loss 1512.8577874331552
INFO:root:current train perplexity3.7695727348327637
INFO:root:current mean train loss 1512.4830192869724
INFO:root:current train perplexity3.7693357467651367
INFO:root:current mean train loss 1512.5178771097485
INFO:root:current train perplexity3.768524408340454
INFO:root:current mean train loss 1511.9776080386903
INFO:root:current train perplexity3.7708442211151123

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:19<00:00, 499.10s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:19<00:00, 499.10s/it]
INFO:root:final mean train loss: 1511.9776080386903
INFO:root:final train perplexity: 3.7708442211151123
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.89s/it]
INFO:root:eval mean loss: 1804.4259206352503
INFO:root:eval perplexity: 5.097614288330078
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.81s/it]
INFO:root:eval mean loss: 2254.450228210882
INFO:root:eval perplexity: 7.933272838592529
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/53
 26%|â–ˆâ–ˆâ–‹       | 53/200 [8:19:20<23:28:56, 575.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1503.1427868652345
INFO:root:current train perplexity3.712905168533325
INFO:root:current mean train loss 1506.8121295166015
INFO:root:current train perplexity3.7262895107269287
INFO:root:current mean train loss 1505.2964571126302
INFO:root:current train perplexity3.729431390762329
INFO:root:current mean train loss 1504.3106103515624
INFO:root:current train perplexity3.733576536178589
INFO:root:current mean train loss 1504.4892817382813
INFO:root:current train perplexity3.736391305923462
INFO:root:current mean train loss 1501.5934851074219
INFO:root:current train perplexity3.734215259552002
INFO:root:current mean train loss 1505.6466305106028
INFO:root:current train perplexity3.7427282333374023
INFO:root:current mean train loss 1504.1078459167481
INFO:root:current train perplexity3.7422597408294678
INFO:root:current mean train loss 1504.6536741807727
INFO:root:current train perplexity3.739415407180786
INFO:root:current mean train loss 1506.2297940673827
INFO:root:current train perplexity3.7441539764404297
INFO:root:current mean train loss 1505.9013297895951
INFO:root:current train perplexity3.7456467151641846
INFO:root:current mean train loss 1505.0783518473306
INFO:root:current train perplexity3.7476489543914795
INFO:root:current mean train loss 1505.0442244779147
INFO:root:current train perplexity3.7476861476898193
INFO:root:current mean train loss 1505.3339814104352
INFO:root:current train perplexity3.7480967044830322
INFO:root:current mean train loss 1507.2343884277343
INFO:root:current train perplexity3.7506000995635986
INFO:root:current mean train loss 1507.129450149536
INFO:root:current train perplexity3.7499961853027344
INFO:root:current mean train loss 1507.3822934139475
INFO:root:current train perplexity3.7518930435180664
INFO:root:current mean train loss 1507.53947394477
INFO:root:current train perplexity3.753509283065796
INFO:root:current mean train loss 1507.3277486379523
INFO:root:current train perplexity3.7547154426574707

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:16<00:00, 496.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:16<00:00, 496.72s/it]
INFO:root:final mean train loss: 1507.6209019647965
INFO:root:final train perplexity: 3.756450653076172
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.37s/it]
INFO:root:eval mean loss: 1807.2365198879377
INFO:root:eval perplexity: 5.110563278198242
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.93s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.94s/it]
INFO:root:eval mean loss: 2258.6349231909353
INFO:root:eval perplexity: 7.963827133178711
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/54
 27%|â–ˆâ–ˆâ–‹       | 54/200 [8:29:03<23:24:39, 577.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1494.1420108570771
INFO:root:current train perplexity3.819457530975342
INFO:root:current mean train loss 1503.4209704276843
INFO:root:current train perplexity3.736863851547241
INFO:root:current mean train loss 1507.1934471306163
INFO:root:current train perplexity3.7274434566497803
INFO:root:current mean train loss 1501.0390016573838
INFO:root:current train perplexity3.7278056144714355
INFO:root:current mean train loss 1500.824559200296
INFO:root:current train perplexity3.730079412460327
INFO:root:current mean train loss 1502.3735141422117
INFO:root:current train perplexity3.7336671352386475
INFO:root:current mean train loss 1501.5369328973359
INFO:root:current train perplexity3.730619192123413
INFO:root:current mean train loss 1501.3686196554654
INFO:root:current train perplexity3.728038787841797
INFO:root:current mean train loss 1501.0872318636684
INFO:root:current train perplexity3.728982925415039
INFO:root:current mean train loss 1499.8076182524537
INFO:root:current train perplexity3.7296266555786133
INFO:root:current mean train loss 1499.2520607917359
INFO:root:current train perplexity3.729428768157959
INFO:root:current mean train loss 1501.0544040171078
INFO:root:current train perplexity3.7341737747192383
INFO:root:current mean train loss 1500.6054156890343
INFO:root:current train perplexity3.7321977615356445
INFO:root:current mean train loss 1501.6304838025578
INFO:root:current train perplexity3.732022523880005
INFO:root:current mean train loss 1501.691586383397
INFO:root:current train perplexity3.7316155433654785
INFO:root:current mean train loss 1502.3013932913955
INFO:root:current train perplexity3.7320265769958496
INFO:root:current mean train loss 1503.0396641700004
INFO:root:current train perplexity3.7341716289520264
INFO:root:current mean train loss 1503.0181978611176
INFO:root:current train perplexity3.736248731613159
INFO:root:current mean train loss 1502.9080476650643
INFO:root:current train perplexity3.736271858215332
INFO:root:current mean train loss 1502.2181797017638
INFO:root:current train perplexity3.7353739738464355

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:15<00:00, 495.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:15<00:00, 495.93s/it]
INFO:root:final mean train loss: 1501.7097939295536
INFO:root:final train perplexity: 3.7370080947875977
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.62s/it]
INFO:root:eval mean loss: 1805.098751419825
INFO:root:eval perplexity: 5.100711345672607
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.91s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.91s/it]
INFO:root:eval mean loss: 2259.1312467967364
INFO:root:eval perplexity: 7.9674601554870605
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/55
 28%|â–ˆâ–ˆâ–Š       | 55/200 [8:38:46<23:19:03, 578.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1503.236884622013
INFO:root:current train perplexity3.756075859069824
INFO:root:current mean train loss 1487.7480386762477
INFO:root:current train perplexity3.709203004837036
INFO:root:current mean train loss 1485.828649276342
INFO:root:current train perplexity3.7030224800109863
INFO:root:current mean train loss 1487.8059622941616
INFO:root:current train perplexity3.713007926940918
INFO:root:current mean train loss 1484.9474301667806
INFO:root:current train perplexity3.6982765197753906
INFO:root:current mean train loss 1486.4890205297577
INFO:root:current train perplexity3.699800491333008
INFO:root:current mean train loss 1489.1659235909158
INFO:root:current train perplexity3.7077229022979736
INFO:root:current mean train loss 1489.55561533416
INFO:root:current train perplexity3.711249351501465
INFO:root:current mean train loss 1490.6799739407693
INFO:root:current train perplexity3.7116236686706543
INFO:root:current mean train loss 1491.7577480667408
INFO:root:current train perplexity3.7132482528686523
INFO:root:current mean train loss 1492.7240608849886
INFO:root:current train perplexity3.7120842933654785
INFO:root:current mean train loss 1492.877452063182
INFO:root:current train perplexity3.712380886077881
INFO:root:current mean train loss 1494.799602292152
INFO:root:current train perplexity3.71271014213562
INFO:root:current mean train loss 1496.553639562055
INFO:root:current train perplexity3.7165236473083496
INFO:root:current mean train loss 1495.738542500899
INFO:root:current train perplexity3.716484308242798
INFO:root:current mean train loss 1495.511841059042
INFO:root:current train perplexity3.7177603244781494
INFO:root:current mean train loss 1496.2947726862521
INFO:root:current train perplexity3.7179758548736572
INFO:root:current mean train loss 1497.1525177037151
INFO:root:current train perplexity3.7207469940185547
INFO:root:current mean train loss 1497.199276124378
INFO:root:current train perplexity3.720982551574707
INFO:root:current mean train loss 1497.3237046534668
INFO:root:current train perplexity3.7223875522613525

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:15<00:00, 495.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:15<00:00, 495.70s/it]
INFO:root:final mean train loss: 1497.0906137532797
INFO:root:final train perplexity: 3.7218847274780273
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.60s/it]
INFO:root:eval mean loss: 1810.84323661209
INFO:root:eval perplexity: 5.127228260040283
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.02s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.04s/it]
INFO:root:eval mean loss: 2264.4786740566824
INFO:root:eval perplexity: 8.006695747375488
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/56
 28%|â–ˆâ–ˆâ–Š       | 56/200 [8:48:22<23:07:55, 578.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1476.1257491766237
INFO:root:current train perplexity3.6984634399414062
INFO:root:current mean train loss 1487.9382097863204
INFO:root:current train perplexity3.68868350982666
INFO:root:current mean train loss 1490.133819701662
INFO:root:current train perplexity3.687455415725708
INFO:root:current mean train loss 1489.38035544371
INFO:root:current train perplexity3.6923935413360596
INFO:root:current mean train loss 1487.3767092550409
INFO:root:current train perplexity3.688112258911133
INFO:root:current mean train loss 1487.2392152762025
INFO:root:current train perplexity3.6925132274627686
INFO:root:current mean train loss 1485.9683522870464
INFO:root:current train perplexity3.6924304962158203
INFO:root:current mean train loss 1487.5658610784578
INFO:root:current train perplexity3.697514057159424
INFO:root:current mean train loss 1488.2558569364626
INFO:root:current train perplexity3.6972031593322754
INFO:root:current mean train loss 1488.734215833662
INFO:root:current train perplexity3.7009220123291016
INFO:root:current mean train loss 1489.7947965525764
INFO:root:current train perplexity3.6987850666046143
INFO:root:current mean train loss 1489.019925989968
INFO:root:current train perplexity3.6992859840393066
INFO:root:current mean train loss 1489.2404950063387
INFO:root:current train perplexity3.700295925140381
INFO:root:current mean train loss 1489.0189690579316
INFO:root:current train perplexity3.6991050243377686
INFO:root:current mean train loss 1490.7100226877476
INFO:root:current train perplexity3.702296018600464
INFO:root:current mean train loss 1490.342902512492
INFO:root:current train perplexity3.70409893989563
INFO:root:current mean train loss 1490.9292982945942
INFO:root:current train perplexity3.7052154541015625
INFO:root:current mean train loss 1491.0273113326982
INFO:root:current train perplexity3.704765796661377
INFO:root:current mean train loss 1491.5866481589344
INFO:root:current train perplexity3.703087091445923
INFO:root:current mean train loss 1491.7533361297092
INFO:root:current train perplexity3.7049405574798584

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:24<00:00, 504.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:24<00:00, 504.16s/it]
INFO:root:final mean train loss: 1491.7533511471038
INFO:root:final train perplexity: 3.7044880390167236
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.04s/it]
INFO:root:eval mean loss: 1808.5699311384917
INFO:root:eval perplexity: 5.11671781539917
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.56s/it]
INFO:root:eval mean loss: 2264.137524327488
INFO:root:eval perplexity: 8.004185676574707
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/57
 28%|â–ˆâ–ˆâ–Š       | 57/200 [8:58:06<23:02:25, 580.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1494.2384966681984
INFO:root:current train perplexity3.697096109390259
INFO:root:current mean train loss 1493.65795535133
INFO:root:current train perplexity3.687288522720337
INFO:root:current mean train loss 1488.9143840732859
INFO:root:current train perplexity3.6812407970428467
INFO:root:current mean train loss 1488.1797110516093
INFO:root:current train perplexity3.680668592453003
INFO:root:current mean train loss 1487.4762805384448
INFO:root:current train perplexity3.6816461086273193
INFO:root:current mean train loss 1487.7709589138838
INFO:root:current train perplexity3.6835548877716064
INFO:root:current mean train loss 1487.8855080975743
INFO:root:current train perplexity3.684319019317627
INFO:root:current mean train loss 1488.06174103419
INFO:root:current train perplexity3.680983066558838
INFO:root:current mean train loss 1485.9087615826163
INFO:root:current train perplexity3.676091432571411
INFO:root:current mean train loss 1485.6617774648114
INFO:root:current train perplexity3.6751065254211426
INFO:root:current mean train loss 1486.1211100946205
INFO:root:current train perplexity3.6776368618011475
INFO:root:current mean train loss 1486.3371644738602
INFO:root:current train perplexity3.679450750350952
INFO:root:current mean train loss 1487.0393867372338
INFO:root:current train perplexity3.681142807006836
INFO:root:current mean train loss 1485.1163641500193
INFO:root:current train perplexity3.6788339614868164
INFO:root:current mean train loss 1485.3135236277567
INFO:root:current train perplexity3.6805272102355957
INFO:root:current mean train loss 1485.3384766870615
INFO:root:current train perplexity3.681762456893921
INFO:root:current mean train loss 1485.8850554322166
INFO:root:current train perplexity3.683638334274292
INFO:root:current mean train loss 1486.2545644492586
INFO:root:current train perplexity3.6843371391296387
INFO:root:current mean train loss 1485.8184479217202
INFO:root:current train perplexity3.6846938133239746
INFO:root:current mean train loss 1486.74844968222
INFO:root:current train perplexity3.6866722106933594

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:59<00:00, 479.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:59<00:00, 479.27s/it]
INFO:root:final mean train loss: 1486.385568069558
INFO:root:final train perplexity: 3.687072992324829
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.06s/it]
INFO:root:eval mean loss: 1811.281965972684
INFO:root:eval perplexity: 5.12925910949707
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.93s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.95s/it]
INFO:root:eval mean loss: 2266.16895786583
INFO:root:eval perplexity: 8.01913833618164
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/58
 29%|â–ˆâ–ˆâ–‰       | 58/200 [9:07:29<22:40:21, 574.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1474.113153435202
INFO:root:current train perplexity3.6426379680633545
INFO:root:current mean train loss 1468.8128134237754
INFO:root:current train perplexity3.651839017868042
INFO:root:current mean train loss 1472.012556537829
INFO:root:current train perplexity3.6525206565856934
INFO:root:current mean train loss 1470.6592763925528
INFO:root:current train perplexity3.654872417449951
INFO:root:current mean train loss 1473.7423901115496
INFO:root:current train perplexity3.658796548843384
INFO:root:current mean train loss 1475.3917217548078
INFO:root:current train perplexity3.6608309745788574
INFO:root:current mean train loss 1475.8835406449589
INFO:root:current train perplexity3.661839723587036
INFO:root:current mean train loss 1474.178538328523
INFO:root:current train perplexity3.657869815826416
INFO:root:current mean train loss 1474.9532004491084
INFO:root:current train perplexity3.6585047245025635
INFO:root:current mean train loss 1475.2230362170844
INFO:root:current train perplexity3.661377429962158
INFO:root:current mean train loss 1474.6356575820853
INFO:root:current train perplexity3.6626482009887695
INFO:root:current mean train loss 1475.4316998574302
INFO:root:current train perplexity3.6618523597717285
INFO:root:current mean train loss 1476.7861899053075
INFO:root:current train perplexity3.664401054382324
INFO:root:current mean train loss 1477.2132804567634
INFO:root:current train perplexity3.6636953353881836
INFO:root:current mean train loss 1478.0675800156514
INFO:root:current train perplexity3.665541410446167
INFO:root:current mean train loss 1479.2781310072457
INFO:root:current train perplexity3.6659793853759766
INFO:root:current mean train loss 1479.8923020360023
INFO:root:current train perplexity3.6698219776153564
INFO:root:current mean train loss 1480.3560367701768
INFO:root:current train perplexity3.6689229011535645
INFO:root:current mean train loss 1481.4456274867373
INFO:root:current train perplexity3.6702256202697754

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:08<00:00, 488.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:08<00:00, 488.62s/it]
INFO:root:final mean train loss: 1481.4344308827178
INFO:root:final train perplexity: 3.67108154296875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.26s/it]
INFO:root:eval mean loss: 1812.2425835792055
INFO:root:eval perplexity: 5.133708477020264
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.29s/it]
INFO:root:eval mean loss: 2270.9326457571474
INFO:root:eval perplexity: 8.054306030273438
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/59
 30%|â–ˆâ–ˆâ–‰       | 59/200 [9:17:02<22:29:09, 574.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1437.082275390625
INFO:root:current train perplexity3.7580106258392334
INFO:root:current mean train loss 1485.9595372817096
INFO:root:current train perplexity3.6581571102142334
INFO:root:current mean train loss 1479.0164057665531
INFO:root:current train perplexity3.668163776397705
INFO:root:current mean train loss 1473.7640376817312
INFO:root:current train perplexity3.6590888500213623
INFO:root:current mean train loss 1473.4662238733092
INFO:root:current train perplexity3.658170223236084
INFO:root:current mean train loss 1476.7223695549833
INFO:root:current train perplexity3.6599581241607666
INFO:root:current mean train loss 1475.7229372956033
INFO:root:current train perplexity3.6535537242889404
INFO:root:current mean train loss 1478.3440329179464
INFO:root:current train perplexity3.6558008193969727
INFO:root:current mean train loss 1477.9308138345543
INFO:root:current train perplexity3.6525464057922363
INFO:root:current mean train loss 1476.9019116319205
INFO:root:current train perplexity3.652017831802368
INFO:root:current mean train loss 1476.5165227406515
INFO:root:current train perplexity3.6508243083953857
INFO:root:current mean train loss 1476.043518952579
INFO:root:current train perplexity3.6515860557556152
INFO:root:current mean train loss 1475.7410321989394
INFO:root:current train perplexity3.653757095336914
INFO:root:current mean train loss 1475.7415126443093
INFO:root:current train perplexity3.653012752532959
INFO:root:current mean train loss 1475.6732539940153
INFO:root:current train perplexity3.654775619506836
INFO:root:current mean train loss 1476.1131269147647
INFO:root:current train perplexity3.652942180633545
INFO:root:current mean train loss 1476.8410701680273
INFO:root:current train perplexity3.6546661853790283
INFO:root:current mean train loss 1477.0575971054275
INFO:root:current train perplexity3.6561238765716553
INFO:root:current mean train loss 1477.8317820964987
INFO:root:current train perplexity3.6566874980926514
INFO:root:current mean train loss 1477.5057500123226
INFO:root:current train perplexity3.6568186283111572

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:10<00:00, 490.43s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:10<00:00, 490.43s/it]
INFO:root:final mean train loss: 1477.263756702479
INFO:root:final train perplexity: 3.657665729522705
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.50s/it]
INFO:root:eval mean loss: 1817.441464255042
INFO:root:eval perplexity: 5.1578569412231445
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.95s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.96s/it]
INFO:root:eval mean loss: 2277.5703237547095
INFO:root:eval perplexity: 8.103571891784668
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/60
 30%|â–ˆâ–ˆâ–ˆ       | 60/200 [9:26:38<22:21:05, 574.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1436.6979851973683
INFO:root:current train perplexity3.603778600692749
INFO:root:current mean train loss 1464.1101874343487
INFO:root:current train perplexity3.606839895248413
INFO:root:current mean train loss 1469.5337086856093
INFO:root:current train perplexity3.615107774734497
INFO:root:current mean train loss 1466.5201821641115
INFO:root:current train perplexity3.629818916320801
INFO:root:current mean train loss 1467.3471560239223
INFO:root:current train perplexity3.634866952896118
INFO:root:current mean train loss 1465.886153557397
INFO:root:current train perplexity3.635287284851074
INFO:root:current mean train loss 1468.1903845274005
INFO:root:current train perplexity3.6365811824798584
INFO:root:current mean train loss 1469.2007723875934
INFO:root:current train perplexity3.6344592571258545
INFO:root:current mean train loss 1467.543236142113
INFO:root:current train perplexity3.6307373046875
INFO:root:current mean train loss 1469.5083822057347
INFO:root:current train perplexity3.631387948989868
INFO:root:current mean train loss 1469.8823963348718
INFO:root:current train perplexity3.6349077224731445
INFO:root:current mean train loss 1470.909239249958
INFO:root:current train perplexity3.6388018131256104
INFO:root:current mean train loss 1470.203886262113
INFO:root:current train perplexity3.63788104057312
INFO:root:current mean train loss 1471.025949797728
INFO:root:current train perplexity3.637291431427002
INFO:root:current mean train loss 1471.8927388208026
INFO:root:current train perplexity3.637178659439087
INFO:root:current mean train loss 1471.6679550080491
INFO:root:current train perplexity3.6381828784942627
INFO:root:current mean train loss 1472.8694943951707
INFO:root:current train perplexity3.63959002494812
INFO:root:current mean train loss 1472.9764056336123
INFO:root:current train perplexity3.641202688217163
INFO:root:current mean train loss 1473.0823994070831
INFO:root:current train perplexity3.6412930488586426
INFO:root:current mean train loss 1472.5015879954567
INFO:root:current train perplexity3.641130208969116

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:11<00:00, 491.86s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:11<00:00, 491.86s/it]
INFO:root:final mean train loss: 1472.4057230908522
INFO:root:final train perplexity: 3.6421005725860596
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.82s/it]
INFO:root:eval mean loss: 1815.8982552602781
INFO:root:eval perplexity: 5.15067720413208
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.63s/it]
INFO:root:eval mean loss: 2277.3864399794993
INFO:root:eval perplexity: 8.102204322814941
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/61
 30%|â–ˆâ–ˆâ–ˆ       | 61/200 [9:36:15<22:12:52, 575.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1459.0397067599827
INFO:root:current train perplexity3.595797538757324
INFO:root:current mean train loss 1471.663050034467
INFO:root:current train perplexity3.627422571182251
INFO:root:current mean train loss 1466.3332545393605
INFO:root:current train perplexity3.6141202449798584
INFO:root:current mean train loss 1463.4148559570312
INFO:root:current train perplexity3.61103892326355
INFO:root:current mean train loss 1464.5107881038562
INFO:root:current train perplexity3.6118478775024414
INFO:root:current mean train loss 1464.4672409740847
INFO:root:current train perplexity3.615668535232544
INFO:root:current mean train loss 1464.598059888156
INFO:root:current train perplexity3.6166398525238037
INFO:root:current mean train loss 1465.0578570158584
INFO:root:current train perplexity3.620394229888916
INFO:root:current mean train loss 1464.469059556295
INFO:root:current train perplexity3.6155736446380615
INFO:root:current mean train loss 1464.7500654693342
INFO:root:current train perplexity3.6155288219451904
INFO:root:current mean train loss 1465.0434000022624
INFO:root:current train perplexity3.6172165870666504
INFO:root:current mean train loss 1465.3205729739766
INFO:root:current train perplexity3.617682456970215
INFO:root:current mean train loss 1466.0374905978206
INFO:root:current train perplexity3.620107412338257
INFO:root:current mean train loss 1466.7533889130918
INFO:root:current train perplexity3.6218559741973877
INFO:root:current mean train loss 1466.7569682086744
INFO:root:current train perplexity3.6205577850341797
INFO:root:current mean train loss 1467.5578049023945
INFO:root:current train perplexity3.6227800846099854
INFO:root:current mean train loss 1467.1465930892086
INFO:root:current train perplexity3.623723268508911
INFO:root:current mean train loss 1467.8171910580402
INFO:root:current train perplexity3.625748634338379
INFO:root:current mean train loss 1468.5678868511923
INFO:root:current train perplexity3.6265933513641357
INFO:root:current mean train loss 1468.2230517174587
INFO:root:current train perplexity3.6268904209136963

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:02<00:00, 482.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:02<00:00, 482.49s/it]
INFO:root:final mean train loss: 1467.5716623218746
INFO:root:final train perplexity: 3.6266772747039795
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.36s/it]
INFO:root:eval mean loss: 1822.5186148569094
INFO:root:eval perplexity: 5.181549072265625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.42s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.45s/it]
INFO:root:eval mean loss: 2285.549206456394
INFO:root:eval perplexity: 8.163187026977539
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/62
 31%|â–ˆâ–ˆâ–ˆ       | 62/200 [9:45:37<21:54:31, 571.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1443.2548759028596
INFO:root:current train perplexity3.575444221496582
INFO:root:current mean train loss 1449.371550117443
INFO:root:current train perplexity3.581264019012451
INFO:root:current mean train loss 1453.652840233603
INFO:root:current train perplexity3.5890748500823975
INFO:root:current mean train loss 1457.9883760014607
INFO:root:current train perplexity3.593881130218506
INFO:root:current mean train loss 1458.430096017867
INFO:root:current train perplexity3.600672960281372
INFO:root:current mean train loss 1459.925219682414
INFO:root:current train perplexity3.601804733276367
INFO:root:current mean train loss 1461.1801208215688
INFO:root:current train perplexity3.603257894515991
INFO:root:current mean train loss 1459.7818893696049
INFO:root:current train perplexity3.5992331504821777
INFO:root:current mean train loss 1460.2165900853147
INFO:root:current train perplexity3.6036276817321777
INFO:root:current mean train loss 1461.5264044618557
INFO:root:current train perplexity3.603625535964966
INFO:root:current mean train loss 1461.344056045228
INFO:root:current train perplexity3.6045806407928467
INFO:root:current mean train loss 1462.5467802446403
INFO:root:current train perplexity3.6047728061676025
INFO:root:current mean train loss 1461.4675220876147
INFO:root:current train perplexity3.608170509338379
INFO:root:current mean train loss 1461.739154688799
INFO:root:current train perplexity3.6080377101898193
INFO:root:current mean train loss 1462.3532956800047
INFO:root:current train perplexity3.6107001304626465
INFO:root:current mean train loss 1462.7698657053636
INFO:root:current train perplexity3.6119132041931152
INFO:root:current mean train loss 1462.4139476667658
INFO:root:current train perplexity3.610903263092041
INFO:root:current mean train loss 1462.324706334899
INFO:root:current train perplexity3.6107609272003174
INFO:root:current mean train loss 1463.096014196784
INFO:root:current train perplexity3.611100435256958
INFO:root:current mean train loss 1463.1522836211518
INFO:root:current train perplexity3.61191725730896

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:18<00:00, 498.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:18<00:00, 498.25s/it]
INFO:root:final mean train loss: 1462.8671929479187
INFO:root:final train perplexity: 3.6117308139801025
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.95s/it]
INFO:root:eval mean loss: 1823.7332512605276
INFO:root:eval perplexity: 5.187232971191406
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.71s/it]
INFO:root:eval mean loss: 2290.404652264101
INFO:root:eval perplexity: 8.199678421020508
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/63
 32%|â–ˆâ–ˆâ–ˆâ–      | 63/200 [9:55:15<21:49:31, 573.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1455.6681570870535
INFO:root:current train perplexity3.5728917121887207
INFO:root:current mean train loss 1458.111444450827
INFO:root:current train perplexity3.5949630737304688
INFO:root:current mean train loss 1460.134826208044
INFO:root:current train perplexity3.5973899364471436
INFO:root:current mean train loss 1461.7784074113176
INFO:root:current train perplexity3.5951950550079346
INFO:root:current mean train loss 1461.7789530003324
INFO:root:current train perplexity3.589982509613037
INFO:root:current mean train loss 1459.9328923810992
INFO:root:current train perplexity3.5908052921295166
INFO:root:current mean train loss 1460.2313447411382
INFO:root:current train perplexity3.5890581607818604
INFO:root:current mean train loss 1457.6976208971691
INFO:root:current train perplexity3.58622670173645
INFO:root:current mean train loss 1457.6846181584501
INFO:root:current train perplexity3.585453748703003
INFO:root:current mean train loss 1458.0486993848663
INFO:root:current train perplexity3.5900163650512695
INFO:root:current mean train loss 1457.2646030319072
INFO:root:current train perplexity3.5878543853759766
INFO:root:current mean train loss 1455.4021423861511
INFO:root:current train perplexity3.5855114459991455
INFO:root:current mean train loss 1456.4605417807272
INFO:root:current train perplexity3.587918996810913
INFO:root:current mean train loss 1455.999415577241
INFO:root:current train perplexity3.586289405822754
INFO:root:current mean train loss 1457.1860889668367
INFO:root:current train perplexity3.589555025100708
INFO:root:current mean train loss 1457.5652449492436
INFO:root:current train perplexity3.5917530059814453
INFO:root:current mean train loss 1457.9427497543975
INFO:root:current train perplexity3.593324661254883
INFO:root:current mean train loss 1457.1882184217204
INFO:root:current train perplexity3.5931761264801025
INFO:root:current mean train loss 1457.010710658109
INFO:root:current train perplexity3.593113660812378
INFO:root:current mean train loss 1458.4087338520185
INFO:root:current train perplexity3.5959279537200928

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:04<00:00, 484.05s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:04<00:00, 484.05s/it]
INFO:root:final mean train loss: 1458.0161074024224
INFO:root:final train perplexity: 3.5963828563690186
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.76s/it]
INFO:root:eval mean loss: 1824.909503909713
INFO:root:eval perplexity: 5.192743301391602
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.98s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.98s/it]
INFO:root:eval mean loss: 2291.084124193124
INFO:root:eval perplexity: 8.204800605773926
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/64
 32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [10:04:41<21:34:58, 571.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1451.8449482534124
INFO:root:current train perplexity3.5634970664978027
INFO:root:current mean train loss 1441.6764751577123
INFO:root:current train perplexity3.5576248168945312
INFO:root:current mean train loss 1440.923828550332
INFO:root:current train perplexity3.5479016304016113
INFO:root:current mean train loss 1442.7137558417112
INFO:root:current train perplexity3.5510637760162354
INFO:root:current mean train loss 1446.9955042030288
INFO:root:current train perplexity3.5542709827423096
INFO:root:current mean train loss 1446.7335502455546
INFO:root:current train perplexity3.5532054901123047
INFO:root:current mean train loss 1448.865407263521
INFO:root:current train perplexity3.5572588443756104
INFO:root:current mean train loss 1448.4040941483183
INFO:root:current train perplexity3.5584936141967773
INFO:root:current mean train loss 1450.455794445154
INFO:root:current train perplexity3.564244031906128
INFO:root:current mean train loss 1451.3442977704296
INFO:root:current train perplexity3.5666608810424805
INFO:root:current mean train loss 1451.0394160210153
INFO:root:current train perplexity3.5684168338775635
INFO:root:current mean train loss 1452.0996446488982
INFO:root:current train perplexity3.5707428455352783
INFO:root:current mean train loss 1452.3144947635915
INFO:root:current train perplexity3.5737314224243164
INFO:root:current mean train loss 1452.818638807763
INFO:root:current train perplexity3.576819658279419
INFO:root:current mean train loss 1452.2442182837194
INFO:root:current train perplexity3.5767104625701904
INFO:root:current mean train loss 1452.1974406155236
INFO:root:current train perplexity3.5752415657043457
INFO:root:current mean train loss 1451.7718877063112
INFO:root:current train perplexity3.5761377811431885
INFO:root:current mean train loss 1452.930163348795
INFO:root:current train perplexity3.5777406692504883
INFO:root:current mean train loss 1453.1763359680338
INFO:root:current train perplexity3.5794782638549805

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:05<00:00, 485.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:05<00:00, 485.16s/it]
INFO:root:final mean train loss: 1453.49219713389
INFO:root:final train perplexity: 3.5821287631988525
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.09s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.11s/it]
INFO:root:eval mean loss: 1822.6366178904864
INFO:root:eval perplexity: 5.182100772857666
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.74s/it]
INFO:root:eval mean loss: 2290.559289377632
INFO:root:eval perplexity: 8.200843811035156
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/65
 32%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/200 [10:14:12<21:24:45, 571.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1435.3109741210938
INFO:root:current train perplexity3.419137716293335
INFO:root:current mean train loss 1425.2132181020884
INFO:root:current train perplexity3.5292651653289795
INFO:root:current mean train loss 1429.2472354664521
INFO:root:current train perplexity3.5232720375061035
INFO:root:current mean train loss 1436.9535871806897
INFO:root:current train perplexity3.533472776412964
INFO:root:current mean train loss 1435.2041109292813
INFO:root:current train perplexity3.5339484214782715
INFO:root:current mean train loss 1437.8662971617684
INFO:root:current train perplexity3.5382211208343506
INFO:root:current mean train loss 1439.5738705262443
INFO:root:current train perplexity3.5373458862304688
INFO:root:current mean train loss 1442.2962606603448
INFO:root:current train perplexity3.5374159812927246
INFO:root:current mean train loss 1443.2544428127915
INFO:root:current train perplexity3.5412702560424805
INFO:root:current mean train loss 1445.2114764188243
INFO:root:current train perplexity3.547307014465332
INFO:root:current mean train loss 1444.505028105352
INFO:root:current train perplexity3.550952434539795
INFO:root:current mean train loss 1444.6426914602087
INFO:root:current train perplexity3.5529468059539795
INFO:root:current mean train loss 1443.0128528683686
INFO:root:current train perplexity3.5507073402404785
INFO:root:current mean train loss 1444.4681720382596
INFO:root:current train perplexity3.55497407913208
INFO:root:current mean train loss 1444.723217651715
INFO:root:current train perplexity3.557373523712158
INFO:root:current mean train loss 1445.3600781217535
INFO:root:current train perplexity3.559417247772217
INFO:root:current mean train loss 1446.8403273128215
INFO:root:current train perplexity3.561350107192993
INFO:root:current mean train loss 1447.103687626655
INFO:root:current train perplexity3.562040328979492
INFO:root:current mean train loss 1446.6744382058966
INFO:root:current train perplexity3.561431646347046
INFO:root:current mean train loss 1448.0504238865956
INFO:root:current train perplexity3.563528537750244

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:58<00:00, 478.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:58<00:00, 478.54s/it]
INFO:root:final mean train loss: 1448.0473647586516
INFO:root:final train perplexity: 3.5650477409362793
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.33s/it]
INFO:root:eval mean loss: 1828.6444615573748
INFO:root:eval perplexity: 5.210279941558838
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.21s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.22s/it]
INFO:root:eval mean loss: 2301.2800154449246
INFO:root:eval perplexity: 8.282013893127441
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/66
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/200 [10:23:33<21:08:49, 568.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1425.2096470424108
INFO:root:current train perplexity3.5073089599609375
INFO:root:current mean train loss 1439.720198702221
INFO:root:current train perplexity3.527635335922241
INFO:root:current mean train loss 1434.996566565328
INFO:root:current train perplexity3.528970718383789
INFO:root:current mean train loss 1438.7176445221232
INFO:root:current train perplexity3.5309159755706787
INFO:root:current mean train loss 1438.0470968142258
INFO:root:current train perplexity3.531893253326416
INFO:root:current mean train loss 1436.5839677396982
INFO:root:current train perplexity3.5327627658843994
INFO:root:current mean train loss 1438.676566745924
INFO:root:current train perplexity3.5351037979125977
INFO:root:current mean train loss 1438.9428622897885
INFO:root:current train perplexity3.5385189056396484
INFO:root:current mean train loss 1438.4644559797503
INFO:root:current train perplexity3.53517746925354
INFO:root:current mean train loss 1439.3842211463423
INFO:root:current train perplexity3.5360655784606934
INFO:root:current mean train loss 1439.687358919717
INFO:root:current train perplexity3.5371410846710205
INFO:root:current mean train loss 1440.7941821572188
INFO:root:current train perplexity3.5412325859069824
INFO:root:current mean train loss 1440.9830590200463
INFO:root:current train perplexity3.5417683124542236
INFO:root:current mean train loss 1440.216282719649
INFO:root:current train perplexity3.540356397628784
INFO:root:current mean train loss 1441.311234712433
INFO:root:current train perplexity3.5435831546783447
INFO:root:current mean train loss 1442.416657437156
INFO:root:current train perplexity3.5471293926239014
INFO:root:current mean train loss 1443.4163457645743
INFO:root:current train perplexity3.5493221282958984
INFO:root:current mean train loss 1443.6826473326962
INFO:root:current train perplexity3.5495798587799072
INFO:root:current mean train loss 1443.584197277423
INFO:root:current train perplexity3.5483381748199463
INFO:root:current mean train loss 1444.194375526154
INFO:root:current train perplexity3.551044225692749

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:07<00:00, 487.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:07<00:00, 487.29s/it]
INFO:root:final mean train loss: 1444.2245435382883
INFO:root:final train perplexity: 3.553103446960449
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.90s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.91s/it]
INFO:root:eval mean loss: 1831.225414086741
INFO:root:eval perplexity: 5.222433090209961
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.95s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.95s/it]
INFO:root:eval mean loss: 2304.037421043883
INFO:root:eval perplexity: 8.303018569946289
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/67
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/200 [10:33:04<21:00:53, 568.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1436.597068385074
INFO:root:current train perplexity3.509920120239258
INFO:root:current mean train loss 1424.850116232167
INFO:root:current train perplexity3.495635747909546
INFO:root:current mean train loss 1431.57462035107
INFO:root:current train perplexity3.5051631927490234
INFO:root:current mean train loss 1433.291545077894
INFO:root:current train perplexity3.5027153491973877
INFO:root:current mean train loss 1434.6672109664848
INFO:root:current train perplexity3.5142323970794678
INFO:root:current mean train loss 1437.0258845786623
INFO:root:current train perplexity3.517641305923462
INFO:root:current mean train loss 1437.9458820976809
INFO:root:current train perplexity3.5181612968444824
INFO:root:current mean train loss 1436.7817728512978
INFO:root:current train perplexity3.5182650089263916
INFO:root:current mean train loss 1437.331105962858
INFO:root:current train perplexity3.519866943359375
INFO:root:current mean train loss 1436.3955214770872
INFO:root:current train perplexity3.5216569900512695
INFO:root:current mean train loss 1436.6435324608246
INFO:root:current train perplexity3.5287270545959473
INFO:root:current mean train loss 1437.6471247614372
INFO:root:current train perplexity3.529979944229126
INFO:root:current mean train loss 1438.4672067669944
INFO:root:current train perplexity3.532205104827881
INFO:root:current mean train loss 1439.174442490833
INFO:root:current train perplexity3.533956289291382
INFO:root:current mean train loss 1438.7025448689044
INFO:root:current train perplexity3.5344109535217285
INFO:root:current mean train loss 1439.27028899075
INFO:root:current train perplexity3.535895586013794
INFO:root:current mean train loss 1439.0420247544882
INFO:root:current train perplexity3.535322427749634
INFO:root:current mean train loss 1438.953163840554
INFO:root:current train perplexity3.536792755126953
INFO:root:current mean train loss 1439.893626481846
INFO:root:current train perplexity3.538146734237671
INFO:root:current mean train loss 1439.773214271317
INFO:root:current train perplexity3.5386242866516113

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:10<00:00, 490.21s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:10<00:00, 490.21s/it]
INFO:root:final mean train loss: 1439.5322718694845
INFO:root:final train perplexity: 3.5384981632232666
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.71s/it]
INFO:root:eval mean loss: 1833.702982151762
INFO:root:eval perplexity: 5.234124660491943
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.44s/it]
INFO:root:eval mean loss: 2308.4358542151485
INFO:root:eval perplexity: 8.336634635925293
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/68
 34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [10:42:37<20:54:40, 570.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1442.1179709694602
INFO:root:current train perplexity3.540862798690796
INFO:root:current mean train loss 1423.8295236895162
INFO:root:current train perplexity3.500633716583252
INFO:root:current mean train loss 1425.0592476639092
INFO:root:current train perplexity3.5045104026794434
INFO:root:current mean train loss 1425.3915998431999
INFO:root:current train perplexity3.5121846199035645
INFO:root:current mean train loss 1425.2417488517342
INFO:root:current train perplexity3.514004707336426
INFO:root:current mean train loss 1427.6192626953125
INFO:root:current train perplexity3.5160813331604004
INFO:root:current mean train loss 1428.9003926750358
INFO:root:current train perplexity3.519218683242798
INFO:root:current mean train loss 1427.783503369464
INFO:root:current train perplexity3.5163960456848145
INFO:root:current mean train loss 1429.444840209247
INFO:root:current train perplexity3.515444278717041
INFO:root:current mean train loss 1431.09873213044
INFO:root:current train perplexity3.516502618789673
INFO:root:current mean train loss 1431.9985646613966
INFO:root:current train perplexity3.5185208320617676
INFO:root:current mean train loss 1432.5292754202178
INFO:root:current train perplexity3.519136905670166
INFO:root:current mean train loss 1433.3355541700387
INFO:root:current train perplexity3.5207021236419678
INFO:root:current mean train loss 1434.1934076625923
INFO:root:current train perplexity3.522308588027954
INFO:root:current mean train loss 1434.4897073332797
INFO:root:current train perplexity3.521953582763672
INFO:root:current mean train loss 1434.7805424631983
INFO:root:current train perplexity3.5230393409729004
INFO:root:current mean train loss 1435.3138700640814
INFO:root:current train perplexity3.524461269378662
INFO:root:current mean train loss 1435.786682998353
INFO:root:current train perplexity3.525200843811035
INFO:root:current mean train loss 1435.2467322007665
INFO:root:current train perplexity3.5254955291748047
INFO:root:current mean train loss 1435.1102121962915
INFO:root:current train perplexity3.524705410003662

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:12<00:00, 492.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:12<00:00, 492.08s/it]
INFO:root:final mean train loss: 1435.0002126227228
INFO:root:final train perplexity: 3.5244486331939697
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.46s/it]
INFO:root:eval mean loss: 1837.411811125194
INFO:root:eval perplexity: 5.251676559448242
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.20s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.20s/it]
INFO:root:eval mean loss: 2315.151130838597
INFO:root:eval perplexity: 8.388222694396973
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/69
 34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [10:52:08<20:45:38, 570.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1443.642361111111
INFO:root:current train perplexity3.5113255977630615
INFO:root:current mean train loss 1425.3121685649073
INFO:root:current train perplexity3.4859447479248047
INFO:root:current mean train loss 1425.5599140840418
INFO:root:current train perplexity3.489469289779663
INFO:root:current mean train loss 1425.9915699292255
INFO:root:current train perplexity3.490323305130005
INFO:root:current mean train loss 1427.4099188335872
INFO:root:current train perplexity3.491703510284424
INFO:root:current mean train loss 1426.9853338495002
INFO:root:current train perplexity3.489511251449585
INFO:root:current mean train loss 1429.0383406139556
INFO:root:current train perplexity3.4928040504455566
INFO:root:current mean train loss 1429.8144659328955
INFO:root:current train perplexity3.5002827644348145
INFO:root:current mean train loss 1428.6272154641808
INFO:root:current train perplexity3.498481035232544
INFO:root:current mean train loss 1428.147483794287
INFO:root:current train perplexity3.5016682147979736
INFO:root:current mean train loss 1429.017618777147
INFO:root:current train perplexity3.5032222270965576
INFO:root:current mean train loss 1429.8891504697833
INFO:root:current train perplexity3.5043208599090576
INFO:root:current mean train loss 1429.4746640763192
INFO:root:current train perplexity3.5043153762817383
INFO:root:current mean train loss 1429.5975130042252
INFO:root:current train perplexity3.5036582946777344
INFO:root:current mean train loss 1430.0658036107602
INFO:root:current train perplexity3.5057928562164307
INFO:root:current mean train loss 1430.0947549057978
INFO:root:current train perplexity3.506801128387451
INFO:root:current mean train loss 1430.4329872678911
INFO:root:current train perplexity3.508617877960205
INFO:root:current mean train loss 1430.5769332989075
INFO:root:current train perplexity3.510122776031494
INFO:root:current mean train loss 1430.7248492770725
INFO:root:current train perplexity3.5107195377349854
INFO:root:current mean train loss 1431.035016785287
INFO:root:current train perplexity3.510658025741577

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:17<00:00, 497.20s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:17<00:00, 497.20s/it]
INFO:root:final mean train loss: 1430.6082630397934
INFO:root:final train perplexity: 3.5108859539031982
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.64s/it]
INFO:root:eval mean loss: 1838.7313357955175
INFO:root:eval perplexity: 5.257936000823975
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.86s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.86s/it]
INFO:root:eval mean loss: 2315.8950173668827
INFO:root:eval perplexity: 8.393956184387207
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/70
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [11:01:47<20:41:34, 573.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1409.926471153002
INFO:root:current train perplexity3.4477334022521973
INFO:root:current mean train loss 1416.9772962136244
INFO:root:current train perplexity3.471898317337036
INFO:root:current mean train loss 1416.5028270470643
INFO:root:current train perplexity3.4729576110839844
INFO:root:current mean train loss 1420.5007465431195
INFO:root:current train perplexity3.483912229537964
INFO:root:current mean train loss 1423.1670635824066
INFO:root:current train perplexity3.487813711166382
INFO:root:current mean train loss 1420.7941469668533
INFO:root:current train perplexity3.486180067062378
INFO:root:current mean train loss 1422.7357687984738
INFO:root:current train perplexity3.486947774887085
INFO:root:current mean train loss 1424.0089208798718
INFO:root:current train perplexity3.487062931060791
INFO:root:current mean train loss 1423.220238736027
INFO:root:current train perplexity3.4879322052001953
INFO:root:current mean train loss 1421.6099417814712
INFO:root:current train perplexity3.4875175952911377
INFO:root:current mean train loss 1421.733879656831
INFO:root:current train perplexity3.489366054534912
INFO:root:current mean train loss 1423.1602320177801
INFO:root:current train perplexity3.4906911849975586
INFO:root:current mean train loss 1423.5203238073664
INFO:root:current train perplexity3.491431713104248
INFO:root:current mean train loss 1424.4216994085787
INFO:root:current train perplexity3.491593599319458
INFO:root:current mean train loss 1424.265610243347
INFO:root:current train perplexity3.493835210800171
INFO:root:current mean train loss 1425.3633630655336
INFO:root:current train perplexity3.495222806930542
INFO:root:current mean train loss 1425.2042096839846
INFO:root:current train perplexity3.494213819503784
INFO:root:current mean train loss 1425.6640283830877
INFO:root:current train perplexity3.495689630508423
INFO:root:current mean train loss 1425.9917460048307
INFO:root:current train perplexity3.4961025714874268

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:56<00:00, 476.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:56<00:00, 476.94s/it]
INFO:root:final mean train loss: 1425.9784003876703
INFO:root:final train perplexity: 3.49664568901062
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.77s/it]
INFO:root:eval mean loss: 1838.9957080597574
INFO:root:eval perplexity: 5.259191036224365
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.27s/it]
INFO:root:eval mean loss: 2316.6943857179467
INFO:root:eval perplexity: 8.400124549865723
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/71
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [11:11:07<20:23:09, 568.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1392.3755696614583
INFO:root:current train perplexity3.486693859100342
INFO:root:current mean train loss 1409.421225493809
INFO:root:current train perplexity3.4686152935028076
INFO:root:current mean train loss 1411.8299394626063
INFO:root:current train perplexity3.474731683731079
INFO:root:current mean train loss 1411.3641046262255
INFO:root:current train perplexity3.46416974067688
INFO:root:current mean train loss 1413.5815961865956
INFO:root:current train perplexity3.4690780639648438
INFO:root:current mean train loss 1413.823547604527
INFO:root:current train perplexity3.466571092605591
INFO:root:current mean train loss 1413.8098368125386
INFO:root:current train perplexity3.470791816711426
INFO:root:current mean train loss 1414.1450497894718
INFO:root:current train perplexity3.4729981422424316
INFO:root:current mean train loss 1418.3108538682052
INFO:root:current train perplexity3.476266384124756
INFO:root:current mean train loss 1418.7313006066329
INFO:root:current train perplexity3.4774913787841797
INFO:root:current mean train loss 1418.6716502741365
INFO:root:current train perplexity3.4768760204315186
INFO:root:current mean train loss 1419.3571339170928
INFO:root:current train perplexity3.4777331352233887
INFO:root:current mean train loss 1420.2802675667886
INFO:root:current train perplexity3.4794960021972656
INFO:root:current mean train loss 1421.2580024286945
INFO:root:current train perplexity3.4808895587921143
INFO:root:current mean train loss 1421.0180908897694
INFO:root:current train perplexity3.4826107025146484
INFO:root:current mean train loss 1421.1310088422351
INFO:root:current train perplexity3.4840853214263916
INFO:root:current mean train loss 1421.8381680575285
INFO:root:current train perplexity3.485445499420166
INFO:root:current mean train loss 1423.013889683928
INFO:root:current train perplexity3.4881625175476074
INFO:root:current mean train loss 1422.924618270046
INFO:root:current train perplexity3.4881787300109863
INFO:root:current mean train loss 1423.38430545963
INFO:root:current train perplexity3.486759901046753

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:58<00:00, 478.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:58<00:00, 478.62s/it]
INFO:root:final mean train loss: 1422.8800047596958
INFO:root:final train perplexity: 3.487147569656372
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.94s/it]
INFO:root:eval mean loss: 1843.1088252507202
INFO:root:eval perplexity: 5.278753280639648
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.27s/it]
INFO:root:eval mean loss: 2324.083718157829
INFO:root:eval perplexity: 8.457341194152832
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/72
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/200 [11:20:31<20:10:38, 567.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1409.49097210428
INFO:root:current train perplexity3.4943251609802246
INFO:root:current mean train loss 1413.0833174542684
INFO:root:current train perplexity3.456768274307251
INFO:root:current mean train loss 1417.3305729750562
INFO:root:current train perplexity3.472308397293091
INFO:root:current mean train loss 1413.9104090829383
INFO:root:current train perplexity3.4612314701080322
INFO:root:current mean train loss 1416.4734544686391
INFO:root:current train perplexity3.4679458141326904
INFO:root:current mean train loss 1415.3333994644777
INFO:root:current train perplexity3.4644222259521484
INFO:root:current mean train loss 1416.5127717289074
INFO:root:current train perplexity3.468029022216797
INFO:root:current mean train loss 1415.9224213617306
INFO:root:current train perplexity3.4681665897369385
INFO:root:current mean train loss 1416.4006970615317
INFO:root:current train perplexity3.46747088432312
INFO:root:current mean train loss 1417.2759528361576
INFO:root:current train perplexity3.4673681259155273
INFO:root:current mean train loss 1415.9059009719804
INFO:root:current train perplexity3.46582293510437
INFO:root:current mean train loss 1416.0284459699187
INFO:root:current train perplexity3.465299367904663
INFO:root:current mean train loss 1416.491208841444
INFO:root:current train perplexity3.4649479389190674
INFO:root:current mean train loss 1417.8144835733772
INFO:root:current train perplexity3.465380907058716
INFO:root:current mean train loss 1418.0415515162454
INFO:root:current train perplexity3.4662253856658936
INFO:root:current mean train loss 1418.3088471080157
INFO:root:current train perplexity3.465196132659912
INFO:root:current mean train loss 1417.9403590627167
INFO:root:current train perplexity3.4665725231170654
INFO:root:current mean train loss 1418.5198395784969
INFO:root:current train perplexity3.46748948097229
INFO:root:current mean train loss 1418.2035975453623
INFO:root:current train perplexity3.4688360691070557
INFO:root:current mean train loss 1418.5536939885799
INFO:root:current train perplexity3.471688985824585

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:01<00:00, 481.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:01<00:00, 481.31s/it]
INFO:root:final mean train loss: 1418.1972137928249
INFO:root:final train perplexity: 3.47284197807312
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.88s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.89s/it]
INFO:root:eval mean loss: 1846.4556313372673
INFO:root:eval perplexity: 5.294723987579346
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.45s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.45s/it]
INFO:root:eval mean loss: 2329.7949058586823
INFO:root:eval perplexity: 8.501830101013184
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/73
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/200 [11:29:55<19:58:55, 566.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1429.8268615722657
INFO:root:current train perplexity3.4537978172302246
INFO:root:current mean train loss 1416.461602783203
INFO:root:current train perplexity3.45296573638916
INFO:root:current mean train loss 1410.051005045573
INFO:root:current train perplexity3.4585862159729004
INFO:root:current mean train loss 1410.024247113396
INFO:root:current train perplexity3.4463753700256348
INFO:root:current mean train loss 1409.0524652654474
INFO:root:current train perplexity3.439805269241333
INFO:root:current mean train loss 1407.5552838360823
INFO:root:current train perplexity3.438180923461914
INFO:root:current mean train loss 1408.4061367034913
INFO:root:current train perplexity3.440009832382202
INFO:root:current mean train loss 1409.7670472841005
INFO:root:current train perplexity3.444514274597168
INFO:root:current mean train loss 1409.9810958135695
INFO:root:current train perplexity3.4448301792144775
INFO:root:current mean train loss 1410.786369550989
INFO:root:current train perplexity3.4454283714294434
INFO:root:current mean train loss 1411.746535550631
INFO:root:current train perplexity3.450385093688965
INFO:root:current mean train loss 1411.1350917883087
INFO:root:current train perplexity3.4514644145965576
INFO:root:current mean train loss 1410.9780909384451
INFO:root:current train perplexity3.452425718307495
INFO:root:current mean train loss 1410.8029827971957
INFO:root:current train perplexity3.4517054557800293
INFO:root:current mean train loss 1411.8950205485025
INFO:root:current train perplexity3.4520771503448486
INFO:root:current mean train loss 1412.7824357466263
INFO:root:current train perplexity3.4544482231140137
INFO:root:current mean train loss 1413.0361945919874
INFO:root:current train perplexity3.4564011096954346
INFO:root:current mean train loss 1413.250854211566
INFO:root:current train perplexity3.4571855068206787
INFO:root:current mean train loss 1413.2767672994862
INFO:root:current train perplexity3.4575300216674805
INFO:root:current mean train loss 1413.293316461622
INFO:root:current train perplexity3.4568800926208496

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:57<00:00, 477.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:57<00:00, 477.58s/it]
INFO:root:final mean train loss: 1413.2919088374229
INFO:root:final train perplexity: 3.4579195976257324
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.61s/it]
INFO:root:eval mean loss: 1848.402064979499
INFO:root:eval perplexity: 5.30403470993042
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.96s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.96s/it]
INFO:root:eval mean loss: 2331.8949533016125
INFO:root:eval perplexity: 8.518248558044434
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/74
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/200 [11:39:14<19:45:09, 564.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1427.0096371299342
INFO:root:current train perplexity3.5129687786102295
INFO:root:current mean train loss 1414.6150843140426
INFO:root:current train perplexity3.475827932357788
INFO:root:current mean train loss 1411.267524452061
INFO:root:current train perplexity3.46525239944458
INFO:root:current mean train loss 1408.9873381969976
INFO:root:current train perplexity3.4626247882843018
INFO:root:current mean train loss 1409.0980638633412
INFO:root:current train perplexity3.456214666366577
INFO:root:current mean train loss 1407.069814575853
INFO:root:current train perplexity3.450502634048462
INFO:root:current mean train loss 1407.0067333761415
INFO:root:current train perplexity3.446897506713867
INFO:root:current mean train loss 1409.5533579495025
INFO:root:current train perplexity3.446445941925049
INFO:root:current mean train loss 1408.34934215924
INFO:root:current train perplexity3.4443702697753906
INFO:root:current mean train loss 1408.0756082086355
INFO:root:current train perplexity3.4442999362945557
INFO:root:current mean train loss 1407.1886389379583
INFO:root:current train perplexity3.442011594772339
INFO:root:current mean train loss 1407.271101599638
INFO:root:current train perplexity3.4424006938934326
INFO:root:current mean train loss 1408.706589388487
INFO:root:current train perplexity3.444871664047241
INFO:root:current mean train loss 1409.8714586115973
INFO:root:current train perplexity3.4466755390167236
INFO:root:current mean train loss 1410.0858934306902
INFO:root:current train perplexity3.44710373878479
INFO:root:current mean train loss 1410.4160393804943
INFO:root:current train perplexity3.4472081661224365
INFO:root:current mean train loss 1410.9719918250462
INFO:root:current train perplexity3.448453903198242
INFO:root:current mean train loss 1410.666893461311
INFO:root:current train perplexity3.448402166366577
INFO:root:current mean train loss 1409.9260419559016
INFO:root:current train perplexity3.447587490081787
INFO:root:current mean train loss 1409.8629013162886
INFO:root:current train perplexity3.4464571475982666

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:14<00:00, 494.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:14<00:00, 494.76s/it]
INFO:root:final mean train loss: 1409.7203388839314
INFO:root:final train perplexity: 3.447094440460205
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.87s/it]
INFO:root:eval mean loss: 1850.1063002998947
INFO:root:eval perplexity: 5.312200546264648
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.90s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.90s/it]
INFO:root:eval mean loss: 2333.5943824281085
INFO:root:eval perplexity: 8.531557083129883
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/75
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/200 [11:48:48<19:41:37, 567.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1404.399938634924
INFO:root:current train perplexity3.4327023029327393
INFO:root:current mean train loss 1401.4138295842313
INFO:root:current train perplexity3.420121669769287
INFO:root:current mean train loss 1399.16443138401
INFO:root:current train perplexity3.416428565979004
INFO:root:current mean train loss 1397.2607816808363
INFO:root:current train perplexity3.41516375541687
INFO:root:current mean train loss 1397.4139370817675
INFO:root:current train perplexity3.4197428226470947
INFO:root:current mean train loss 1395.6010291335474
INFO:root:current train perplexity3.418179988861084
INFO:root:current mean train loss 1398.6757457518083
INFO:root:current train perplexity3.4204599857330322
INFO:root:current mean train loss 1401.3626871429365
INFO:root:current train perplexity3.42231822013855
INFO:root:current mean train loss 1402.9205605792781
INFO:root:current train perplexity3.4262330532073975
INFO:root:current mean train loss 1403.0709941636856
INFO:root:current train perplexity3.425435781478882
INFO:root:current mean train loss 1402.652633240769
INFO:root:current train perplexity3.422825336456299
INFO:root:current mean train loss 1403.545223307569
INFO:root:current train perplexity3.4249606132507324
INFO:root:current mean train loss 1404.3895491715316
INFO:root:current train perplexity3.4252593517303467
INFO:root:current mean train loss 1405.0260584579978
INFO:root:current train perplexity3.4279496669769287
INFO:root:current mean train loss 1405.2956040277572
INFO:root:current train perplexity3.428879976272583
INFO:root:current mean train loss 1405.4211412597035
INFO:root:current train perplexity3.4293384552001953
INFO:root:current mean train loss 1404.8733051623637
INFO:root:current train perplexity3.4304471015930176
INFO:root:current mean train loss 1405.291328163534
INFO:root:current train perplexity3.4319024085998535
INFO:root:current mean train loss 1405.6189992735708
INFO:root:current train perplexity3.4317808151245117
INFO:root:current mean train loss 1405.5107071247508
INFO:root:current train perplexity3.4333958625793457

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:07<00:00, 487.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:07<00:00, 487.26s/it]
INFO:root:final mean train loss: 1405.3189496585233
INFO:root:final train perplexity: 3.4338016510009766
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.34s/it]
INFO:root:eval mean loss: 1851.6018023118904
INFO:root:eval perplexity: 5.3193769454956055
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.91s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.91s/it]
INFO:root:eval mean loss: 2338.846054185367
INFO:root:eval perplexity: 8.572813987731934
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/76
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [11:58:16<19:32:30, 567.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1388.125944368132
INFO:root:current train perplexity3.390565872192383
INFO:root:current mean train loss 1394.0913558880072
INFO:root:current train perplexity3.4026596546173096
INFO:root:current mean train loss 1394.4944904759986
INFO:root:current train perplexity3.406996726989746
INFO:root:current mean train loss 1392.6810074828165
INFO:root:current train perplexity3.406006097793579
INFO:root:current mean train loss 1395.7717536258115
INFO:root:current train perplexity3.4084672927856445
INFO:root:current mean train loss 1395.1684524871775
INFO:root:current train perplexity3.406855344772339
INFO:root:current mean train loss 1393.6267849470876
INFO:root:current train perplexity3.406705856323242
INFO:root:current mean train loss 1394.8139291948976
INFO:root:current train perplexity3.4077045917510986
INFO:root:current mean train loss 1395.1197272749193
INFO:root:current train perplexity3.4098665714263916
INFO:root:current mean train loss 1396.0884440679001
INFO:root:current train perplexity3.411241054534912
INFO:root:current mean train loss 1395.9117315276624
INFO:root:current train perplexity3.4119551181793213
INFO:root:current mean train loss 1397.4201212257622
INFO:root:current train perplexity3.413102865219116
INFO:root:current mean train loss 1399.483757935043
INFO:root:current train perplexity3.4163198471069336
INFO:root:current mean train loss 1400.3424561073418
INFO:root:current train perplexity3.4163904190063477
INFO:root:current mean train loss 1400.1265166683695
INFO:root:current train perplexity3.4176852703094482
INFO:root:current mean train loss 1401.5334758842464
INFO:root:current train perplexity3.419229745864868
INFO:root:current mean train loss 1401.561934477334
INFO:root:current train perplexity3.419635772705078
INFO:root:current mean train loss 1401.789206857857
INFO:root:current train perplexity3.4214727878570557
INFO:root:current mean train loss 1402.019594189479
INFO:root:current train perplexity3.4219768047332764

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:44<00:00, 464.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:44<00:00, 464.82s/it]
INFO:root:final mean train loss: 1401.8265262667242
INFO:root:final train perplexity: 3.423290252685547
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.05s/it]
INFO:root:eval mean loss: 1853.7420039616577
INFO:root:eval perplexity: 5.329662322998047
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.41s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.41s/it]
INFO:root:eval mean loss: 2342.541605198637
INFO:root:eval perplexity: 8.601968765258789
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/77
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/200 [12:07:21<19:09:42, 560.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1298.0994873046875
INFO:root:current train perplexity3.2407822608947754
INFO:root:current mean train loss 1391.8417482729312
INFO:root:current train perplexity3.37937068939209
INFO:root:current mean train loss 1394.957980816181
INFO:root:current train perplexity3.3813436031341553
INFO:root:current mean train loss 1397.6654179560674
INFO:root:current train perplexity3.3903331756591797
INFO:root:current mean train loss 1397.2231056362975
INFO:root:current train perplexity3.3879222869873047
INFO:root:current mean train loss 1395.6158492921845
INFO:root:current train perplexity3.3883347511291504
INFO:root:current mean train loss 1397.4642470510382
INFO:root:current train perplexity3.395277976989746
INFO:root:current mean train loss 1397.1049377096574
INFO:root:current train perplexity3.399710178375244
INFO:root:current mean train loss 1395.0064904241278
INFO:root:current train perplexity3.4001383781433105
INFO:root:current mean train loss 1394.4338731135565
INFO:root:current train perplexity3.401198148727417
INFO:root:current mean train loss 1396.3370839679053
INFO:root:current train perplexity3.4055709838867188
INFO:root:current mean train loss 1397.329739016316
INFO:root:current train perplexity3.4061880111694336
INFO:root:current mean train loss 1396.7070449930154
INFO:root:current train perplexity3.406733989715576
INFO:root:current mean train loss 1397.2998441643672
INFO:root:current train perplexity3.4061243534088135
INFO:root:current mean train loss 1397.0913160497491
INFO:root:current train perplexity3.4067463874816895
INFO:root:current mean train loss 1397.8002795313018
INFO:root:current train perplexity3.408362865447998
INFO:root:current mean train loss 1398.7329858428802
INFO:root:current train perplexity3.4101834297180176
INFO:root:current mean train loss 1399.171939108355
INFO:root:current train perplexity3.412900924682617
INFO:root:current mean train loss 1398.5256328076389
INFO:root:current train perplexity3.4124326705932617
INFO:root:current mean train loss 1398.6610175878498
INFO:root:current train perplexity3.413024663925171

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:08<00:00, 488.93s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:08<00:00, 488.93s/it]
INFO:root:final mean train loss: 1398.4076283849733
INFO:root:final train perplexity: 3.413031578063965
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.62s/it]
INFO:root:eval mean loss: 1855.9146529393838
INFO:root:eval perplexity: 5.34012508392334
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.27s/it]
INFO:root:eval mean loss: 2344.6359971499614
INFO:root:eval perplexity: 8.618535041809082
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/78
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [12:16:55<19:07:51, 564.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1396.6328369140624
INFO:root:current train perplexity3.353350877761841
INFO:root:current mean train loss 1389.059884765625
INFO:root:current train perplexity3.3784401416778564
INFO:root:current mean train loss 1389.1863373480903
INFO:root:current train perplexity3.370115041732788
INFO:root:current mean train loss 1385.8507748647837
INFO:root:current train perplexity3.3679964542388916
INFO:root:current mean train loss 1385.3561764705883
INFO:root:current train perplexity3.368711471557617
INFO:root:current mean train loss 1386.8656198846727
INFO:root:current train perplexity3.3771517276763916
INFO:root:current mean train loss 1387.3698328125
INFO:root:current train perplexity3.37872576713562
INFO:root:current mean train loss 1391.0600190261314
INFO:root:current train perplexity3.386775493621826
INFO:root:current mean train loss 1392.3585799893465
INFO:root:current train perplexity3.3927829265594482
INFO:root:current mean train loss 1393.1499014199746
INFO:root:current train perplexity3.3941731452941895
INFO:root:current mean train loss 1393.4114613900533
INFO:root:current train perplexity3.3960464000701904
INFO:root:current mean train loss 1393.6740858289932
INFO:root:current train perplexity3.397700786590576
INFO:root:current mean train loss 1393.0330160833864
INFO:root:current train perplexity3.3960819244384766
INFO:root:current mean train loss 1394.1277011165978
INFO:root:current train perplexity3.396933078765869
INFO:root:current mean train loss 1393.8372537177906
INFO:root:current train perplexity3.396129608154297
INFO:root:current mean train loss 1393.9134423027665
INFO:root:current train perplexity3.3971893787384033
INFO:root:current mean train loss 1393.728434720553
INFO:root:current train perplexity3.3988277912139893
INFO:root:current mean train loss 1394.1771440500454
INFO:root:current train perplexity3.3991079330444336
INFO:root:current mean train loss 1393.6068075101668
INFO:root:current train perplexity3.3991758823394775
INFO:root:current mean train loss 1393.8060698432428
INFO:root:current train perplexity3.398371696472168

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:01<00:00, 481.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:01<00:00, 481.51s/it]
INFO:root:final mean train loss: 1393.4851160092721
INFO:root:final train perplexity: 3.3983144760131836
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.26s/it]
INFO:root:eval mean loss: 1857.544685526097
INFO:root:eval perplexity: 5.347989082336426
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.89s/it]
INFO:root:eval mean loss: 2347.380875824191
INFO:root:eval perplexity: 8.640296936035156
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mp_gpt2/79
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/200 [12:26:21<18:59:21, 564.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1387.288853236607
INFO:root:current train perplexity3.372572898864746
INFO:root:current mean train loss 1380.1797046930017
INFO:root:current train perplexity3.3645687103271484
INFO:root:current mean train loss 1387.875150822411
INFO:root:current train perplexity3.3745484352111816
INFO:root:current mean train loss 1385.2753652829176
INFO:root:current train perplexity3.3714747428894043
INFO:root:current mean train loss 1385.0914723668163
INFO:root:current train perplexity3.3706910610198975
INFO:root:current mean train loss 1382.8237016403368
INFO:root:current train perplexity3.3713667392730713
INFO:root:current mean train loss 1382.5615186839832
INFO:root:current train perplexity3.3764336109161377
INFO:root:current mean train loss 1382.1939547556751
INFO:root:current train perplexity3.3801276683807373
INFO:root:current mean train loss 1383.7771729675437
INFO:root:current train perplexity3.3812520503997803
INFO:root:current mean train loss 1383.4905705219114
INFO:root:current train perplexity3.380324363708496
INFO:root:current mean train loss 1384.4730003195853
INFO:root:current train perplexity3.382718801498413
INFO:root:current mean train loss 1383.8792807984894
INFO:root:current train perplexity3.3802640438079834
slurmstepd: error: *** JOB 26219400 ON ga003 CANCELLED AT 2022-10-24T11:44:35 ***
